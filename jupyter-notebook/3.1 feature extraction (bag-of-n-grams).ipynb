{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_window_size_1gram = 5\n",
    "real_window_size_class = 3\n",
    "\n",
    "window_size_class = (real_window_size_class * 2) + 1\n",
    "window_size_1gram = (real_window_size_1gram * 2) + 1\n",
    "\n",
    "using_1gram = 1\n",
    "using_2gram = 1\n",
    "using_3gram = 0\n",
    "using_5gram = 0\n",
    "using_freqwords_eachclass = 0\n",
    "\n",
    "#######################################################################################################\n",
    "from data_handler import *\n",
    "from feature_design_for_ngrams import *\n",
    "\n",
    "pre_X_train = load('pre_X_train') # preprocessed text data\n",
    "pre_X_test = load('pre_X_test') # preprocessed text data\n",
    "\n",
    "word_one_gram_lookup_table = load('word_one_gram_lookup_table')\n",
    "word_bi_gram_lookup_table = load('word_bi_gram_lookup_table')\n",
    "word_tri_gram_lookup_table = load('word_tri_gram_lookup_table')\n",
    "word_five_gram_lookup_table = load('word_five_gram_lookup_table')\n",
    "\n",
    "lookup_table_class_component = load('lookup_table_class_component')\n",
    "lookup_table_class_refinement_of_component = load('lookup_table_class_refinement_of_component')\n",
    "lookup_table_class_action = load('lookup_table_class_action')\n",
    "lookup_table_class_refinement_of_action = load('lookup_table_class_refinement_of_action')\n",
    "lookup_table_class_condition = load('lookup_table_class_condition')\n",
    "lookup_table_class_priority = load('lookup_table_class_priority')\n",
    "lookup_table_class_motivation = load('lookup_table_class_motivation')\n",
    "lookup_table_class_role = load('lookup_table_class_role')\n",
    "lookup_table_class_object = load('lookup_table_class_object')\n",
    "lookup_table_class_refinement_of_object = load('lookup_table_class_refinement_of_object')\n",
    "lookup_table_class_sub_action = load('lookup_table_class_sub_action')\n",
    "lookup_table_class_sub_argument_of_action = load('lookup_table_class_sub_argument_of_action')\n",
    "lookup_table_class_sub_priority = load('lookup_table_class_sub_priority')\n",
    "lookup_table_class_sub_role = load('lookup_table_class_sub_role')\n",
    "lookup_table_class_sub_object = load('lookup_table_class_sub_object')\n",
    "lookup_table_class_sub_refinement_of_object = load('lookup_table_class_sub_refinement_of_object')\n",
    "lookup_table_class_none = load('lookup_table_class_none')\n",
    "\n",
    "### FE_X_train와 FE_X_test 초기화\n",
    "# FE_X_data는 feature extraction을 통해서 text에서 numerical vector로 변환된다.\n",
    "FE_X_train = []\n",
    "for sentence in (pre_X_train):\n",
    "    temp = []\n",
    "    for token in (sentence):\n",
    "        temp.append('point_to_be_vector_via_feature_extraction')\n",
    "    FE_X_train.append(temp)\n",
    "\n",
    "FE_X_test = []\n",
    "for sentence in (pre_X_test):\n",
    "    temp = []\n",
    "    for token in (sentence):\n",
    "        temp.append('point_to_be_vector_via_feature_extraction')\n",
    "    FE_X_test.append(temp)\n",
    "\n",
    "# for feature information...\n",
    "feature_info_without_LK = ['###### ************** Feature Information ************** ######']\n",
    "this_feature_length = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction \n",
    "### based on lookup table or without *linguistic knowledge*\n",
    "\n",
    "* feature extraction할 때, numbering함수를 통해서 unknown_token/phrase의 포함여부를 결정지을 수 있다.\n",
    "* 상대적으로 크기가 작은 lookup table은 unknown을 포함시키지 말자. (-> 실험적으로 판단하자. 몇몇의 실험에서는 성능이 조금 더 증가하였다.)\n",
    "* high-dimensional and sparse feature를 더 압축하고 abstraction할 수 있는 도구인 PCA를 사용할 수 있을지 생각해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Using word one-gram lookup table \n",
    "* 기본적으로 current time의 token만 고려하지만,\n",
    "* window 개념을 도입하면, 그 size에 만큼 feature를 더 확장할 수 있다.\n",
    "* 우선은 current time의 token만 사용하자\n",
    "\n",
    "* 만약 window size가 3이라면 다음과 같이 feature를 구성한다.\n",
    "   - [ token(t-1), token(t), token(t+1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature length 1-gram =  8019\n"
     ]
    }
   ],
   "source": [
    "if using_1gram == 1:\n",
    "    # for training data\n",
    "    for i, sentence in enumerate(pre_X_train):\n",
    "        for j, token in enumerate(sentence):\n",
    "            FE_X_train[i][j] = feature_extraction_1gram(j, sentence, word_one_gram_lookup_table, window_size_1gram) \n",
    "            this_feature_length = len(FE_X_train[i][j])\n",
    "\n",
    "    # for testing data\n",
    "    for i, sentence in enumerate(pre_X_test):\n",
    "        for j, token in enumerate(sentence):\n",
    "            FE_X_test[i][j] = feature_extraction_1gram(j, sentence, word_one_gram_lookup_table, window_size_1gram)                 \n",
    "\n",
    "    # Write feature information\n",
    "    #feature_info_without_LK.append('word one gram (t) and dim = '+str(this_feature_length))\n",
    "    feature_info_without_LK.append('=> word one gram window size'+ str(window_size_1gram) +'and dim = '+str(this_feature_length))\n",
    "    print('feature length 1-gram = ', this_feature_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주의 할점: Step1같은 경우 처음으로 feature vector를 만드는 단계이므로 FE_X_test[i][j] = step1_feature_vector와 같이 feature vector에 할당한다. 하지만, 그 이후의 단계에서는 feature vector를 += 와 같이 concatenation해야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Using word bi-gram lookup table \n",
    "\n",
    "* 크기가 2이기 때문에 언제나 비대칭을 이룬다. \n",
    "* 따라서, 다음과 같이 2개의 경우가 생긴다\n",
    "   - [ token(t-1), token(t) ]\n",
    "   - [ token(t), token(t+1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature length 2-gram =  1076\n"
     ]
    }
   ],
   "source": [
    "if using_2gram == 1:\n",
    "    # for training data\n",
    "    for i, sentence in enumerate(pre_X_train):\n",
    "        for j, token in enumerate(sentence):\n",
    "            step2_feature_vector = [] # [ 1g_t(t-1), 1g_t(t), 1g_t(t+1) ]\n",
    "\n",
    "            \"\"\" [ token(t-1), token(t) ] \"\"\"\n",
    "            step2_feature_vector += numbering_token_using_2gram_lookup(j-1, j, sentence, word_bi_gram_lookup_table)\n",
    "            \"\"\" [ token(t), token(t-1) ] \"\"\"\n",
    "            step2_feature_vector += numbering_token_using_2gram_lookup(j, j+1, sentence, word_bi_gram_lookup_table)\n",
    "\n",
    "            FE_X_train[i][j] += step2_feature_vector # feature assignment\n",
    "            this_feature_length = len(step2_feature_vector)\n",
    "\n",
    "    # for testing data\n",
    "    for i, sentence in enumerate(pre_X_test):\n",
    "        for j, token in enumerate(sentence):\n",
    "            step2_feature_vector = [] # [ 1g_t(t-1), 1g_t(t), 1g_t(t+1) ]\n",
    "\n",
    "            \"\"\" [ token(t-1), token(t) ] \"\"\"\n",
    "            step2_feature_vector += numbering_token_using_2gram_lookup(j-1, j, sentence, word_bi_gram_lookup_table)\n",
    "            \"\"\" [ token(t), token(t-1) ] \"\"\"\n",
    "            step2_feature_vector += numbering_token_using_2gram_lookup(j, j+1, sentence, word_bi_gram_lookup_table)\n",
    "\n",
    "            FE_X_test[i][j] += step2_feature_vector # feature assignment\n",
    "\n",
    "    # Wrtie feature information\n",
    "    feature_info_without_LK.append('=> word bi-gram (t-1), (t) and dim = '+str(this_feature_length))\n",
    "    feature_info_without_LK.append('=> word bi-gram (t), (t+1) and dim = '+str(this_feature_length))\n",
    "    print('feature length 2-gram = ', this_feature_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Using word tri-gram lookup table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if using_3gram == 1:\n",
    "    # for training data\n",
    "    for i, sentence in enumerate(pre_X_train):\n",
    "        for j, token in enumerate(sentence):\n",
    "\n",
    "            step3_feature_vector = [] # [ 1g_t(t-1), 1g_t(t), 1g_t(t+1) ]\n",
    "\n",
    "            \"\"\" [ token(t-2), token(t-1), token(t) ] \"\"\"\n",
    "            step3_feature_vector += numbering_token_using_3gram_lookup(j-2, j-1, j, sentence, word_tri_gram_lookup_table)\n",
    "            \"\"\" [ token(t-1), token(t), token(t+1) ] \"\"\"\n",
    "            step3_feature_vector += numbering_token_using_3gram_lookup(j-1, j, j+1, sentence, word_tri_gram_lookup_table)\n",
    "            \"\"\" [ token(t), token(t+1), token(t+2) ] \"\"\"\n",
    "            step3_feature_vector += numbering_token_using_3gram_lookup(j, j+1, j+2, sentence, word_tri_gram_lookup_table)\n",
    "\n",
    "            FE_X_train[i][j] += step3_feature_vector # feature assignment  \n",
    "            this_feature_length = len(step3_feature_vector)\n",
    "\n",
    "    # for testing data\n",
    "    for i, sentence in enumerate(pre_X_test):\n",
    "        for j, token in enumerate(sentence):\n",
    "\n",
    "            step3_feature_vector = [] # [ 1g_t(t-1), 1g_t(t), 1g_t(t+1) ]\n",
    "\n",
    "            \"\"\" [ token(t-2), token(t-1), token(t) ] \"\"\"\n",
    "            step3_feature_vector += numbering_token_using_3gram_lookup(j-2, j-1, j, sentence, word_tri_gram_lookup_table)\n",
    "            \"\"\" [ token(t-1), token(t), token(t+1) ] \"\"\"\n",
    "            step3_feature_vector += numbering_token_using_3gram_lookup(j-1, j, j+1, sentence, word_tri_gram_lookup_table)\n",
    "            \"\"\" [ token(t), token(t+1), token(t+2) ] \"\"\"\n",
    "            step3_feature_vector += numbering_token_using_3gram_lookup(j, j+1, j+2, sentence, word_tri_gram_lookup_table)\n",
    "\n",
    "            FE_X_test[i][j] += step3_feature_vector # feature assignment\n",
    "\n",
    "    # Wrtie feature information\n",
    "    feature_info_without_LK.append('=> word tri-gram (t-1), (t), (t+1) and dim = '+str(this_feature_length))\n",
    "    print('feature length 3-gram = ', this_feature_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Using word five-gram lookup table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if using_5gram == 1:\n",
    "    # for training data\n",
    "    for i, sentence in enumerate(pre_X_train):\n",
    "        for j, token in enumerate(sentence):\n",
    "\n",
    "            step4_feature_vector = [] # [ 1g_t(t-1), 1g_t(t), 1g_t(t+1) ]\n",
    "\n",
    "            \"\"\" [ token(t-2), token(t-1), token(t), token(t+1), token(t+2) ] \"\"\"\n",
    "            step4_feature_vector += numbering_token_using_5gram_lookup(j-2, j-1, j, j+1, j+2, sentence, word_five_gram_lookup_table)\n",
    "\n",
    "            FE_X_train[i][j] += step4_feature_vector # feature assignment \n",
    "            this_feature_length = len(step4_feature_vector)\n",
    "\n",
    "    # for testing data\n",
    "    for i, sentence in enumerate(pre_X_test):\n",
    "        for j, token in enumerate(sentence):\n",
    "\n",
    "            step4_feature_vector = [] # [ 1g_t(t-1), 1g_t(t), 1g_t(t+1) ]\n",
    "\n",
    "            \"\"\" [ token(t-2), token(t-1), token(t), token(t+1), token(t+2) ] \"\"\"\n",
    "            step4_feature_vector += numbering_token_using_5gram_lookup(j-2, j-1, j, j+1, j+2, sentence, word_five_gram_lookup_table)\n",
    "\n",
    "            FE_X_test[i][j] += step4_feature_vector # feature assignment\n",
    "\n",
    "    # Wrtie feature information\n",
    "    feature_info_without_LK.append('=> word five-gram (t-2), (t-1), (t), (t+1), (t+2) and dim = '+str(this_feature_length))\n",
    "    print('feature length 5-gram = ', this_feature_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Using class lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if using_freqwords_eachclass == 1:\n",
    "    # for training data\n",
    "    for i, sentence in enumerate(pre_X_train):\n",
    "        for j, token in enumerate(sentence):\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_component, window_size_class)\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_refinement_of_component, window_size_class)\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_action, window_size_class)\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_refinement_of_action, window_size_class)\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_condition, window_size_class)\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_priority, window_size_class)\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_motivation, window_size_class)\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_role, window_size_class)\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_object, window_size_class)\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_refinement_of_object, window_size_class)\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_sub_action, window_size_class)\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_sub_argument_of_action, window_size_class)\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_sub_priority, window_size_class)\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_sub_role, window_size_class)\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_sub_object, window_size_class)\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_sub_refinement_of_object, window_size_class)\n",
    "            FE_X_train[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_none, window_size_class)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             FE_X_train[i][j] += FE_for_classLUT(j, sentence) \n",
    "#             this_feature_length = len(FE_for_classLUT(j, sentence))\n",
    "\n",
    "    # for testing data\n",
    "    for i, sentence in enumerate(pre_X_test):\n",
    "        for j, token in enumerate(sentence):\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_component, window_size_class)\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_refinement_of_component, window_size_class)\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_action, window_size_class)\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_refinement_of_action, window_size_class)\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_condition, window_size_class)\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_priority, window_size_class)\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_motivation, window_size_class)\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_role, window_size_class)\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_object, window_size_class)\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_refinement_of_object, window_size_class)\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_sub_action, window_size_class)\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_sub_argument_of_action, window_size_class)\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_sub_priority, window_size_class)\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_sub_role, window_size_class)\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_sub_object, window_size_class)\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_sub_refinement_of_object, window_size_class)\n",
    "            FE_X_test[i][j] += feature_extraction_1gram(j, sentence, lookup_table_class_none, window_size_class)            \n",
    "            \n",
    "\n",
    "#             FE_X_test[i][j] += FE_for_classLUT(j, sentence) \n",
    "#     print('All class lookup table features = ', this_feature_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Total feature length = 9095 9095\n"
     ]
    }
   ],
   "source": [
    "# if last step,\n",
    "feature_info_without_LK.append('****** Total feature dimension = '+str(len(FE_X_train[7][7])))\n",
    "dump(FE_X_train, 'FE_X_train')\n",
    "dump(FE_X_test, 'FE_X_test')\n",
    "dump(feature_info_without_LK, 'feature_info_without_LK')\n",
    "print('===> Total feature length =', len(FE_X_train[0][0]), len(FE_X_test[0][0]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
