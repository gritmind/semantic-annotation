{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from seqlearn.perceptron import StructuredPerceptron\n",
    "\n",
    "# clf = StructuredPerceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def features(sequence, i):\n",
    "#     yield \"word=\" + sequence[i].lower()\n",
    "#     if sequence[i].isupper():\n",
    "#         yield \"Uppercase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from seqlearn.datasets import load_conll\n",
    "# X_train, y_train, lenths_train = load_conll(\"train.txt\", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_for_RNN(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    zero_array = [0]*len(X_train[0][0])\n",
    "    padding_X_train = []\n",
    "    for i, sentence in enumerate(X_train):\n",
    "        for j, token in enumerate(sentence):\n",
    "            temp_point = []\n",
    "            if j==0 or j==len(sentence)-1:\n",
    "                temp_point.append(zero_array)\n",
    "                temp_point.append(zero_array)\n",
    "                temp_point.append(sentence[j])\n",
    "                padding_X_train.append(temp_point)\n",
    "            elif j==1 or j==len(sentence)-2:\n",
    "                temp_point.append(zero_array)\n",
    "                temp_point.append(sentence[j-1])\n",
    "                temp_point.append(sentence[j])\n",
    "                padding_X_train.append(temp_point)\n",
    "            else:\n",
    "                temp_point.append(sentence[j-2])\n",
    "                temp_point.append(sentence[j-1])\n",
    "                temp_point.append(sentence[j])\n",
    "                padding_X_train.append(temp_point)    \n",
    "    padding_X_train = np.array(padding_X_train, dtype='float32')\n",
    "    unfolded_Y_train = []\n",
    "    for sentence in Y_train:\n",
    "        for token in sentence:\n",
    "            unfolded_Y_train.append(token)\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(unfolded_Y_train)\n",
    "    encoded_Y = encoder.transform(unfolded_Y_train)\n",
    "    dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "    \n",
    "    ###############################################################\n",
    "    zero_array = [0]*len(X_test[0][0])\n",
    "    padding_X_test = []\n",
    "    for i, sentence in enumerate(X_test):\n",
    "        for j, token in enumerate(sentence):\n",
    "            temp_point = []\n",
    "            if j==0 or j==len(sentence)-1:\n",
    "                temp_point.append(zero_array)\n",
    "                temp_point.append(zero_array)\n",
    "                temp_point.append(sentence[j])\n",
    "                padding_X_test.append(temp_point)\n",
    "            elif j==1 or j==len(sentence)-2:\n",
    "                temp_point.append(zero_array)\n",
    "                temp_point.append(sentence[j-1])\n",
    "                temp_point.append(sentence[j])\n",
    "                padding_X_test.append(temp_point)\n",
    "            else:\n",
    "                temp_point.append(sentence[j-2])\n",
    "                temp_point.append(sentence[j-1])\n",
    "                temp_point.append(sentence[j])\n",
    "                padding_X_test.append(temp_point)    \n",
    "    padding_X_test = np.array(padding_X_test, dtype='float32')\n",
    "    unfolded_Y_test = []\n",
    "    for sentence in Y_test:\n",
    "        for token in sentence:\n",
    "            unfolded_Y_test.append(token)\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(unfolded_Y_test)\n",
    "    encoded_Y = encoder.transform(unfolded_Y_test)\n",
    "    dummy_y_test = np_utils.to_categorical(encoded_Y)\n",
    "    return padding_X_train, dummy_y, padding_X_test, dummy_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Total feature length = 10121\n"
     ]
    }
   ],
   "source": [
    "# Choose what you want to use among various kinds of features\n",
    "using_1type = 1\n",
    "using_2type = 1\n",
    "using_3type = 1\n",
    "using_4type = 1\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from pandas import read_csv \n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import precision_recall_fscore_support \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from data_handler import *\n",
    "from data_handling_for_NNs import *\n",
    "\n",
    "# Large CNN model for the CIFAR-10 Dataset \n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential \n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from keras.utils import np_utils \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Dropout \n",
    "from keras.layers import Flatten \n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD \n",
    "from keras.layers.convolutional import Convolution2D \n",
    "from keras.layers.convolutional import MaxPooling2D \n",
    "from keras.utils import np_utils \n",
    "from keras import backend as K \n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# CNN for the IMDB problem \n",
    "from keras.datasets import imdb \n",
    "from keras.layers.convolutional import Convolution1D \n",
    "from keras.layers.convolutional import MaxPooling1D \n",
    "from keras.layers.embeddings import Embedding \n",
    "from keras.preprocessing import sequence \n",
    "\n",
    "# LSTM with Variable Length Input Sequences to One Character Output \n",
    "from keras.layers import LSTM \n",
    "from keras.utils import np_utils \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "feature_path = 'assets/feature-vec/'\n",
    "tempdata_path = 'assets/temp-data/'\n",
    "\n",
    "### Data Load\n",
    "FE_X_train = load(feature_path+'FE_X_train')\n",
    "FE_X_test = load(feature_path+'FE_X_test')\n",
    "################################### FE\n",
    "FE2_X_train = load(feature_path+'FE2_X_train')\n",
    "FE2_X_test = load(feature_path+'FE2_X_test')\n",
    "################################### FE 2\n",
    "FE3_X_train = load(feature_path+'FE3_X_train')\n",
    "FE3_X_test = load(feature_path+'FE3_X_test')\n",
    "################################### FE 3\n",
    "FE4_X_train = load(feature_path+'FE4_X_train')\n",
    "FE4_X_test = load(feature_path+'FE4_X_test')\n",
    "Y_train = load(feature_path+'Ytrain')\n",
    "Y_test = load(feature_path+'Ytest')\n",
    "################################### Y\n",
    "feature_info_without_LK = load('feature_info_without_LK')\n",
    "\n",
    "### Choice Feature Set \n",
    "X_train, X_test = choice_feature_set(using_1type, using_2type, using_3type, using_4type,\\\n",
    "                                     FE_X_train, FE2_X_train, FE3_X_train, FE4_X_train,\\\n",
    "                                     FE_X_test, FE2_X_test, FE3_X_test, FE4_X_test)\n",
    "\n",
    "print('===> Total feature length =', len(X_train[0][0]))\n",
    "\n",
    "### Prepare data for each model\n",
    "X_train_FNN, Y_train_FNN, X_test_FNN, Y_test_FNN = prepare_for_FNN(X_train, Y_train, X_test, Y_test)    \n",
    "X_train_CNN, Y_train_CNN, X_test_CNN, Y_test_CNN = prepare_for_CNN(X_train, Y_train, X_test, Y_test)   \n",
    "X_train_RNN, Y_train_RNN, X_test_RNN, Y_test_RNN = prepare_for_RNN(X_train, Y_train, X_test, Y_test)     \n",
    "\n",
    "assert(len(X_train) == len(Y_train))\n",
    "assert(len(X_test) == len(Y_test))\n",
    "#assert(len(X_test[0]) == len(X_train[0]))\n",
    "assert(len(X_train_FNN) == len(Y_train_FNN))\n",
    "assert(len(X_test_FNN) == len(Y_test_FNN))\n",
    "assert(len(X_train_CNN) == len(Y_train_CNN))\n",
    "assert(len(X_test_CNN) == len(Y_test_CNN))\n",
    "assert(len(X_train_RNN) == len(Y_train_RNN))\n",
    "assert(len(X_test_RNN) == len(Y_test_RNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for FNN\n",
    "input_dim = len(X_train_FNN[0])\n",
    "hidden_dim = 17\n",
    "output_dim = len(Y_train_FNN[0])\n",
    "n_epoch_fnn = 10\n",
    "\n",
    "# for CNN\n",
    "num_classes = Y_train_CNN.shape[1]\n",
    "x_square = X_train_CNN.shape[3]\n",
    "n_epoch_cnn = 3\n",
    "\n",
    "# for RNN\n",
    "time_length = X_train_RNN.shape[1]\n",
    "feature_dim = X_train_RNN.shape[2]\n",
    "n_hidden_node = 100\n",
    "output_dim = Y_train_RNN.shape[1]\n",
    "n_epoch_rnn = 3\n",
    "batch_size = 1 \n",
    "\n",
    "def feedforward():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_dim, input_dim = input_dim, init='normal', activation='relu')) \n",
    "    model.add(Dense(output_dim, init='normal', activation='sigmoid'))\n",
    "    # Compile model \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])     \n",
    "    return model\n",
    "\n",
    "def convolutional():\n",
    "    model = Sequential() \n",
    "    model.add(Convolution2D(32, 3, 3, input_shape=(1, x_square, x_square), activation='relu', border_mode='same')) \n",
    "#     model.add(Dropout(0.2)) \n",
    "    model.add(Convolution2D(32, 3, 3, activation='relu', border_mode='same')) \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Convolution2D(64, 3, 3, activation='relu', border_mode='same')) \n",
    "#     model.add(Dropout(0.2)) \n",
    "#     model.add(Convolution2D(64, 3, 3, activation='relu', border_mode='same')) \n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "    # model.add(Convolution2D(128, 3, 3, activation='relu', border_mode='same')) \n",
    "    # model.add(Dropout(0.2)) \n",
    "    # model.add(Convolution2D(128, 3, 3, activation='relu', border_mode='same')) \n",
    "    # model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "    model.add(Flatten()) \n",
    "    # model.add(Dropout(0.2)) \n",
    "    # model.add(Dense(1024, activation='relu', W_constraint=maxnorm(3))) \n",
    "    # model.add(Dropout(0.2)) \n",
    "    model.add(Dense(512, activation='relu', W_constraint=maxnorm(3))) \n",
    "    # model.add(Dropout(0.2)) \n",
    "    model.add(Dense(num_classes, activation='softmax')) \n",
    "\n",
    "    # Compile model \n",
    "    lrate = 0.1 \n",
    "    decay = lrate/n_epoch_cnn \n",
    "    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False) \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) \n",
    "    #print(model.summary()) \n",
    "    return model\n",
    "\n",
    "def recurrent():\n",
    "    model = Sequential() \n",
    "    model.add(LSTM(n_hidden_node, input_shape=(time_length, feature_dim))) \n",
    "    model.add(Dense(output_dim, activation='softmax')) \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "    #print(model.summary())  \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  Model Training  *********************\n",
      ">>> FNN Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Office\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(17, activation=\"relu\", kernel_initializer=\"normal\", input_dim=10121)`\n",
      "C:\\Users\\Office\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:23: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(17, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\Office\\Anaconda3\\lib\\site-packages\\keras\\models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10236/10236 [==============================] - 8s 760us/step - loss: 1.3913 - acc: 0.5491\n",
      "Epoch 2/10\n",
      "10236/10236 [==============================] - 7s 721us/step - loss: 0.7568 - acc: 0.7535\n",
      "Epoch 3/10\n",
      "10236/10236 [==============================] - 7s 725us/step - loss: 0.5317 - acc: 0.8302\n",
      "Epoch 4/10\n",
      "10236/10236 [==============================] - 7s 711us/step - loss: 0.3671 - acc: 0.8899\n",
      "Epoch 5/10\n",
      "10236/10236 [==============================] - 7s 710us/step - loss: 0.2443 - acc: 0.9301\n",
      "Epoch 6/10\n",
      "10236/10236 [==============================] - 7s 718us/step - loss: 0.1551 - acc: 0.9599\n",
      "Epoch 7/10\n",
      "10236/10236 [==============================] - 7s 711us/step - loss: 0.0968 - acc: 0.9777\n",
      "Epoch 8/10\n",
      "10236/10236 [==============================] - 7s 717us/step - loss: 0.0587 - acc: 0.9873\n",
      "Epoch 9/10\n",
      "10236/10236 [==============================] - 7s 710us/step - loss: 0.0362 - acc: 0.9938\n",
      "Epoch 10/10\n",
      "10236/10236 [==============================] - 7s 724us/step - loss: 0.0215 - acc: 0.9965\n",
      "\n",
      "\n",
      ">>> CNN Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Office\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:30: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(1, 101, 1...)`\n",
      "C:\\Users\\Office\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:32: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\Office\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:46: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, kernel_constraint=<keras.con..., activation=\"relu\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10236/10236 [==============================] - 559s 55ms/step - loss: 1.3846 - acc: 0.5582\n",
      "Epoch 2/3\n",
      "10236/10236 [==============================] - 566s 55ms/step - loss: 0.7645 - acc: 0.7440\n",
      "Epoch 3/3\n",
      "10236/10236 [==============================] - 562s 55ms/step - loss: 0.5606 - acc: 0.8114\n",
      "\n",
      "\n",
      ">>> RNN Training...\n",
      "Epoch 1/3\n",
      "10236/10236 [==============================] - 1260s 123ms/step - loss: 0.7783 - acc: 0.7585\n",
      "Epoch 2/3\n",
      "10236/10236 [==============================] - 1224s 120ms/step - loss: 0.1647 - acc: 0.9495\n",
      "Epoch 3/3\n",
      "10236/10236 [==============================] - 1169s 114ms/step - loss: 0.0437 - acc: 0.9879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x248ff987dd8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"*******************  Model Training  *********************\")\n",
    "#for name, model, X, Y in models:\n",
    "#     kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "#     cv_results = cross_val_score(model, unfolded_X_train, unfolded_Y_train, cv=kfold, scoring=scoring)\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "#     print(msg)\n",
    "\n",
    "# just fit\n",
    "print('>>> FNN Training...')\n",
    "FNN_model = feedforward()\n",
    "FNN_model.fit(X_train_FNN, Y_train_FNN, nb_epoch=n_epoch_fnn, batch_size=5, verbose=1)\n",
    "print('\\n')\n",
    "print('>>> CNN Training...')\n",
    "CNN_model = convolutional()\n",
    "CNN_model.fit(X_train_CNN, Y_train_CNN, nb_epoch=n_epoch_cnn, batch_size=64, verbose=1) \n",
    "print('\\n')\n",
    "print('>>> RNN Training...')   \n",
    "RNN_model = recurrent()\n",
    "RNN_model.fit(X_train_RNN, Y_train_RNN, nb_epoch=n_epoch_rnn, batch_size=batch_size, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_result(model, X_test, Y_test):\n",
    "    predictions = model.predict_classes(X_test, verbose=0)\n",
    "    print(accuracy_score(Y_test, predictions))\n",
    "    print(confusion_matrix(Y_test, predictions))\n",
    "    print('\\n')\n",
    "    print(classification_report(Y_test, predictions)) \n",
    "    \n",
    "    pre, rec, f1, sup = (precision_recall_fscore_support(Y_test, predictions, average='micro'))\n",
    "    print('micro => ', pre, '\\t', rec, '\\t', f1, '\\t', sup)\n",
    "    pre, rec, f1, sup = (precision_recall_fscore_support(Y_test, predictions, average='macro'))\n",
    "    print('macro => ', pre, '\\t', rec, '\\t', f1, '\\t', sup)\n",
    "    pre, rec, f1, sup = (precision_recall_fscore_support(Y_test, predictions, average='weighted'))\n",
    "    print('weighted => ',pre, '\\t', rec, '\\t', f1, '\\t', sup)\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  Model Evaluation  *********************\n",
      ">>> FNN Evaluation\n",
      "0.747356051704\n",
      "[[ 73   0   0   0   1   1   0   1  12   0   0   0   0   0   0   0   4]\n",
      " [  1   7   1   3   0   0   0   0   4   5   0   0   0   0   0   0   9]\n",
      " [  1   0 133   6   0   1   0   0   5   4   2   0   0   0   2   0   9]\n",
      " [  1   3   1 219   5   1   0   0  22  56   0   3   0   0   2   4  20]\n",
      " [  2   0   2  20 108   0   2   0   2  16   0   1   0   0   0   1   8]\n",
      " [  0   0   4   1   0 198   0   0   1   1   0   0   0   0   0   0   6]\n",
      " [  0   0   3   7   4   0 109   0   1   6   1   0   0   1   3   0   9]\n",
      " [  1   0   0   0   0   0   0 136   0   0   0   0   0   0   0   0   5]\n",
      " [  6   0   1  27   3   1   2   0 208  16   1   0   0   0   2   0  14]\n",
      " [  0   1   0  41   3   0   2   0  13 148   1   9   1   0   4  13  13]\n",
      " [  0   0   4   1   0   0   1   0   1   0   8   0   0   0   1   0   2]\n",
      " [  0   0   0  12   1   0   0   0   0  13   0  25   0   0   0   1   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   7   0   0   0   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  11   0   0   1]\n",
      " [  0   0   0   1   0   0   1   0   5   5   0   0   0   0  17   0   1]\n",
      " [  0   0   0  12   0   0   1   0   1  35   0   1   0   0   0  13   0]\n",
      " [  3   2   3  21   5   3   0   0  11  15   1   1   1   0   2   2 488]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.79      0.81        92\n",
      "          1       0.54      0.23      0.33        30\n",
      "          2       0.88      0.82      0.84       163\n",
      "          3       0.59      0.65      0.62       337\n",
      "          4       0.83      0.67      0.74       162\n",
      "          5       0.97      0.94      0.95       211\n",
      "          6       0.92      0.76      0.83       144\n",
      "          7       0.99      0.96      0.97       142\n",
      "          8       0.73      0.74      0.73       281\n",
      "          9       0.46      0.59      0.52       249\n",
      "         10       0.57      0.44      0.50        18\n",
      "         11       0.62      0.47      0.54        53\n",
      "         12       0.78      0.88      0.82         8\n",
      "         13       0.92      0.92      0.92        12\n",
      "         14       0.52      0.57      0.54        30\n",
      "         15       0.38      0.21      0.27        63\n",
      "         16       0.83      0.87      0.85       558\n",
      "\n",
      "avg / total       0.75      0.75      0.75      2553\n",
      "\n",
      "micro =>  0.747356051704 \t 0.747356051704 \t 0.747356051704 \t None\n",
      "macro =>  0.726483837727 \t 0.6766075567 \t 0.693369829637 \t None\n",
      "weighted =>  0.754735706973 \t 0.747356051704 \t 0.747083893544 \t None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">>> CNN1 Evaluation\n",
      "0.743439091265\n",
      "[[ 68   0   1   0   2   1   0   0  19   0   0   0   0   0   0   0   1]\n",
      " [  1   5   1  12   1   0   0   0   4   2   0   0   0   0   0   0   4]\n",
      " [  0   0 140   4   0   9   0   0   3   2   1   0   0   0   1   0   3]\n",
      " [  2   1   5 188  17   0   3   0  17  83   2   0   0   0   2   0  17]\n",
      " [  1   0   1  24 114   0   2   0   6   8   0   2   0   0   1   0   3]\n",
      " [  0   0   2   0   0 202   0   0   1   0   0   0   0   0   0   0   6]\n",
      " [  0   0   2   5   9   1 111   0   0   2   4   0   0   1   4   0   5]\n",
      " [  1   0   0   0   0   1   0 138   1   0   0   0   0   0   0   0   1]\n",
      " [  3   0   6  22   4   0   3   1 210  20   0   0   0   0   7   0   5]\n",
      " [  0   5   3  43   3   0   6   0   5 168   3   0   1   0   4   0   8]\n",
      " [  0   0   7   0   0   0   1   0   0   2   8   0   0   0   0   0   0]\n",
      " [  0   0   0  18   2   0   2   0   1  10   0  18   0   0   0   0   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   8   0   0   0   0]\n",
      " [  1   0   0   0   0   0   0   0   0   0   0   0   0  11   0   0   0]\n",
      " [  0   0   1   3   0   0   0   0   4   2   0   0   0   0  20   0   0]\n",
      " [  0   0   0   2   2   0   1   0   0  47   0   3   0   0   0   7   1]\n",
      " [  1   2   6   9  16   6   2   2  12  16   0   0   1   0   3   0 482]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.74      0.80        92\n",
      "          1       0.38      0.17      0.23        30\n",
      "          2       0.80      0.86      0.83       163\n",
      "          3       0.57      0.56      0.56       337\n",
      "          4       0.67      0.70      0.69       162\n",
      "          5       0.92      0.96      0.94       211\n",
      "          6       0.85      0.77      0.81       144\n",
      "          7       0.98      0.97      0.98       142\n",
      "          8       0.74      0.75      0.74       281\n",
      "          9       0.46      0.67      0.55       249\n",
      "         10       0.44      0.44      0.44        18\n",
      "         11       0.78      0.34      0.47        53\n",
      "         12       0.80      1.00      0.89         8\n",
      "         13       0.92      0.92      0.92        12\n",
      "         14       0.48      0.67      0.56        30\n",
      "         15       1.00      0.11      0.20        63\n",
      "         16       0.90      0.86      0.88       558\n",
      "\n",
      "avg / total       0.76      0.74      0.74      2553\n",
      "\n",
      "micro =>  0.743439091265 \t 0.743439091265 \t 0.743439091265 \t None\n",
      "macro =>  0.738993403499 \t 0.675918286621 \t 0.675571717441 \t None\n",
      "weighted =>  0.762104668916 \t 0.743439091265 \t 0.739057096516 \t None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">>> RNN Evaluation\n",
      "0.755581668625\n",
      "[[ 80   0   0   0   2   1   0   0   6   1   0   0   0   0   0   0   2]\n",
      " [  3   8   1   1   0   0   0   1   0   2   0   0   0   0   0   1  13]\n",
      " [  1   0 128   0   0   5   0   0   6   4   1   2   0   0   0   0  16]\n",
      " [  0   0   2 195   6   0   2   1  16  57   3   4   0   0   1   1  49]\n",
      " [  1   1   2  12 105   0   6   0   2  11   0   8   0   0   0   0  14]\n",
      " [  0   1   4   0   0 202   0   0   0   0   0   0   0   0   0   0   4]\n",
      " [  0   0   0   3   5   0 117   0   0   5   0   1   2   1   0   0  10]\n",
      " [  4   0   0   0   0   0   0 132   0   0   0   0   0   0   0   0   6]\n",
      " [  6   0   3  27   3   0   0   0 207  10   0   1   0   0   2   0  22]\n",
      " [  1   0   1  37   3   1   1   0   3 172   1   6   1   0   1   5  16]\n",
      " [  0   0   4   1   0   0   1   0   0   0  11   0   0   0   0   0   1]\n",
      " [  0   0   0  12   2   0   0   0   0  12   1  18   0   0   0   2   6]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   7   0   0   0   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  11   0   0   1]\n",
      " [  0   0   0   2   0   0   0   0   8   3   0   0   0   0  14   1   2]\n",
      " [  0   0   0  16   0   0   0   0   0  37   0   0   0   0   0  10   0]\n",
      " [  3   0   1  11   6   1   0   1   6  12   1   1   1   0   1   1 512]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.87      0.84        92\n",
      "          1       0.80      0.27      0.40        30\n",
      "          2       0.88      0.79      0.83       163\n",
      "          3       0.62      0.58      0.60       337\n",
      "          4       0.80      0.65      0.71       162\n",
      "          5       0.96      0.96      0.96       211\n",
      "          6       0.92      0.81      0.86       144\n",
      "          7       0.98      0.93      0.95       142\n",
      "          8       0.81      0.74      0.77       281\n",
      "          9       0.53      0.69      0.60       249\n",
      "         10       0.61      0.61      0.61        18\n",
      "         11       0.44      0.34      0.38        53\n",
      "         12       0.64      0.88      0.74         8\n",
      "         13       0.92      0.92      0.92        12\n",
      "         14       0.74      0.47      0.57        30\n",
      "         15       0.48      0.16      0.24        63\n",
      "         16       0.76      0.92      0.83       558\n",
      "\n",
      "avg / total       0.76      0.76      0.75      2553\n",
      "\n",
      "micro =>  0.755581668625 \t 0.755581668625 \t 0.755581668625 \t None\n",
      "macro =>  0.745506877446 \t 0.680028963658 \t 0.694862250457 \t None\n",
      "weighted =>  0.759098219484 \t 0.755581668625 \t 0.749356821836 \t None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"*******************  Model Evaluation  *********************\")\n",
    "\n",
    "print('>>> FNN Evaluation')\n",
    "show_result(FNN_model, X_test_FNN, dummy_to_integer(Y_test_FNN))\n",
    "#scores = model.evaluate(X_test_FNN, Y_test_FNN, verbose=0)\n",
    "#print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    \n",
    "print('\\n')\n",
    "print('>>> CNN1 Evaluation')\n",
    "show_result(CNN_model, X_test_CNN, dummy_to_integer(Y_test_CNN))\n",
    "#scores = model.evaluate(X_test_CNN, Y_test_CNN, verbose=0) \n",
    "#print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "print('\\n')\n",
    "print('>>> RNN Evaluation')    \n",
    "show_result(RNN_model, X_test_RNN, dummy_to_integer(Y_test_RNN))\n",
    "#scores = model.evaluate(X_test_RNN, Y_test_RNN, verbose=1) \n",
    "#print(\"\\nModel Accuracy: %.2f%%\" % (scores[1]*100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# *******************  Model Evaluation  *********************\n",
    "# >>> FNN Evaluation\n",
    "# (2583,)\n",
    "# 0.730158730159\n",
    "# [[ 57   1   0   0   0   1   0   1   5   1   0   0   0   0   0   0   6]\n",
    "#  [  0   5   0   5   0   0   0   0   1   5   0   0   0   0   0   0   4]\n",
    "#  [  0   0 144   3   0   3   4   0   3   2   4   0   0   0   0   0   8]\n",
    "#  [  0   0   1 187   9   0   3   0   6  54   0   8   0   0   3  12  25]\n",
    "#  [  0   0   2  19  73   0  19   1   1  11   0   1   0   0   0   1   9]\n",
    "#  [  2   0   3   0   0 230   2   0   0   0   0   0   1   0   0   0  13]\n",
    "#  [  0   0   0   5  10   0 100   0   0   4   1   3   0   0   3   0   6]\n",
    "#  [  5   0   0   0   0   0   0 144   0   0   0   0   0   0   0   0   2]\n",
    "#  [  6   0   2  16   2   2   8   1 204   3   0   0   0   0   6   1  12]\n",
    "#  [  1   6   4  59   2   1  16   0   6 135   2  13   1   0   0   3  33]\n",
    "#  [  0   0   4   4   0   0   1   0   0   1   8   0   0   0   0   0   3]\n",
    "#  [  0   0   0   7   0   0   1   0   2   5   0   4   0   0   0   0   6]\n",
    "#  [  0   0   0   0   0   1   2   0   0   0   0   0   7   1   0   0   2]\n",
    "#  [  0   0   0   0   0   0   0   3   0   1   0   0   0  10   0   0   0]\n",
    "#  [  0   0   0   3   0   0   0   0   6   7   0   0   0   0  13   1   3]\n",
    "#  [  0   0   0   7   0   0   0   0   3  38   0   4   0   0   0  38   5]\n",
    "#  [  2   0   4  13   1   9   7   1  11  17   0   1   0   0   0   2 527]]\n",
    "\n",
    "\n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#           0       0.78      0.79      0.79        72\n",
    "#           1       0.42      0.25      0.31        20\n",
    "#           2       0.88      0.84      0.86       171\n",
    "#           3       0.57      0.61      0.59       308\n",
    "#           4       0.75      0.53      0.62       137\n",
    "#           5       0.93      0.92      0.92       251\n",
    "#           6       0.61      0.76      0.68       132\n",
    "#           7       0.95      0.95      0.95       151\n",
    "#           8       0.82      0.78      0.80       263\n",
    "#           9       0.48      0.48      0.48       282\n",
    "#          10       0.53      0.38      0.44        21\n",
    "#          11       0.12      0.16      0.14        25\n",
    "#          12       0.78      0.54      0.64        13\n",
    "#          13       0.91      0.71      0.80        14\n",
    "#          14       0.52      0.39      0.45        33\n",
    "#          15       0.66      0.40      0.50        95\n",
    "#          16       0.79      0.89      0.84       595\n",
    "\n",
    "# avg / total       0.73      0.73      0.73      2583\n",
    "\n",
    "\n",
    "\n",
    "# >>> CNN1 Evaluation\n",
    "# (2583,)\n",
    "# 0.708091366628\n",
    "# [[ 54   2   2   0   1   0   0   1   6   3   0   0   0   0   0   0   3]\n",
    "#  [  0   6   0   3   0   1   0   0   0   9   0   0   0   0   0   0   1]\n",
    "#  [  0   0 155   3   0   0   3   0   1   6   0   0   0   0   0   0   3]\n",
    "#  [  0   1   4 138   6   0   4   0   6 124   0   9   0   0   0   0  16]\n",
    "#  [  0   4   1  12  73   1  15   0   4  16   0   1   0   0   0   0  10]\n",
    "#  [  2   0   7   0   1 229   0   0   0   0   0   0   0   0   0   0  12]\n",
    "#  [  0   1   0  11  13   2  82   0   0  12   2   0   0   0   1   0   8]\n",
    "#  [  5   0   0   0   0   0   0 144   0   0   0   0   0   0   0   0   2]\n",
    "#  [  9   1  10   5   3   1   6   0 183  25   0   0   0   0   3   0  17]\n",
    "#  [  1   7   6  46   1   0   3   0   2 197   0   2   0   0   0   0  17]\n",
    "#  [  0   0   8   2   0   0   0   0   1   4   5   0   0   0   0   0   1]\n",
    "#  [  0   0   0   8   1   0   0   0   0  11   0   2   0   0   0   0   3]\n",
    "#  [  0   0   0   0   0   1   3   0   0   1   0   0   6   1   0   0   1]\n",
    "#  [  0   0   0   0   0   0   2   2   0   2   0   0   0   8   0   0   0]\n",
    "#  [  0   0   0   1   1   0   2   0   4  11   0   0   0   0  12   0   2]\n",
    "#  [  0   0   0   6   0   0   2   0   0  74   0   0   0   0   0   6   7]\n",
    "#  [  2   0   7  12   1  10   2   2   1  27   0   2   0   0   0   0 529]]\n",
    "\n",
    "\n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#           0       0.74      0.75      0.74        72\n",
    "#           1       0.27      0.30      0.29        20\n",
    "#           2       0.78      0.91      0.84       171\n",
    "#           3       0.56      0.45      0.50       308\n",
    "#           4       0.72      0.53      0.61       137\n",
    "#           5       0.93      0.91      0.92       251\n",
    "#           6       0.66      0.62      0.64       132\n",
    "#           7       0.97      0.95      0.96       151\n",
    "#           8       0.88      0.70      0.78       263\n",
    "#           9       0.38      0.70      0.49       282\n",
    "#          10       0.71      0.24      0.36        21\n",
    "#          11       0.12      0.08      0.10        25\n",
    "#          12       1.00      0.46      0.63        13\n",
    "#          13       0.89      0.57      0.70        14\n",
    "#          14       0.75      0.36      0.49        33\n",
    "#          15       1.00      0.06      0.12        95\n",
    "#          16       0.84      0.89      0.86       595\n",
    "\n",
    "# avg / total       0.75      0.71      0.70      2583\n",
    "\n",
    "\n",
    "\n",
    "# >>> RNN Evaluation\n",
    "# (2583,)\n",
    "# 0.739450251645\n",
    "# [[ 59   1   0   0   0   0   0   2   5   0   0   0   0   0   0   0   5]\n",
    "#  [  0   1   1   4   0   0   0   0   1   7   0   0   0   0   0   0   6]\n",
    "#  [  0   0 152   1   0   0   3   0   4   2   3   0   0   0   1   0   5]\n",
    "#  [  1   0   2 179  11   0   2   0  14  58   1  16   0   0   1   0  23]\n",
    "#  [  1   1   1  10  79   0  12   0   2  10   1   1   0   0   1   0  18]\n",
    "#  [  2   0   7   0   0 212   1   0   0   2   0   0   0   0   0   1  26]\n",
    "#  [  0   0   0   6   9   0 100   0   0   3   2   3   0   0   3   1   5]\n",
    "#  [  4   0   0   0   0   0   0 145   0   0   0   0   0   0   0   0   2]\n",
    "#  [  6   0   4  14   7   1   5   0 209   6   1   2   0   0   3   0   5]\n",
    "#  [  0   5   5  47   1   0   6   0   9 150   2  11   0   0   3   5  38]\n",
    "#  [  0   0   4   4   0   0   0   0   1   3   8   0   0   0   0   0   1]\n",
    "#  [  0   0   0   5   0   0   0   0   2   7   0   6   0   0   1   0   4]\n",
    "#  [  0   0   0   0   0   1   2   0   0   0   0   0   7   1   0   0   2]\n",
    "#  [  0   0   0   0   0   0   0   2   0   0   0   1   0  10   0   0   1]\n",
    "#  [  0   0   0   3   1   0   0   0  10   3   0   0   0   0  14   0   2]\n",
    "#  [  0   0   0   9   0   0   0   0   2  40   0   2   0   0   0  37   5]\n",
    "#  [  0   0  10   9   2   5   4   1   6  11   1   2   0   0   0   2 542]]\n",
    "\n",
    "\n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#           0       0.81      0.82      0.81        72\n",
    "#           1       0.12      0.05      0.07        20\n",
    "#           2       0.82      0.89      0.85       171\n",
    "#           3       0.62      0.58      0.60       308\n",
    "#           4       0.72      0.58      0.64       137\n",
    "#           5       0.97      0.84      0.90       251\n",
    "#           6       0.74      0.76      0.75       132\n",
    "#           7       0.97      0.96      0.96       151\n",
    "#           8       0.79      0.79      0.79       263\n",
    "#           9       0.50      0.53      0.51       282\n",
    "#          10       0.42      0.38      0.40        21\n",
    "#          11       0.14      0.24      0.17        25\n",
    "#          12       1.00      0.54      0.70        13\n",
    "#          13       0.91      0.71      0.80        14\n",
    "#          14       0.52      0.42      0.47        33\n",
    "#          15       0.80      0.39      0.52        95\n",
    "#          16       0.79      0.91      0.84       595\n",
    "\n",
    "# avg / total       0.74      0.74      0.74      2583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# >>> FNN Evaluation\n",
    "# 0.733643050716\n",
    "# [[ 55   0   1   0   0   1   0   1   9   0   0   0   0   0   0   0   5]\n",
    "#  [  1   5   0   3   1   0   0   0   1   6   0   0   0   0   0   0   3]\n",
    "#  [  0   0 153   0   0   1   1   0   7   3   2   0   0   0   0   0   4]\n",
    "#  [  0   0   1 159  17   0   3   0  17  64   1   5   0   0   3  16  22]\n",
    "#  [  0   0   2  13  87   0  11   0   1  11   0   1   0   0   1   0  10]\n",
    "#  [  2   0   6   0   0 227   1   0   0   0   0   0   1   0   0   0  14]\n",
    "#  [  0   0   0   5  11   0  97   0   1   5   2   3   0   0   2   0   6]\n",
    "#  [  5   0   0   0   0   0   0 146   0   0   0   0   0   0   0   0   0]\n",
    "#  [  7   0   3  14   4   2   3   0 210   2   0   0   0   0   8   0  10]\n",
    "#  [  1   4   4  46   1   1   8   0   7 147   3   9   1   0   6   9  35]\n",
    "#  [  0   0   6   2   0   0   1   0   0   2   7   0   0   0   0   0   3]\n",
    "#  [  0   0   0   5   0   0   1   0   3   5   0   4   0   0   1   0   6]\n",
    "#  [  0   0   0   0   0   1   2   0   0   1   0   0   7   1   0   0   1]\n",
    "#  [  0   0   0   0   0   0   0   2   0   1   0   0   0  10   0   0   1]\n",
    "#  [  0   0   0   3   0   0   0   0   7   5   0   0   0   0  15   0   3]\n",
    "#  [  0   0   0   3   0   0   0   0   3  42   0   1   0   0   0  41   5]\n",
    "#  [  3   0   5   7   3   6   6   3  12  20   0   0   0   0   2   3 525]]\n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#           0       0.74      0.76      0.75        72\n",
    "#           1       0.56      0.25      0.34        20\n",
    "#           2       0.85      0.89      0.87       171\n",
    "#           3       0.61      0.52      0.56       308\n",
    "#           4       0.70      0.64      0.67       137\n",
    "#           5       0.95      0.90      0.93       251\n",
    "#           6       0.72      0.73      0.73       132\n",
    "#           7       0.96      0.97      0.96       151\n",
    "#           8       0.76      0.80      0.78       263\n",
    "#           9       0.47      0.52      0.49       282\n",
    "#          10       0.47      0.33      0.39        21\n",
    "#          11       0.17      0.16      0.17        25\n",
    "#          12       0.78      0.54      0.64        13\n",
    "#          13       0.91      0.71      0.80        14\n",
    "#          14       0.39      0.45      0.42        33\n",
    "#          15       0.59      0.43      0.50        95\n",
    "#          16       0.80      0.88      0.84       595\n",
    "\n",
    "# avg / total       0.73      0.73      0.73      2583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hidden node 2: 54.08%\n",
    "# hidden node 17: 65.69%\n",
    "# hidden node 30: 66.03%\n",
    "# hidden node 100: 66.97%\n",
    "\n",
    "# RNN : parsing tree feature 추가하니까 67.68% 조금 올랐다. 반면, LR은 조금 떨어짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proper way to evaluate precision and recall is to use `preds = model.predict(X_test)` and then compute `precision(preds, y_test)` via e.g. a sklearn utility function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # fix random seed for reproducibility \n",
    "# seed = 7 \n",
    "# numpy.random.seed(seed) \n",
    "# # load the dataset but only keep the top n words, zero the rest \n",
    "# top_words = 5000 \n",
    "# (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words) \n",
    "# # pad dataset to a maximum review length in words \n",
    "# max_words = 50 \n",
    "# X_train = sequence.pad_sequences(X_train, maxlen=max_words) \n",
    "# X_test = sequence.pad_sequences(X_test, maxlen=max_words) \n",
    "# print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# temp_x = [[[3],[2],[1]],[[3],[2],[1]],[[3],[2],[1]],[[3],[2],[1]],[[3],[2],[1]]]\n",
    "# temp_x = [[[3]],[[1]],[[1]],[[1]],[[1]]]\n",
    "# temp_x = [[[2],[1]],[[2],[1]],[[2],[1]],[[2],[1]],[[2],[1]]]\n",
    "# temp_x = numpy.array(temp_x)\n",
    "# temp_y = [1, 0, 0, 0, 1]\n",
    "# temp_y = numpy.array(temp_y)\n",
    "# print(temp_x.shape, temp_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # create the model \n",
    "# model = Sequential() \n",
    "# #model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "# #model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "\n",
    "# #model.add(Convolution1D(64, 3, border_mode='same', input_shape=(3, 1)))\n",
    "# model.add(Convolution1D(nb_filter=32, filter_length=2, border_mode='same', activation='relu', input_shape=(2, 1)))\n",
    " \n",
    "# model.add(MaxPooling1D(pool_length=2)) \n",
    "# model.add(Flatten()) \n",
    "\n",
    "# model.add(Dense(250, activation='relu')) \n",
    "# model.add(Dense(1, activation='sigmoid')) \n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "# print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Fit the model \n",
    "# model.fit(temp_x, dummy_y, validation_data=(temp_x, dummy_y), nb_epoch=2, batch_size=128, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Final evaluation of the model \n",
    "# scores = model.evaluate(temp_x, temp_y, verbose=0) \n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RNN\n",
    "\n",
    "* RNN (LSTM) 모델에 대한 engineering도 나중에 해보자.\n",
    "   - 원래는 validation set와 함께 학습을 통해서 model capacity와 같은 parameter를 결정해야되지만 데이터가 작으면 그냥 하나씩 일일이 해보면서 제일 좋은 parameter를 찾아내는 것도 괜찮은 방법이라 생각한다.\n",
    "* Bi-directional LSTM은 어떨까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # summarize performance of the model \n",
    "# scores = model.evaluate(X_test_RNN, Y_test_RNN, verbose=0) \n",
    "# print(\"Model Accuracy: %.2f%%\" % (scores[1]*100)) \n",
    "# # # demonstrate some model predictions \n",
    "# # for i in range(20): \n",
    "# #     pattern_index = numpy.random.randint(len(dataX)) \n",
    "# #     pattern = dataX[pattern_index] \n",
    "# #     x = pad_sequences([pattern], maxlen=max_len, dtype='float32') \n",
    "# #     x = numpy.reshape(x, (1, max_len, 1)) \n",
    "# #     x = x / float(len(alphabet)) \n",
    "# #     prediction = model.predict(x, verbose=0) \n",
    "# #     index = numpy.argmax(prediction) \n",
    "# #     result = int_to_char[index] \n",
    "# #     seq_in = [int_to_char[value] for value in pattern] \n",
    "# #     print(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# statistical\n",
    "66.97 # all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## # fix random seed for reproducibility \n",
    "# numpy.random.seed(7) \n",
    "# # define the raw dataset \n",
    "# alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \n",
    "# # create mapping of characters to integers (0-25) and the reverse \n",
    "# char_to_int = dict((c, i) for i, c in enumerate(alphabet)) \n",
    "# int_to_char = dict((i, c) for i, c in enumerate(alphabet)) \n",
    "# # prepare the dataset of input to output pairs encoded as integers \n",
    "# num_inputs = 1000 \n",
    "# max_len = 3 \n",
    "# dataX = [] \n",
    "# dataY = [] \n",
    "# for i in range(num_inputs): \n",
    "#     start = numpy.random.randint(len(alphabet)-2) \n",
    "#     end = numpy.random.randint(start, min(start+max_len,len(alphabet)-1)) \n",
    "#     sequence_in = alphabet[start:end+1] \n",
    "#     sequence_out = alphabet[end + 1] \n",
    "#     dataX.append([char_to_int[char] for char in sequence_in]) \n",
    "#     dataY.append(char_to_int[sequence_out]) \n",
    "#     print(sequence_in, '->', sequence_out) \n",
    "    \n",
    "# # convert list of lists to array and pad sequences if needed \n",
    "# X = pad_sequences(dataX, maxlen=max_len, dtype='float32') \n",
    "# # reshape X to be [samples, time steps, features] \n",
    "# X = numpy.reshape(X, (X.shape[0], max_len, 1)) \n",
    "\n",
    "# # normalize \n",
    "# X = X / float(len(alphabet)) \n",
    "# # one hot encode the output variable \n",
    "# y = np_utils.to_categorical(dataY) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
