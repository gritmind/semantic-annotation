{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 규칙의 역할: 특정 문제에만 집중적으로 공격(엔지니어링)\n",
    "조금 어거지일수도 있다는 느낌이 들수도 있음 (general하지 않고 specific)\n",
    "그래서 특정 문제에만 적용가능한 점으로 제한을 둔다.\n",
    "<br>\n",
    "parser를 통해서 grouping하는 것까지 했다면, 해당 grouping의 정체?를 밝혀주는게 규칙기반 feature의 역할이다.\n",
    "예를 들어, 부사절과 주절을 따로 grouping을 했지만 부사절의 종류(if문인지, so that문인지)를 구분하는 방법이 필요하다. 여기서 간단히 규칙칙으로 if, so that의 거리를 측정해서 이들을 구별하고자 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Office\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\regexp.py:123: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return [tok for tok in self._regexp.split(text) if tok]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from data_handlder import *\n",
    "\n",
    "import collections\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import treebank\n",
    "from data_handling import *\n",
    "from nltk import chunk\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "from feature_design_for_linguistic import *\n",
    "from feature_design_for_rules import *\n",
    "\n",
    "from chunkers import ClassifierChunker\n",
    "from nltk.corpus import treebank_chunk \n",
    "from chunkers import TagChunker \n",
    "train_chunks = treebank_chunk.chunked_sents()[:3000] # training data\n",
    "# test_chunks = treebank_chunk.chunked_sents()[3000:] # testing data\n",
    "# score = chunker.evaluate(test_chunks)\n",
    "# score.accuracy() # 0.9721733155838022\n",
    "chunker = ClassifierChunker(train_chunks) # a classifier trained by treebank dataset\n",
    "detokenizer = MosesDetokenizer()\n",
    "\n",
    "import os\n",
    "os.environ['STANFORD_PARSER'] = 'C:\\\\stanford-parser-full-2016-10-31\\\\stanford-parser.jar'\n",
    "os.environ['STANFORD_MODELS'] = 'C:\\\\stanford-parser-full-2016-10-31\\\\stanford-parser-3.7.0-models.jar'\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "parser = StanfordParser(model_path='edu\\\\stanford\\\\nlp\\\\models\\\\lexparser\\\\englishPCFG.ser.gz')\n",
    "\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "dep_parser = StanfordDependencyParser(model_path=\"edu\\\\stanford\\\\nlp\\\\models\\\\lexparser\\\\englishPCFG.ser.gz\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pre_X_train = load('pre_X_train') # preprocessed text data\n",
    "pre_X_test = load('pre_X_test') # preprocessed text data\n",
    "\n",
    "### FE2_X_train와 FE2_X_test 초기화\n",
    "FE3_X_train = []\n",
    "for sentence in (pre_X_train):\n",
    "    temp = []\n",
    "    for token in (sentence):\n",
    "        temp.append([])\n",
    "    FE3_X_train.append(temp)\n",
    "\n",
    "FE3_X_test = []\n",
    "for sentence in (pre_X_test):\n",
    "    temp = []\n",
    "    for token in (sentence):\n",
    "        temp.append([])\n",
    "    FE3_X_test.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 일반동사가 ROOT가 되지 않는 경우들...\n",
    "# be able to\n",
    "# it would be\n",
    "# would be nice\n",
    "# it is possible\n",
    "# want\n",
    "# will be nice\n",
    "# could be nice\n",
    "# will really nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall search model\n"
     ]
    }
   ],
   "source": [
    "li_test = 'shall search for model'\n",
    "test = 'it would be nice if you be a good product.'\n",
    "find_phrase = 'be nice'\n",
    "test.find(find_phrase)\n",
    "len(find_phrase)\n",
    "\n",
    "import re\n",
    "li_test = re.sub('search for', 'search',li_test)\n",
    "print(li_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'along', 'word'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_word_list = 'word some one long two phrase three about above along after against'\n",
    "long_word_set = set(long_word_list.split())\n",
    "set('word along river'.split()) & long_word_set\n",
    "#set(['along'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Rule based Feature Extraction\n",
    "* itwouldbenice 종류의 패턴이 있는지 없는지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(pre_X_train): # for training data\n",
    "    detokenized_sent = detokenizer.detokenize(sentence, return_str=True)\n",
    "    \n",
    "    for j, token in enumerate(sentence):\n",
    "        \n",
    "        FE3_X_train[i][j] += check_wouldbenice_pattern(detokenized_sent)\n",
    "        FE3_X_train[i][j] += is_auxVerb_leftside(j, sentence)\n",
    "        FE3_X_train[i][j] += is_sothatORif_leftside(j, sentence)\n",
    "\n",
    "        # for sub action, sub obect, ... \n",
    "        FE3_X_train[i][j] += is_causative_verb(j, sentence) # 사역동사\n",
    "        FE3_X_train[i][j] += is_abstract_noun(j, sentence) # 추상명사\n",
    "        \n",
    "        \n",
    "for i, sentence in enumerate(pre_X_test): # for training data\n",
    "    detokenized_sent = detokenizer.detokenize(sentence, return_str=True)\n",
    "    \n",
    "    for j, token in enumerate(sentence):\n",
    "        \n",
    "        FE3_X_test[i][j] += check_wouldbenice_pattern(detokenized_sent)\n",
    "        FE3_X_test[i][j] += is_auxVerb_leftside(j, sentence)\n",
    "        FE3_X_test[i][j] += is_sothatORif_leftside(j, sentence)\n",
    "        \n",
    "        # for sub action, sub obect, ... \n",
    "        FE3_X_test[i][j] += is_causative_verb(j, sentence) # 사역동사\n",
    "        FE3_X_test[i][j] += is_abstract_noun(j, sentence) # 추상명사  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i, sentence in enumerate(pre_X_train): # for training data\n",
    "#     index_which = position_which(sentence)\n",
    "#     index_sothat = position_sothat(sentence)\n",
    "#     for j, token in enumerate(sentence):\n",
    "#         #FE3_X_train[i][j] = convert_dummy((j+1) / len(sentence)) # position in a sentence\n",
    "#         #FE3_X_train[i][j] = vectorized_token_using_prep_lookup(j, sentence, lookup_prep)# prep position\n",
    "#         #FE3_X_train[i][j] = FE_which_left_right(j, index_which)\n",
    "#         #FE3_X_train[i][j] = FE_sothat_left_right(j, index_sothat)\n",
    "    \n",
    "# for i, sentence in enumerate(pre_X_test): # for training data\n",
    "#     index_which = position_which(sentence)\n",
    "#     index_sothat = position_sothat(sentence)\n",
    "#     for j, token in enumerate(sentence):\n",
    "#         #FE3_X_test[i][j] = convert_dummy((j+1) / len(sentence))\n",
    "#         #FE3_X_test[i][j] = vectorized_token_using_prep_lookup(j, sentence, lookup_prep)\n",
    "#         #FE3_X_test[i][j] = FE_which_left_right(j, index_which)\n",
    "#         #FE3_X_test[i][j] = FE_sothat_left_right(j, index_sothat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Priority Class\n",
    "\n",
    "해당 토큰이 조동사인지 아닌지...\n",
    "조동사에는 다음과 같이 3가지 종류가 있는데, 우리는 두 번째 modal auxiliary verbs만 사용하기로.. 다른 것들은 중복여지가 있다.\n",
    "   1. Primary auxiliary verbs\n",
    "      - be / do / have\n",
    "   2. Modal auxiliary verbs\n",
    "      - can / could / will / would / shall / should / must / may / might\n",
    "   3. Semi-modal auxiliary verbs\n",
    "      - ought to / used to / need / dare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# auxiliary_verb_set = ['shall', 'can', 'want', 'would', 'could', 'should', 'must', 'will', 'may', 'might']\n",
    "\n",
    "# for i, sentence in enumerate(pre_X_train): # for training data\n",
    "#     for j, token in enumerate(sentence):\n",
    "#         if any(token in t for t in auxiliary_verb_set):\n",
    "#             FE3_X_train[i][j] = [1] # just 1 bit\n",
    "#         else:\n",
    "#             FE3_X_train[i][j] = [0]\n",
    "            \n",
    "# for i, sentence in enumerate(pre_X_test): # for training data\n",
    "#     for j, token in enumerate(sentence):\n",
    "#         if any(token in t for t in auxiliary_verb_set):\n",
    "#             FE3_X_test[i][j] = [1]\n",
    "#         else:\n",
    "#             FE3_X_test[i][j] = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clause_set = ['that', 'which']\n",
    "\n",
    "# for i, sentence in enumerate(pre_X_train): # for training data\n",
    "#     clasue = False\n",
    "#     for j, token in enumerate(sentence):\n",
    "#         if any(token in t for t in clause_set):\n",
    "#             clause = True\n",
    "        \n",
    "#         if clause == True:\n",
    "#             FE3_X_train[i][j] = [1] \n",
    "#         else:\n",
    "#             FE3_X_train[i][j] = [0]\n",
    "            \n",
    "            \n",
    "# for i, sentence in enumerate(pre_X_test): # for training data\n",
    "#     clasue = False\n",
    "#     for j, token in enumerate(sentence):\n",
    "#         if any(token in t for t in clause_set):\n",
    "#             clause = True\n",
    "        \n",
    "#         if clause == True:\n",
    "#             FE3_X_test[i][j] = [1] \n",
    "#         else:\n",
    "#             FE3_X_test[i][j] = [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Total feature length = 6\n"
     ]
    }
   ],
   "source": [
    "dump(FE3_X_train, 'FE3_X_train')\n",
    "dump(FE3_X_test, 'FE3_X_test')\n",
    "print('===> Total feature length =', len(FE3_X_train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
