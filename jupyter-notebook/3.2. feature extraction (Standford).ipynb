{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Office\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\regexp.py:123: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return [tok for tok in self._regexp.split(text) if tok]\n"
     ]
    }
   ],
   "source": [
    "real_window_size_pos = 9\n",
    "real_window_size_np_chunk = 0\n",
    "real_window_size_n_dep = 5\n",
    "real_window_size_depth = 5\n",
    "real_window_size_n_siblings = 5 \n",
    "\n",
    "using_pos = 0\n",
    "#using_chunking = 1\n",
    "using_parsing = 1\n",
    "#using_dependency = 1\n",
    "#################################################################################################################\n",
    "window_size_pos = (real_window_size_pos * 2) + 1\n",
    "window_size_np_chunk = (real_window_size_np_chunk * 2) + 1\n",
    "window_size_n_dep = (real_window_size_n_dep * 2) + 1\n",
    "window_size_depth = (real_window_size_depth * 2) + 1\n",
    "window_size_n_siblings = (real_window_size_n_siblings * 2) + 1\n",
    "\n",
    "\n",
    "import collections\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import treebank\n",
    "from data_handler import *\n",
    "from nltk import chunk\n",
    "from feature_design_for_linguistic import *\n",
    "\n",
    "from chunkers import ClassifierChunker\n",
    "from nltk.corpus import treebank_chunk \n",
    "from chunkers import TagChunker \n",
    "train_chunks = treebank_chunk.chunked_sents()[:3000] # training data\n",
    "# test_chunks = treebank_chunk.chunked_sents()[3000:] # testing data\n",
    "# score = chunker.evaluate(test_chunks)\n",
    "# score.accuracy() # 0.9721733155838022\n",
    "chunker = ClassifierChunker(train_chunks) # a classifier trained by treebank dataset\n",
    "\n",
    "import os\n",
    "os.environ['STANFORD_PARSER'] = 'C:\\\\stanford-parser-full-2016-10-31\\\\stanford-parser.jar'\n",
    "os.environ['STANFORD_MODELS'] = 'C:\\\\stanford-parser-full-2016-10-31\\\\stanford-parser-3.7.0-models.jar'\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "parser = StanfordParser(model_path='edu\\\\stanford\\\\nlp\\\\models\\\\lexparser\\\\englishPCFG.ser.gz')\n",
    "\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "dep_parser = StanfordDependencyParser(model_path=\"edu\\\\stanford\\\\nlp\\\\models\\\\lexparser\\\\englishPCFG.ser.gz\")\n",
    "\n",
    "#################################################################################################################\n",
    "# pre_X_train = load('pre_X_train') # preprocessed text data\n",
    "# pre_X_test = load('pre_X_test') # preprocessed text data\n",
    "pre_X_train = load('X_train') \n",
    "pre_X_test = load('X_test') \n",
    "Y_train = load('pre_Y_train')\n",
    "\n",
    "### FE2_X_train와 FE2_X_test 초기화\n",
    "FE2_X_train = []\n",
    "for sentence in (pre_X_train):\n",
    "    temp1 = []\n",
    "    for token in (sentence):\n",
    "        temp1.append([])\n",
    "    FE2_X_train.append(temp1)\n",
    "\n",
    "FE2_X_test = []\n",
    "for sentence in (pre_X_test):\n",
    "    temp2 = []\n",
    "    for token in (sentence):\n",
    "        temp2.append([])\n",
    "    FE2_X_test.append(temp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Part of speech (POS)\n",
    "\n",
    "* POS feature 역시 사전-based feature와 같이 binary feature로 구성하면 될 것 같다. NLTK에서 제공하는 POS 종류는 45개가 있다. 그런데, 45종류를 모두 포함해버리면 또 sparsity problem이 생길 것 같다. (물론 여태까지 그런 문제가 발생하기 보다는 성능이 좋아졌지만...)\n",
    "* 일단은 모두 포함하는 걸로 하고, 나중에 best feature set을 찾을 때, 비슷한 것끼리 묶는 법도 생각해보자.\n",
    "* 다른 사전들과는 다르게 POS 사전은 unknown_token이 없다.\n",
    "* 45개 모두 사용한 POS 사전의 성능은 41%정도 밖에 안된다. information gain이 크진 않다. 나중에 45개 범주를 더 줄이는 방안도 고려해봐야겠다.\n",
    "* window size를 적용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(lookup_table_POS) =  37\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "\"\"\" CREATE POS LOOKUP TABLE \"\"\"\n",
    "###############################\n",
    "# voca_set_POS = ['$', \"''\", '(', ')', ',', '--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', \n",
    "#                 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM',\n",
    "#                 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
    "list_POS = [\n",
    "    'DT','JJ','NN','MD','VB',',','CC','IN','.','VBG','VBZ','VBN','TO','WRB','NNS','RB','PRP','VBP','PRP$','CD',\n",
    "    'NNP','EX','VBD','JJR',':','WDT','RP','NNPS','POS',\"''\",'RBR','WP','PDT','JJS','RBS','$','FW'\n",
    "]\n",
    "\n",
    "lookup_table_POS = { }\n",
    "for i, token in enumerate(list_POS):\n",
    "    lookup_table_POS[token] = i # POS에서는 1부터(i+1) 시작안해도 된다. POS에는 unknown이 없기 때문이다.\n",
    "# Add unknown token\n",
    "print('len(lookup_table_POS) = ', len(lookup_table_POS))\n",
    "\n",
    "##############################\n",
    "\"\"\" POS FEATURE EXTRACTION \"\"\"\n",
    "##############################\n",
    "if using_pos == 1:    \n",
    "    # For training data\n",
    "    for i, sentence in enumerate(pre_X_train):\n",
    "        pos_tagged_sentence = nltk.pos_tag(sentence)\n",
    "        #print(pos_tagged_sentence)\n",
    "        for j, tuple_ in enumerate(pos_tagged_sentence): # pos_sentence\n",
    "            FE2_X_train[i][j] += FE_POS(j, pos_tagged_sentence, window_size_pos, lookup_table_POS)        \n",
    "            \n",
    "    # For testing data\n",
    "    for i, sentence in enumerate(pre_X_test):\n",
    "        pos_tagged_sentence = nltk.pos_tag(sentence)\n",
    "        for j, tuple_ in enumerate(pos_tagged_sentence): # pos_sentence\n",
    "            FE2_X_test[i][j] += FE_POS(j, pos_tagged_sentence, window_size_pos, lookup_table_POS)\n",
    "\n",
    "    print('window size pos = ', window_size_pos)\n",
    "    print('===> current feature length', len(FE2_X_train[0][0]))\n",
    "    len_pos_feature = len(FE2_X_train[0][0])\n",
    "    assert(len(FE2_X_train[0][0]) == len(FE2_X_test[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Chunking (shallow parsing)\n",
    "\n",
    "* NP를 기반으로 window size = 3으로 binary value로 feature를 구성해보자.\n",
    "\n",
    "Parse tree 구조는 다음과 같다. <br>\n",
    "가장 작은 단위로 묶는 단위는 POS tag이다. e.g., IN, VBP, DT, NN 등등 <br>\n",
    "그 다음으로 큰 단위는 chunk 단위이다. e.g, NP (Noun Phrase)\n",
    "chuck 단위에서 왜 VP (Verb Phrase)와 PP (Prepositional Phrase)는 언급하지 않나? 이유는 VP와 PP는 NP를 포함할 수 있기 때문이다.\n",
    "따라서, NP와 동등한 단위가 될 수 없다. 물론 NP가 VP 또는 PP와 동등한 단위가 될 수도 있다. 여기서 이야기 하는 바는 최소한의 단위이다. <br>\n",
    "NP가 가장 작은 단위의 chunk이고, PP와 VP는 그 상위 레벨에 있는 기준이다. 따라서 이들과 구분하는 것이 좋다. <br>\n",
    "따라서 나는 PP와 VP는 chunking의 다음 단계인 full parsing에서 다루도록 하겠다. <br><br>\n",
    "\n",
    "NLTK 라이브러리에 있는 chunking 도구 중에서 규칙으로 (사람이 메뉴얼로 작성) 만들 수 있는 chunking이 있다. 여기에 VP, NP, PP를 구성할 수도 있다. 이렇게되면 chunking으로 VP, NP, PP를 모두 추출할 수 있는데, 문제는 성능이 낮다. (60%) 반면, 일반적인 (기계학습기반 그리고 NP만 추출하는..) chunking 도구들의 성능은 95%~97%정도 한다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if using_chunking == 1:\n",
    "#     #########################\n",
    "#     \"\"\" For training data \"\"\"\n",
    "#     #########################\n",
    "#     for i, sentence in enumerate(pre_X_train):\n",
    "#         pos_tagged_sentence = nltk.pos_tag(sentence)\n",
    "#         tree = chunker.parse(pos_tagged_sentence)\n",
    "#         print(tree)\n",
    "        \n",
    "#         NP_word_list = [word[0] for word,pos in tree.pos() if pos=='NP']\n",
    "#         print(NP_word_list)\n",
    "#         #for j, token in enumerate(sentence):\n",
    "#         #    FE2_X_train[i][j] += FE_NP_CHUNK(j, sentence, NP_word_list, window_size_np_chunk)\n",
    "#         print('============================================================================================')\n",
    "#         print('\\n\\n')\n",
    "            \n",
    "#     ########################\n",
    "#     \"\"\" For testing data \"\"\"\n",
    "#     ########################\n",
    "#     for i, sentence in enumerate(pre_X_test):\n",
    "#         pos_tagged_sentence = nltk.pos_tag(sentence)\n",
    "#         tree = chunker.parse(pos_tagged_sentence)\n",
    "#         NP_word_list = [word[0] for word,pos in tree.pos() if pos=='NP']\n",
    "#         for j, token in enumerate(sentence):\n",
    "#             FE2_X_test[i][j] += FE_NP_CHUNK(j, sentence, NP_word_list, window_size_np_chunk)\n",
    "\n",
    "#     print('window size np chunk = ', window_size_np_chunk)\n",
    "#     print('===> current feature length', len(FE2_X_train[0][0])-len_pos_feature)\n",
    "#     len_chunk_feature = len(FE2_X_train[0][0])-len_pos_feature\n",
    "#     assert(len(FE2_X_train[0][0]) == len(FE2_X_test[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (Full) Parsing, (Constituency based Parsing Tree)\n",
    "\n",
    "NLTK는 full parsing과 dependency parsing이 규칙기반으로 이뤄진 모델밖에 없어서. 여기서부터는 다른 라이브러리를 사용한다. <br>\n",
    "기본적으로 NLTK는 noun phrase chunking까지만 제공해준다. 그 이후의 단계 (full parsing)는 규칙을 이용해서 만든다. <br>\n",
    "\n",
    "Parsing은 stanford NLP에서 제공해주는 parser를 사용할 것이고, java로 구현되어 있는 stanford parser를 NLTK가 python으로 인터페이스해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PP, NP, VP를 추가하면 성능이 조금 떨어진 이유를 생각해보면.\n",
    "depth, sibling과 correlation이 있기 때문이다\n",
    "이건 사실이다. 둘 다 비슷한 feature이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Features for several phrases\n",
    "def initial_tree_parameter(temp_sentence):\n",
    "\n",
    "    global list_current_depth\n",
    "    global previous_height\n",
    "    global current_height\n",
    "    global previous_depth\n",
    "    global current_depth\n",
    "    global phrase_list_until_leaf\n",
    "    global per_token\n",
    "    global depth_cnt\n",
    "    \n",
    "    ######\n",
    "    global index_NPSBAR\n",
    "    global index_VPSBAR\n",
    "    global index_SSBAR\n",
    "    global index_SQSBAR\n",
    "    \n",
    "    global index_PPPP\n",
    "    global index_VPPP\n",
    "    global index_NPPP\n",
    "    \n",
    "    global index_NPNP\n",
    "    global index_VPNP\n",
    "    global index_SNP\n",
    "    \n",
    "    global index_VPVP\n",
    "    global index_SVP\n",
    "    \n",
    "    ######\n",
    "    global list_NPSBAR\n",
    "    global list_VPSBAR\n",
    "    global list_SSBAR\n",
    "    global list_SQSBAR\n",
    "    \n",
    "    global list_PPPP\n",
    "    global list_VPPP\n",
    "    global list_NPPP\n",
    "    \n",
    "    global list_NPNP\n",
    "    global list_VPNP\n",
    "    global list_SNP\n",
    "    \n",
    "    global list_VPVP\n",
    "    global list_SVP\n",
    "    \n",
    "    list_NPSBAR = [0] * len(temp_sentence)\n",
    "    list_VPSBAR = [0] * len(temp_sentence)\n",
    "    list_SSBAR = [0] * len(temp_sentence)\n",
    "    list_SQSBAR = [0] * len(temp_sentence)\n",
    "    \n",
    "    list_PPPP = [0] * len(temp_sentence)\n",
    "    list_VPPP = [0] * len(temp_sentence)\n",
    "    list_NPPP = [0] * len(temp_sentence)\n",
    "    \n",
    "    list_NPNP = [0] * len(temp_sentence)\n",
    "    list_VPNP = [0] * len(temp_sentence)\n",
    "    list_SNP = [0] * len(temp_sentence)\n",
    "    \n",
    "    list_VPVP = [0] * len(temp_sentence)\n",
    "    list_SVP = [0] * len(temp_sentence)\n",
    "    \n",
    "    phrase_list_until_leaf = []\n",
    "    per_depth = []\n",
    "    per_token = []\n",
    "    list_current_depth = []\n",
    "    previous_height = 0\n",
    "    current_height = 0\n",
    "    previous_depth = 0\n",
    "    current_depth = 0\n",
    "    depth_cnt = 0\n",
    "    \n",
    "    index_NPSBAR = -1\n",
    "    index_VPSBAR = -1\n",
    "    index_SSBAR = -1\n",
    "    index_SQSBAR = -1\n",
    "    \n",
    "    index_PPPP = -1\n",
    "    index_VPPP = -1\n",
    "    index_NPPP = -1\n",
    "    \n",
    "    index_NPNP = -1\n",
    "    index_VPNP = -1\n",
    "    index_SNP = -1\n",
    "    \n",
    "    index_VPVP = -1\n",
    "    index_SVP = -1\n",
    "\n",
    "\n",
    "def traverseTree(tree):\n",
    "\n",
    "    global list_current_depth\n",
    "    global previous_height\n",
    "    global current_height\n",
    "    global previous_depth\n",
    "    global current_depth\n",
    "    global phrase_list_until_leaf\n",
    "    global per_token\n",
    "    global depth_cnt\n",
    "    global temp_sentence\n",
    "    \n",
    "    #####\n",
    "    global index_NPSBAR\n",
    "    global index_VPSBAR\n",
    "    global index_SSBAR\n",
    "    global index_SQSBAR\n",
    "    \n",
    "    global index_PPPP\n",
    "    global index_VPPP\n",
    "    global index_NPPP\n",
    "    \n",
    "    global index_NPNP\n",
    "    global index_VPNP\n",
    "    global index_SNP\n",
    "    \n",
    "    global index_VPVP\n",
    "    global index_SVP\n",
    "       \n",
    "    ######\n",
    "    global list_NPSBAR\n",
    "    global list_VPSBAR\n",
    "    global list_SSBAR\n",
    "    global list_SQSBAR\n",
    "    \n",
    "    global list_PPPP\n",
    "    global list_VPPP\n",
    "    global list_NPPP\n",
    "    \n",
    "    global list_NPNP\n",
    "    global list_VPNP\n",
    "    global list_SNP\n",
    "    \n",
    "    global list_VPVP\n",
    "    global list_SVP\n",
    "    \n",
    "    \n",
    "    if type(tree) == nltk.tree.Tree:\n",
    "        \n",
    "        # Initialication\n",
    "        current_height = tree.height()\n",
    "        \n",
    "        \n",
    "        # For SBAR, NPPP, VPPP, NP\n",
    "#         if tree.label() == 'SBAR':\n",
    "#             tokens_SBAR = tree.leaves()\n",
    "# #             print(len(tokens_SBAR))\n",
    "# #             print(len(temp_sentence))\n",
    "# #             print(tokens_SBAR)\n",
    "# #             print(temp_sentence)\n",
    "# #             print(\"**************************\")\n",
    "#             for i, token in enumerate(temp_sentence):\n",
    "#                 if temp_sentence[i] == tokens_SBAR[0]:\n",
    "#                     if temp_sentence[i+len(tokens_SBAR)-1] == tokens_SBAR[-1]:\n",
    "#                         index_SBAR = i\n",
    "#                         break\n",
    "            \n",
    "#             if not index_SBAR == -1:\n",
    "#                 for j in range(index_SBAR, index_SBAR + len(tokens_SBAR)):\n",
    "#                     list_SBAR[j] = 1\n",
    "\n",
    "        # NP - SBAR\n",
    "        if tree.label() == 'SBAR' and list_current_depth[-1] == 'NP':\n",
    "            for i, token in enumerate(temp_sentence):\n",
    "                if temp_sentence[i] == tree.leaves()[0] and temp_sentence[i+len(tree.leaves())-1] == tree.leaves()[-1]:\n",
    "                    index_NPSBAR = i\n",
    "                    break    \n",
    "            if not index_NPSBAR == -1:\n",
    "                for j in range(index_NPSBAR, index_NPSBAR+len(tree.leaves())):\n",
    "                    list_NPSBAR[j] = 1              \n",
    "        \n",
    "        # VP - SBAR\n",
    "        elif tree.label() == 'SBAR' and list_current_depth[-1] == 'VP':\n",
    "            \n",
    "            for i, token in enumerate(temp_sentence):\n",
    "                if temp_sentence[i] == tree.leaves()[0] and temp_sentence[i+len(tree.leaves())-1] == tree.leaves()[-1]:\n",
    "                    index_VPSBAR = i\n",
    "                    break   \n",
    "            if not index_VPSBAR == -1:\n",
    "                for j in range(index_VPSBAR, index_VPSBAR+len(tree.leaves())):\n",
    "                    list_VPSBAR[j] = 1         \n",
    "        \n",
    "        # S - SBAR\n",
    "        elif tree.label() == 'SBAR' and list_current_depth[-1] == 'S':\n",
    "            for i, token in enumerate(temp_sentence):\n",
    "                if temp_sentence[i] == tree.leaves()[0] and temp_sentence[i+len(tree.leaves())-1] == tree.leaves()[-1]:\n",
    "                    index_SSBAR = i\n",
    "                    break    \n",
    "            if not index_SSBAR == -1:\n",
    "                for j in range(index_SSBAR, index_SSBAR+len(tree.leaves())):\n",
    "                    list_SSBAR[j] = 1      \n",
    "        \n",
    "        # SQ - SBAR\n",
    "        elif tree.label() == 'SBAR' and list_current_depth[-1] == 'SQ':\n",
    "            for i, token in enumerate(temp_sentence):\n",
    "                if temp_sentence[i] == tree.leaves()[0] and temp_sentence[i+len(tree.leaves())-1] == tree.leaves()[-1]:\n",
    "                    index_SQSBAR = i\n",
    "                    break    \n",
    "            if not index_SQSBAR == -1:\n",
    "                for j in range(index_SQSBAR, index_SQSBAR+len(tree.leaves())):\n",
    "                    list_SQSBAR[j] = 1        \n",
    "\n",
    "\n",
    "        # PP - PP\n",
    "        elif tree.label() == 'PP' and list_current_depth[-1] == 'PP':\n",
    "            for i, token in enumerate(temp_sentence):\n",
    "                if temp_sentence[i] == tree.leaves()[0] and temp_sentence[i+len(tree.leaves())-1] == tree.leaves()[-1]:\n",
    "                    index_PPPP = i\n",
    "                    break    \n",
    "            if not index_PPPP == -1:\n",
    "                for j in range(index_PPPP, index_PPPP+len(tree.leaves())):\n",
    "                    list_PPPP[j] = 1\n",
    "                    \n",
    "        # VP - PP\n",
    "        elif tree.label() == 'PP' and list_current_depth[-1] == 'VP':\n",
    "            for i, token in enumerate(temp_sentence):\n",
    "                if temp_sentence[i] == tree.leaves()[0] and temp_sentence[i+len(tree.leaves())-1] == tree.leaves()[-1]:\n",
    "                    index_VPPP = i\n",
    "                    break    \n",
    "            if not index_VPPP == -1:\n",
    "                for j in range(index_VPPP, index_VPPP+len(tree.leaves())):\n",
    "                    list_VPPP[j] = 1\n",
    "                    \n",
    "        # NP - PP\n",
    "        elif tree.label() == 'PP' and list_current_depth[-1] == 'NP':\n",
    "            for i, token in enumerate(temp_sentence):\n",
    "                if temp_sentence[i] == tree.leaves()[0] and temp_sentence[i+len(tree.leaves())-1] == tree.leaves()[-1]:\n",
    "                    index_NPPP = i\n",
    "                    break    \n",
    "            if not index_NPPP == -1:\n",
    "                for j in range(index_NPPP, index_NPPP+len(tree.leaves())):\n",
    "                    list_NPPP[j] = 1\n",
    "        \n",
    "        # NP - NP\n",
    "        elif tree.label() == 'NP' and list_current_depth[-1] == 'NP':\n",
    "            for i, token in enumerate(temp_sentence):\n",
    "                if temp_sentence[i] == tree.leaves()[0] and temp_sentence[i+len(tree.leaves())-1] == tree.leaves()[-1]:\n",
    "                    index_NPNP = i\n",
    "                    break    \n",
    "            if not index_NPNP == -1:\n",
    "                for j in range(index_NPNP, index_NPNP+len(tree.leaves())):\n",
    "                    list_NPNP[j] = 1\n",
    "\n",
    "        # VP - NP\n",
    "        elif tree.label() == 'NP' and list_current_depth[-1] == 'VP':\n",
    "            for i, token in enumerate(temp_sentence):\n",
    "                if temp_sentence[i] == tree.leaves()[0] and temp_sentence[i+len(tree.leaves())-1] == tree.leaves()[-1]:\n",
    "                    index_VPNP = i\n",
    "                    break    \n",
    "            if not index_VPNP == -1:\n",
    "                for j in range(index_VPNP, index_VPNP+len(tree.leaves())):\n",
    "                    list_VPNP[j] = 1\n",
    "\n",
    "        # S - NP\n",
    "        elif tree.label() == 'NP' and list_current_depth[-1] == 'S':\n",
    "            for i, token in enumerate(temp_sentence):\n",
    "                if temp_sentence[i] == tree.leaves()[0] and temp_sentence[i+len(tree.leaves())-1] == tree.leaves()[-1]:\n",
    "                    index_SNP = i\n",
    "                    break    \n",
    "            if not index_SNP == -1:\n",
    "                for j in range(index_SNP, index_SNP+len(tree.leaves())):\n",
    "                    list_SNP[j] = 1\n",
    "                    \n",
    "        # VP - VP\n",
    "        elif tree.label() == 'VP' and list_current_depth[-1] == 'VP':\n",
    "            for i, token in enumerate(temp_sentence):\n",
    "                if temp_sentence[i] == tree.leaves()[0] and temp_sentence[i+len(tree.leaves())-1] == tree.leaves()[-1]:\n",
    "                    index_VPVP = i\n",
    "                    break    \n",
    "            if not index_VPVP == -1:\n",
    "                for j in range(index_VPVP, index_VPVP+len(tree.leaves())):\n",
    "                    list_VPVP[j] = 1\n",
    "        \n",
    "        # S - VP\n",
    "        elif tree.label() == 'VP' and list_current_depth[-1] == 'S':\n",
    "            for i, token in enumerate(temp_sentence):\n",
    "                if temp_sentence[i] == tree.leaves()[0] and temp_sentence[i+len(tree.leaves())-1] == tree.leaves()[-1]:\n",
    "                    index_SVP = i\n",
    "                    break    \n",
    "            if not index_SVP == -1:\n",
    "                for j in range(index_SVP, index_SVP+len(tree.leaves())):\n",
    "                    list_SVP[j] = 1\n",
    "        \n",
    "#         elif tree.label() == 'NP':\n",
    "#             tokens_NP = tree.leaves()\n",
    "\n",
    "#             for i, token in enumerate(temp_sentence):\n",
    "#                 if temp_sentence[i] == tokens_NP[0] and temp_sentence[i+len(tokens_NP)-1] == tokens_NP[-1]:\n",
    "#                     index_NP = i\n",
    "            \n",
    "#             if not index_NP == -1:\n",
    "#                 for j in range(index_NP, index_NP + len(tokens_NP)):\n",
    "#                     list_NP[j] = 1        \n",
    "        \n",
    "        ### For Depth\n",
    "        # fist tree search\n",
    "        if depth_cnt == 0:\n",
    "            current_depth = current_height # depth initialization\n",
    "            list_current_depth.append(tree.label())\n",
    "            depth_cnt += 1\n",
    "        # what is the current depth?\n",
    "        if current_height == previous_depth-1:\n",
    "            current_depth = current_height\n",
    "            list_current_depth.append(tree.label())\n",
    "        \n",
    "#         if current_height > previous_height:\n",
    "#             per_token = []\n",
    "#         if current_height == 2:\n",
    "#             phrase_list_until_leaf.append(per_token)\n",
    "#         else:\n",
    "#             per_token.append(tree.label())\n",
    "        \n",
    "        previous_height = current_height\n",
    "        previous_depth = current_depth\n",
    "        \n",
    "\n",
    "#         print(\"tree:\", tree)\n",
    "#         print(\"======> tree.pos()\", tree.pos())\n",
    "#         print(\"======> tree.height()\", tree.height())\n",
    "#         print(\"======> tree.label()\", tree.label())\n",
    "#         print(\"======> tree.leaves()\", tree.leaves())\n",
    "#         print(\"======> current_depth\", current_depth)\n",
    "#         print('======> current depth list', list_current_depth)\n",
    "        \n",
    "#         print('*************************************************************************************************')    \n",
    "#         print('\\n')\n",
    "\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == nltk.tree.Tree:\n",
    "            traverseTree(subtree) # recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ############ For Testing\n",
    "\n",
    "# def traverseTree1(tree):\n",
    "#     for subtree in tree:\n",
    "#         if type(subtree) == nltk.tree.Tree:\n",
    "#             traverseTree(subtree) # recursive\n",
    "\n",
    "# ttt = 0\n",
    "\n",
    "# sentence = 'Fischler proposed measures after EU reports.'\n",
    "# temp = nltk.pos_tag(sentence)\n",
    "# par_result = parser.tagged_parse(temp)\n",
    "# temp_sentence = sentence\n",
    "# initial_tree_parameter(temp_sentence)\n",
    "    \n",
    "# for tree in par_result:\n",
    "#     parse_tree = tree\n",
    "\n",
    "# # if ttt==1:\n",
    "# #     print(parse_tree)\n",
    "    \n",
    "# # ttt+=1\n",
    "\n",
    "# traverseTree1(parse_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if using_parsing == 1:\n",
    "    #########################\n",
    "    \"\"\" For training data \"\"\"\n",
    "    #########################\n",
    "    cnt = 0\n",
    "    temp_depth_list = []\n",
    "    temp_n_sibling_list = []\n",
    "    for i, sentence in enumerate(pre_X_train):\n",
    "        temp = nltk.pos_tag(sentence)\n",
    "        par_result = parser.tagged_parse(temp)\n",
    "        for tree in par_result:\n",
    "            parse_tree = tree\n",
    "        #print(parse_tree)\n",
    "\n",
    "        # For PP, NP, VP,...\n",
    "        temp_sentence = sentence\n",
    "        initial_tree_parameter(sentence)\n",
    "        traverseTree(parse_tree) # For getting tree information\n",
    "\n",
    "        # For depth, n_sibling...\n",
    "        depth_list, n_siblings_list = tree_depth_and_n_sibling(parse_tree)\n",
    "        assert(len(sentence) == len(depth_list))\n",
    "    #     print(sentence)\n",
    "    #     print(parse_tree.leaves())\n",
    "        normalized_li_depth = normalizing_depth_list(depth_list)\n",
    "        normalized_li_n_siblings = normalizing_n_siblings_list(n_siblings_list)\n",
    "\n",
    "        for j, token in enumerate(sentence):\n",
    "            ### Tree depth and sibling\n",
    "            FE2_X_train[i][j] += FE_depth(j, sentence, normalized_li_depth, window_size_depth) \n",
    "            FE2_X_train[i][j] += FE_n_siblings(j, sentence, normalized_li_n_siblings, window_size_n_siblings)     \n",
    "\n",
    "            ### Phrases-based Features\n",
    "#             FE2_X_train[i][j] += [list_NPSBAR[j]] # defalut 오직 SBAR만 뽑는다.\n",
    "#             FE2_X_train[i][j] += [list_VPSBAR[j]]\n",
    "#             FE2_X_train[i][j] += [list_SSBAR[j]]\n",
    "#             FE2_X_train[i][j] += [list_SQSBAR[j]]\n",
    "\n",
    "#             FE2_X_train[i][j] += [list_PPPP[j]] # default 오직 PP만 뽑는다.\n",
    "#             FE2_X_train[i][j] += [list_VPPP[j]]\n",
    "#             FE2_X_train[i][j] += [list_NPPP[j]] # sub_refinement_of_object를 추출하는데 도움..\n",
    "\n",
    "#             FE2_X_train[i][j] += [list_NPNP[j]]\n",
    "#             FE2_X_train[i][j] += [list_VPNP[j]] \n",
    "#             FE2_X_train[i][j] += [list_SNP[j]] \n",
    "\n",
    "#             FE2_X_train[i][j] += [list_VPVP[j]] \n",
    "#             FE2_X_train[i][j] += [list_SVP[j]]      \n",
    "\n",
    "        \n",
    "    #     cnt += 1\n",
    "    #     print('===================================================================')\n",
    "    #     if cnt == 1:\n",
    "    #         break\n",
    "\n",
    "    ########################\n",
    "    \"\"\" For testing data \"\"\"\n",
    "    ########################\n",
    "    cnt = 0\n",
    "    for i, sentence in enumerate(pre_X_test):\n",
    "        temp = nltk.pos_tag(sentence)\n",
    "        par_result = parser.tagged_parse(temp)\n",
    "        for tree in par_result:\n",
    "            parse_tree = tree\n",
    "        #print(parse_tree)\n",
    "\n",
    "        # For PP, NP, VP, ...\n",
    "        temp_sentence = sentence\n",
    "        initial_tree_parameter(sentence)\n",
    "        traverseTree(parse_tree) # For getting tree information\n",
    "\n",
    "        # For dpeth, n_sibling\n",
    "        depth_list, n_siblings_list = tree_depth_and_n_sibling(parse_tree)\n",
    "        assert(len(sentence) == len(depth_list))\n",
    "    #     print(sentence)\n",
    "    #     print(parse_tree.leaves())\n",
    "        normalized_li_depth = normalizing_depth_list(depth_list)\n",
    "        normalized_li_n_siblings = normalizing_n_siblings_list(n_siblings_list)\n",
    "\n",
    "        for j, token in enumerate(sentence):\n",
    "\n",
    "            ### Tree-depth, Sibling \n",
    "            FE2_X_test[i][j] += FE_depth(j, sentence, normalized_li_depth, window_size_depth) \n",
    "            FE2_X_test[i][j] += FE_n_siblings(j, sentence, normalized_li_n_siblings, window_size_n_siblings) \n",
    "\n",
    "            ## Phrases-based Festures\n",
    "#             FE2_X_test[i][j] += [list_NPSBAR[j]]\n",
    "#             FE2_X_test[i][j] += [list_VPSBAR[j]]\n",
    "#             FE2_X_test[i][j] += [list_SSBAR[j]]\n",
    "#             FE2_X_test[i][j] += [list_SQSBAR[j]]\n",
    "\n",
    "#             FE2_X_test[i][j] += [list_PPPP[j]]\n",
    "#             FE2_X_test[i][j] += [list_VPPP[j]]\n",
    "#             FE2_X_test[i][j] += [list_NPPP[j]] \n",
    "\n",
    "#             FE2_X_test[i][j] += [list_NPNP[j]]\n",
    "#             FE2_X_test[i][j] += [list_VPNP[j]] \n",
    "#             FE2_X_test[i][j] += [list_SNP[j]] \n",
    "\n",
    "#             FE2_X_test[i][j] += [list_VPVP[j]] \n",
    "#             FE2_X_test[i][j] += [list_SVP[j]]\n",
    "\n",
    "\n",
    "\n",
    "    #     print(convert_dummy2(depth_list[j], 0))\n",
    "    #     print(convert_dummy2(n_siblings_list[j], 1))\n",
    "    #     print([list_SBAR[j]])\n",
    "    #     print([list_VPPP[j]])\n",
    "    #     print([list_NPPP[j]])\n",
    "\n",
    "    #     cnt += 1\n",
    "    #     print('===================================================================')\n",
    "    #     if cnt == 1:\n",
    "    #         break\n",
    "       \n",
    "#     print('===> current feature length', len(FE2_X_train[0][0])-len_pos_feature-len_chunk_feature)\n",
    "#     len_parsing_feature = len(FE2_X_train[0][0])-len_pos_feature-len_chunk_feature\n",
    "#     assert(len(FE2_X_train[0][0]) == len(FE2_X_test[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dependency Parsing\n",
    "\n",
    "dependency parsing은 spacy 라이브러리도 유명하지만, standford dependency parsing이 좀 더 성능이 좋다는 글을 보고 사용하기로 했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['STANFORD_PARSER'] = 'C:\\\\stanford-parser-full-2016-10-31\\\\stanford-parser.jar'\n",
    "# os.environ['STANFORD_MODELS'] = 'C:\\\\stanford-parser-full-2016-10-31\\\\stanford-parser-3.7.0-models.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list_dependency = [\n",
    "#      'nsubj','det','amod','compound','aux','dobj','appos','conj','cc','nmod','case','advcl','mark','nsubjpass',\n",
    "#      'auxpass','xcomp','advmod','acl','neg','nmod:poss','mwe','nummod','acl:relcl','cop','expl','dep','ccomp',\n",
    "#      'parataxis','cc:preconj','compound:prt','csubj','nmod:tmod','iobj','det:predet','nmod:npmod','discourse'\n",
    "# ]\n",
    "\n",
    "# dict_dependency = { }\n",
    "# for i, token in enumerate(list_dependency):\n",
    "#     dict_dependency[token] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ################### Testing\n",
    "# cntt = 0\n",
    "# for i, sentence in enumerate(pre_X_train):\n",
    "#     temp = nltk.pos_tag(sentence)\n",
    "#     par_result = dep_parser.tagged_parse(temp)\n",
    "#     for tree in par_result:\n",
    "#         parse_tree = tree\n",
    "\n",
    "#     current_sentence = sentence\n",
    "#     triple_list = list(parse_tree.triples())\n",
    "\n",
    "#     ### For number of dependencies\n",
    "#     list_n_dependencies = [1] * len(current_sentence)\n",
    "#     dic_temp_n_dependencies = { }\n",
    "#     ### Just counts\n",
    "#     for triple in triple_list:\n",
    "#         word = triple[0][0]\n",
    "\n",
    "#         if not word in dic_temp_n_dependencies:\n",
    "#             dic_temp_n_dependencies[word] = 1\n",
    "#         else:\n",
    "#             dic_temp_n_dependencies[word] += 1\n",
    "        \n",
    "#     ### Add dependency counts\n",
    "#     for m, triple in reversed(list(enumerate(triple_list))): # reversed for loop\n",
    "#         outbounder = triple[0][0]\n",
    "#         inbounder = triple[2][0]\n",
    "#         if outbounder in dic_temp_n_dependencies and inbounder in dic_temp_n_dependencies:\n",
    "#             # give inbounder count to outbounder count\n",
    "#             dic_temp_n_dependencies[outbounder] += dic_temp_n_dependencies[inbounder]\n",
    "\n",
    "#     for n, word in enumerate(current_sentence):\n",
    "#         if word in dic_temp_n_dependencies:\n",
    "#             list_n_dependencies[n] = list_n_dependencies[n] + dic_temp_n_dependencies[word]\n",
    "    \n",
    "#     print(list_n_dependencies)\n",
    "#     sentence_n_dependencies = normalized_list_n_dependencies(list_n_dependencies)\n",
    "    \n",
    "#         ################################################################################################################ \n",
    "#     cntt+=1\n",
    "#     if cntt==100:\n",
    "#         break\n",
    "# #     for j, token in enumerate(current_sentence):\n",
    "# #         ### Tree depth and sibling\n",
    "\n",
    "# #         FE2_X_train[i][j] += [sentence_n_dependencies[j]] # converted into list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if using_dependency == 1:\n",
    "#     #####################\n",
    "#     \"\"\" Training Data \"\"\"\n",
    "#     #####################\n",
    "#     cntt = 0\n",
    "#     for i, sentence in enumerate(pre_X_train):\n",
    "#         temp = nltk.pos_tag(sentence)\n",
    "#         par_result = dep_parser.tagged_parse(temp)\n",
    "#         for tree in par_result:\n",
    "#             parse_tree = tree\n",
    "\n",
    "#         current_sentence = sentence\n",
    "#         triple_list = list(parse_tree.triples())\n",
    "\n",
    "#         ### For number of dependencies\n",
    "#         list_n_dependencies = [1] * len(current_sentence)\n",
    "#         dic_temp_n_dependencies = { }\n",
    "#         ### Just counts\n",
    "#         for triple in triple_list:\n",
    "#             word = triple[0][0]\n",
    "\n",
    "#             if not word in dic_temp_n_dependencies:\n",
    "#                 dic_temp_n_dependencies[word] = 1\n",
    "#             else:\n",
    "#                 dic_temp_n_dependencies[word] += 1\n",
    "#         ### Add dependency counts\n",
    "#         for m, triple in reversed(list(enumerate(triple_list))): # reversed for loop\n",
    "#             outbounder = triple[0][0]\n",
    "#             inbounder = triple[2][0]\n",
    "#             if outbounder in dic_temp_n_dependencies and inbounder in dic_temp_n_dependencies:\n",
    "#                 # give inbounder count to outbounder count\n",
    "#                 dic_temp_n_dependencies[outbounder] += dic_temp_n_dependencies[inbounder]\n",
    "\n",
    "#         for n, word in enumerate(current_sentence):\n",
    "#             if word in dic_temp_n_dependencies:\n",
    "#                 list_n_dependencies[n] = list_n_dependencies[n] + dic_temp_n_dependencies[word]\n",
    "\n",
    "#         normalized_li_n_dep = normalized_list_n_dependencies(list_n_dependencies)\n",
    "\n",
    "#         ################################################################################################################ \n",
    "#         for j, token in enumerate(current_sentence):\n",
    "#             ### Tree depth and sibling\n",
    "\n",
    "#             FE2_X_train[i][j] += FE_n_dependencies(j, sentence, normalized_li_n_dep, window_size_n_dep)\n",
    "\n",
    "# #             FE2_X_train[i][j] += FE_dependency(j, sentence, triple_list, 0, dict_dependency)\n",
    "# #             FE2_X_train[i][j] += FE_dependency(j, sentence, triple_list, 1, dict_dependency)\n",
    "        \n",
    "#     #     cntt += 1\n",
    "#     #     if cntt == 1:\n",
    "#     #         break\n",
    "\n",
    "#     ####################\n",
    "#     \"\"\" Testing Data \"\"\"\n",
    "#     ####################\n",
    "#     cntt = 0\n",
    "#     for i, sentence in enumerate(pre_X_test):\n",
    "#         temp = nltk.pos_tag(sentence)\n",
    "#         par_result = dep_parser.tagged_parse(temp)\n",
    "#         for tree in par_result:\n",
    "#             parse_tree = tree\n",
    "\n",
    "#         current_sentence = sentence\n",
    "#         triple_list = list(parse_tree.triples())\n",
    "\n",
    "#         ### For number of dependencies\n",
    "#         list_n_dependencies = [1] * len(current_sentence)\n",
    "#         dic_temp_n_dependencies = { }\n",
    "#         ### Just counts\n",
    "#         for triple in triple_list:\n",
    "#             word = triple[0][0]\n",
    "\n",
    "#             if not word in dic_temp_n_dependencies:\n",
    "#                 dic_temp_n_dependencies[word] = 1\n",
    "#             else:\n",
    "#                 dic_temp_n_dependencies[word] += 1\n",
    "#         ### Add dependency counts\n",
    "#         for m, triple in reversed(list(enumerate(triple_list))): # reversed for loop\n",
    "#             outbounder = triple[0][0]\n",
    "#             inbounder = triple[2][0]\n",
    "#             if outbounder in dic_temp_n_dependencies and inbounder in dic_temp_n_dependencies:\n",
    "#                 # give inbounder count to outbounder count\n",
    "#                 dic_temp_n_dependencies[outbounder] += dic_temp_n_dependencies[inbounder]\n",
    "\n",
    "#         for n, word in enumerate(current_sentence):\n",
    "#             if word in dic_temp_n_dependencies:\n",
    "#                 list_n_dependencies[n] = list_n_dependencies[n] + dic_temp_n_dependencies[word]\n",
    "\n",
    "#         normalized_li_n_dep = normalized_list_n_dependencies(list_n_dependencies)\n",
    "\n",
    "#         ################################################################################################################ \n",
    "#         for j, token in enumerate(current_sentence):\n",
    "#             ### Tree depth and sibling\n",
    "\n",
    "#             FE2_X_test[i][j] += FE_n_dependencies(j, sentence, normalized_li_n_dep, window_size_n_dep)\n",
    "\n",
    "# #             FE2_X_test[i][j] += FE_dependency(j, sentence, triple_list, 0, dict_dependency)\n",
    "# #             FE2_X_test[i][j] += FE_dependency(j, sentence, triple_list, 1, dict_dependency)\n",
    "        \n",
    "#     #     cntt += 1\n",
    "#     #     if cntt == 1:\n",
    "#     #         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Triple List\n",
    "# [(('include', 'VB'), 'nsubj', ('section', 'NN')),\n",
    "#  (('section', 'NN'), 'det', ('A', 'DT')),\n",
    "#  (('section', 'NN'), 'amod', ('clinical', 'JJ')),\n",
    "#  (('section', 'NN'), 'compound', ('lab', 'NN')),\n",
    "#  (('include', 'VB'), 'aux', ('shall', 'MD')),\n",
    "#  (('include', 'VB'), 'dobj', ('name', 'NN')),\n",
    "#  (('name', 'NN'), 'det', ('the', 'DT')),\n",
    "#  (('name', 'NN'), 'amod', ('clinical', 'JJ')),\n",
    "#  (('name', 'NN'), 'compound', ('site', 'NN')),\n",
    "#  (('name', 'NN'), 'appos', ('class', 'NN')),\n",
    "#  (('class', 'NN'), 'det', ('the', 'DT')),\n",
    "#  (('class', 'NN'), 'conj', ('instructor', 'NN')),\n",
    "#  (('class', 'NN'), 'conj', ('day', 'NN')),\n",
    "#  (('class', 'NN'), 'cc', ('and', 'CC')),\n",
    "#  (('class', 'NN'), 'conj', ('time', 'NN')),\n",
    "#  (('class', 'NN'), 'nmod', ('lab', 'NN')),\n",
    "#  (('lab', 'NN'), 'case', ('of', 'IN')),\n",
    "#  (('lab', 'NN'), 'det', ('the', 'DT'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cntt = 0\n",
    "# for i, sentence in enumerate(pre_X_test):\n",
    "#     temp = nltk.pos_tag(sentence)\n",
    "#     par_result = parser.tagged_parse(temp)\n",
    "#     for tree in par_result:\n",
    "#         parse_tree = tree\n",
    "#     print(parse_tree)\n",
    "    \n",
    "#     cntt += 1\n",
    "#     if cntt == 100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Write out files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Total feature length = 22 22\n"
     ]
    }
   ],
   "source": [
    "dump(FE2_X_train, 'FE2_X_train')\n",
    "dump(FE2_X_test, 'FE2_X_test')\n",
    "print('===> Total feature length =', len(FE2_X_train[0][0]), len(FE2_X_test[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 1. Ngram tagger\n",
    "# from nltk.tag import DefaultTagger\n",
    "# from nltk.tag import UnigramTagger\n",
    "# from nltk.tag import BigramTagger\n",
    "# from nltk.tag import TrigramTagger\n",
    "# from nltk.tag import NgramTagger\n",
    "\n",
    "# from nltk.corpus import treebank\n",
    "# train_sents = treebank.tagged_sents()\n",
    "\n",
    "# def backoff_tagger(train_sents, tagger_classes, backoff=None):  \n",
    "#     for cls in tagger_classes:    \n",
    "#         backoff = cls(train_sents, backoff=backoff)    \n",
    "#     return backoff \n",
    "\n",
    "# backoff = DefaultTagger('NN')\n",
    "# tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=backoff) \n",
    "\n",
    "# # 2. tnt Tagger\n",
    "# from nltk.tag import tnt \n",
    "# from nltk.tag import DefaultTagger\n",
    "# unk = DefaultTagger('NN') \n",
    "# tnt_tagger = tnt.TnT() \n",
    "# tnt_tagger = tnt.TnT(unk=unk, Trained=True)\n",
    "# tnt_tagger.train(train_sents)\n",
    "\n",
    "# # predefined tnt tagger\n",
    "# from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "# tagger = ClassifierBasedPOSTagger(train=train_sents) \n",
    "\n",
    "# # tagger can be saved\n",
    "# import pickle \n",
    "# f = open('tagger.pickle', 'wb') \n",
    "# pickle.dump(tagger, f) \n",
    "# f.close() \n",
    "# f = open('tagger.pickle', 'rb') \n",
    "# tagger = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# yD0 = []\n",
    "# yD1 = []\n",
    "# yD2 = []\n",
    "# yD3 = []\n",
    "# yD4 = []\n",
    "# yD5 = []\n",
    "# yD6 = [] \n",
    "# yD7 = []\n",
    "# yD8 = []\n",
    "# yD9 = []\n",
    "# yD10 = []\n",
    "# yD11 = []\n",
    "# yD12 = []\n",
    "# yD13 = []\n",
    "# yD14 = []\n",
    "# yD15 = []\n",
    "# yD16 = []\n",
    "# yS0 = []\n",
    "# yS1 = []\n",
    "# yS2 = []\n",
    "# yS3 = []\n",
    "# yS4 = []\n",
    "# yS5 = []\n",
    "# yS6 = [] \n",
    "# yS7 = []\n",
    "# yS8 = []\n",
    "# yS9 = []\n",
    "# yS10 = []\n",
    "# yS11 = []\n",
    "# yS12 = []\n",
    "# yS13 = []\n",
    "# yS14 = []\n",
    "# yS15 = []\n",
    "# yS16 = []\n",
    "        # For counting depth, n_sibling\n",
    "#         if Y_train[i][j] == 0:\n",
    "#             yD0.append(depth_list[j])\n",
    "#             yS0.append(n_siblings_list[j])\n",
    "#         elif Y_train[i][j] == 1:\n",
    "#             yD1.append(depth_list[j])\n",
    "#             yS1.append(n_siblings_list[j])        \n",
    "#         elif Y_train[i][j] == 2:\n",
    "#             yD2.append(depth_list[j])\n",
    "#             yS2.append(n_siblings_list[j]) \n",
    "#         elif Y_train[i][j] == 3:\n",
    "#             yD3.append(depth_list[j])\n",
    "#             yS3.append(n_siblings_list[j]) \n",
    "#         elif Y_train[i][j] == 4:\n",
    "#             yD4.append(depth_list[j])\n",
    "#             yS4.append(n_siblings_list[j])        \n",
    "#         elif Y_train[i][j] == 5:\n",
    "#             yD5.append(depth_list[j])\n",
    "#             yS5.append(n_siblings_list[j]) \n",
    "#         elif Y_train[i][j] == 6:\n",
    "#             yD6.append(depth_list[j])\n",
    "#             yS6.append(n_siblings_list[j])\n",
    "#         elif Y_train[i][j] == 7:\n",
    "#             yD7.append(depth_list[j])\n",
    "#             yS7.append(n_siblings_list[j])        \n",
    "#         elif Y_train[i][j] == 8:\n",
    "#             yD8.append(depth_list[j])\n",
    "#             yS8.append(n_siblings_list[j]) \n",
    "#         elif Y_train[i][j] == 9:\n",
    "#             yD9.append(depth_list[j])\n",
    "#             yS9.append(n_siblings_list[j]) \n",
    "#         elif Y_train[i][j] == 10:\n",
    "#             yD10.append(depth_list[j])\n",
    "#             yS10.append(n_siblings_list[j])        \n",
    "#         elif Y_train[i][j] == 11:\n",
    "#             yD11.append(depth_list[j])\n",
    "#             yS11.append(n_siblings_list[j]) \n",
    "#         elif Y_train[i][j] == 12:\n",
    "#             yD12.append(depth_list[j])\n",
    "#             yS12.append(n_siblings_list[j]) \n",
    "#         elif Y_train[i][j] == 13:\n",
    "#             yD13.append(depth_list[j])\n",
    "#             yS13.append(n_siblings_list[j])        \n",
    "#         elif Y_train[i][j] == 14:\n",
    "#             yD14.append(depth_list[j])\n",
    "#             yS14.append(n_siblings_list[j]) \n",
    "#         elif Y_train[i][j] == 15:\n",
    "#             yD15.append(depth_list[j])\n",
    "#             yS15.append(n_siblings_list[j]) \n",
    "#         elif Y_train[i][j] == 16:\n",
    "#             yD16.append(depth_list[j])\n",
    "#             yS16.append(n_siblings_list[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Frequency for depth and sibling\n",
    "# unfolded_temp_depth_list, unfolded_temp_n_sibling_list = unfold_data(temp_depth_list, temp_n_sibling_list)\n",
    "# a = np.array(unfolded_temp_depth_list)\n",
    "# b = np.bincount(a)\n",
    "# ii = np.nonzero(b)[0]\n",
    "# np.vstack((ii,b[ii])).T"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
