{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리 연구에서 stemming이 더 좋은 이유는 parser의 성능에 예민하기 때문이다. <br>\n",
    "lemmatization이 한번 실수하면 parser의 결과가 엉망이 될 수도 있다. <br>\n",
    "반면, stemming은 규칙기반으로 실수하는 일이 거의 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stemming: 그냥 chopping off한다. 즉, 단어의 형태(form)만 보고 판단한다.\n",
    "* WordNetLemmatizer: valid root word (=lemma(단어의 기본형))를 찾는다. 즉, 단어의 의미(meaning)를 보고 판단한다. \n",
    "\n",
    "### About Verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: going -> go\n",
      "Stemming: gone -> gone\n",
      "Stemming: goes -> goe\n",
      "Stemming: went -> went\n",
      "========================================\n",
      "Lemmatise (with pos=n): going -> going\n",
      "Lemmatise (with pos=n): gone -> gone\n",
      "Lemmatise (with pos=n): goes -> go\n",
      "Lemmatise (with pos=n): went -> went\n",
      "========================================\n",
      "Lemmatise (with pos=v): going -> go\n",
      "Lemmatise (with pos=v): gone -> go\n",
      "Lemmatise (with pos=v): goes -> go\n",
      "Lemmatise (with pos=v): went -> go\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemming: %s -> %s\" % (\"going\", stemmer.stem(\"going\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"gone\", stemmer.stem(\"gone\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"goes\", stemmer.stem(\"goes\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"went\", stemmer.stem(\"went\")))\n",
    "print(\"========================================\")\n",
    "print(\"Lemmatise (with pos=n): %s -> %s\" % (\"going\", lemmatiser.lemmatize(\"going\")))\n",
    "print(\"Lemmatise (with pos=n): %s -> %s\" % (\"gone\", lemmatiser.lemmatize(\"gone\")))\n",
    "print(\"Lemmatise (with pos=n): %s -> %s\" % (\"goes\", lemmatiser.lemmatize(\"goes\")))\n",
    "print(\"Lemmatise (with pos=n): %s -> %s\" % (\"went\", lemmatiser.lemmatize(\"went\")))\n",
    "print(\"========================================\")\n",
    "print(\"Lemmatise (with pos=v): %s -> %s\" % (\"going\", lemmatiser.lemmatize(\"going\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v): %s -> %s\" % (\"gone\", lemmatiser.lemmatize(\"gone\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v): %s -> %s\" % (\"goes\", lemmatiser.lemmatize(\"goes\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v): %s -> %s\" % (\"went\", lemmatiser.lemmatize(\"went\", pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: is -> is\n",
      "Stemming: are -> are\n",
      "Stemming: was -> wa\n",
      "Stemming: were -> were\n",
      "Stemming: being -> be\n",
      "Stemming: been -> been\n",
      "========================================\n",
      "Lemmatise (with pos=n): is -> is\n",
      "Lemmatise (with pos=n): are -> are\n",
      "Lemmatise (with pos=n): was -> wa\n",
      "Lemmatise (with pos=n): were -> were\n",
      "Lemmatise (with pos=n): being -> being\n",
      "Lemmatise (with pos=n): been -> been\n",
      "========================================\n",
      "Lemmatise (with pos=v): is -> be\n",
      "Lemmatise (with pos=v): are -> be\n",
      "Lemmatise (with pos=v): was -> be\n",
      "Lemmatise (with pos=v): were -> be\n",
      "Lemmatise (with pos=v): being -> be\n",
      "Lemmatise (with pos=v): been -> be\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemming: %s -> %s\" % (\"is\", stemmer.stem(\"is\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"are\", stemmer.stem(\"are\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"was\", stemmer.stem(\"was\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"were\", stemmer.stem(\"were\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"being\", stemmer.stem(\"being\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"been\", stemmer.stem(\"been\")))\n",
    "print(\"========================================\")\n",
    "print(\"Lemmatise (with pos=n): %s -> %s\" % (\"is\", lemmatiser.lemmatize(\"is\")))\n",
    "print(\"Lemmatise (with pos=n): %s -> %s\" % (\"are\", lemmatiser.lemmatize(\"are\")))\n",
    "print(\"Lemmatise (with pos=n): %s -> %s\" % (\"was\", lemmatiser.lemmatize(\"was\")))\n",
    "print(\"Lemmatise (with pos=n): %s -> %s\" % (\"were\", lemmatiser.lemmatize(\"were\")))\n",
    "print(\"Lemmatise (with pos=n): %s -> %s\" % (\"being\", lemmatiser.lemmatize(\"being\")))\n",
    "print(\"Lemmatise (with pos=n): %s -> %s\" % (\"been\", lemmatiser.lemmatize(\"been\")))\n",
    "print(\"========================================\")\n",
    "print(\"Lemmatise (with pos=v): %s -> %s\" % (\"is\", lemmatiser.lemmatize(\"is\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v): %s -> %s\" % (\"are\", lemmatiser.lemmatize(\"are\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v): %s -> %s\" % (\"was\", lemmatiser.lemmatize(\"was\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v): %s -> %s\" % (\"were\", lemmatiser.lemmatize(\"were\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v): %s -> %s\" % (\"being\", lemmatiser.lemmatize(\"being\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v): %s -> %s\" % (\"been\", lemmatiser.lemmatize(\"been\", pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: studying -> studi\n",
      "Stemming: study -> studi\n",
      "Stemming: studies -> studi\n",
      "Stemming: studied -> studi\n",
      "========================================\n",
      "Lemmatise (with pos=n) studying -> studying\n",
      "Lemmatise (with pos=n) study -> study\n",
      "Lemmatise (with pos=n) studies -> study\n",
      "Lemmatise (with pos=n) studied -> studied\n",
      "========================================\n",
      "Lemmatise (with pos=v) studying -> study\n",
      "Lemmatise (with pos=v) study -> study\n",
      "Lemmatise (with pos=v) studies -> study\n",
      "Lemmatise (with pos=v) studied -> study\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemming: %s -> %s\" % (\"studying\", stemmer.stem(\"studying\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"study\", stemmer.stem(\"study\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"studies\", stemmer.stem(\"studies\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"studied\", stemmer.stem(\"studied\")))\n",
    "print(\"========================================\")\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"studying\", lemmatiser.lemmatize(\"studying\")))\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"study\", lemmatiser.lemmatize(\"study\")))\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"studies\", lemmatiser.lemmatize(\"studies\")))\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"studied\", lemmatiser.lemmatize(\"studied\")))\n",
    "print(\"========================================\")\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"studying\", lemmatiser.lemmatize(\"studying\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"study\", lemmatiser.lemmatize(\"study\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"studies\", lemmatiser.lemmatize(\"studies\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"studied\", lemmatiser.lemmatize(\"studied\", pos=\"v\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### About Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: story -> stori\n",
      "Stemming: storys -> stori\n",
      "Stemming: stories -> stori\n",
      "========================================\n",
      "Lemmatise (with pos=n) story -> story\n",
      "Lemmatise (with pos=n) storys -> story\n",
      "Lemmatise (with pos=n) stories -> story\n",
      "========================================\n",
      "Lemmatise (with pos=v) story -> story\n",
      "Lemmatise (with pos=v) storys -> storys\n",
      "Lemmatise (with pos=v) stories -> stories\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemming: %s -> %s\" % (\"story\", stemmer.stem(\"story\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"storys\", stemmer.stem(\"storys\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"stories\", stemmer.stem(\"stories\")))\n",
    "print(\"========================================\")\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"story\", lemmatiser.lemmatize(\"story\")))\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"storys\", lemmatiser.lemmatize(\"storys\")))\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"stories\", lemmatiser.lemmatize(\"stories\")))\n",
    "print(\"========================================\")\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"story\", lemmatiser.lemmatize(\"story\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"storys\", lemmatiser.lemmatize(\"storys\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"stories\", lemmatiser.lemmatize(\"stories\", pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: Members -> member\n",
      "Stemming: storys -> stori\n",
      "Stemming: stories -> stori\n",
      "========================================\n",
      "Lemmatise (with pos=n) Members -> Members\n",
      "Lemmatise (with pos=n) storys -> story\n",
      "Lemmatise (with pos=n) stories -> story\n",
      "========================================\n",
      "Lemmatise (with pos=v) Members -> Members\n",
      "Lemmatise (with pos=v) storys -> storys\n",
      "Lemmatise (with pos=v) stories -> stories\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemming: %s -> %s\" % (\"Members\", stemmer.stem(\"Members\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"storys\", stemmer.stem(\"storys\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"stories\", stemmer.stem(\"stories\")))\n",
    "print(\"========================================\")\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"Members\", lemmatiser.lemmatize(\"Members\")))\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"storys\", lemmatiser.lemmatize(\"storys\")))\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"stories\", lemmatiser.lemmatize(\"stories\", pos=\"n\")))\n",
    "print(\"========================================\")\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"Members\", lemmatiser.lemmatize(\"Members\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"storys\", lemmatiser.lemmatize(\"storys\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"stories\", lemmatiser.lemmatize(\"stories\", pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: believes -> believ\n",
      "Stemming: believe -> believ\n",
      "Stemming: belief -> belief\n",
      "========================================\n",
      "Lemmatise (with pos=n) believes -> belief\n",
      "Lemmatise (with pos=n) believe -> believe\n",
      "Lemmatise (with pos=n) belief -> belief\n",
      "========================================\n",
      "Lemmatise (with pos=v) believes -> believe\n",
      "Lemmatise (with pos=v) believe -> believe\n",
      "Lemmatise (with pos=v) belief -> belief\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemming: %s -> %s\" % (\"believes\", stemmer.stem(\"believes\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"believe\", stemmer.stem(\"believe\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"belief\", stemmer.stem(\"belief\")))\n",
    "print(\"========================================\")\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"believes\", lemmatiser.lemmatize(\"believes\")))\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"believe\", lemmatiser.lemmatize(\"believe\")))\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"belief\", lemmatiser.lemmatize(\"belief\", pos=\"n\")))\n",
    "print(\"========================================\")\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"believes\", lemmatiser.lemmatize(\"believes\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"believe\", lemmatiser.lemmatize(\"believe\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"belief\", lemmatiser.lemmatize(\"belief\", pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: Administrators -> administr\n",
      "Stemming: administrators -> administr\n",
      "Stemming: As -> As\n",
      "Stemming: as -> as\n",
      "========================================\n",
      "Lemmatise (with pos=n) Administrators -> Administrators\n",
      "Lemmatise (with pos=n) administrators -> administrator\n",
      "Lemmatise (with pos=n) As -> As\n",
      "Lemmatise (with pos=n) as -> a\n",
      "========================================\n",
      "Lemmatise (with pos=v) Administrators -> Administrators\n",
      "Lemmatise (with pos=v) administrators -> administrators\n",
      "Lemmatise (with pos=v) As -> As\n",
      "Lemmatise (with pos=v) as -> as\n",
      "========================================\n",
      "Lemmatise (with pos=a) As -> As\n",
      "Lemmatise (with pos=a) as -> as\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemming: %s -> %s\" % (\"Administrators\", stemmer.stem(\"Administrators\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"administrators\", stemmer.stem(\"administrators\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"As\", stemmer.stem(\"As\")))\n",
    "print(\"Stemming: %s -> %s\" % (\"as\", stemmer.stem(\"as\")))\n",
    "print(\"========================================\")\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"Administrators\", lemmatiser.lemmatize(\"Administrators\", pos=\"n\")))\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"administrators\", lemmatiser.lemmatize(\"administrators\", pos=\"n\")))\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"As\", lemmatiser.lemmatize(\"As\", pos=\"n\")))\n",
    "print(\"Lemmatise (with pos=n) %s -> %s\" % (\"as\", lemmatiser.lemmatize(\"as\", pos=\"n\")))\n",
    "print(\"========================================\")\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"Administrators\", lemmatiser.lemmatize(\"Administrators\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"administrators\", lemmatiser.lemmatize(\"administrators\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"As\", lemmatiser.lemmatize(\"As\", pos=\"v\")))\n",
    "print(\"Lemmatise (with pos=v) %s -> %s\" % (\"as\", lemmatiser.lemmatize(\"as\", pos=\"v\")))\n",
    "print(\"========================================\")\n",
    "print(\"Lemmatise (with pos=a) %s -> %s\" % (\"As\", lemmatiser.lemmatize(\"As\", pos=\"a\")))\n",
    "print(\"Lemmatise (with pos=a) %s -> %s\" % (\"as\", lemmatiser.lemmatize(\"as\", pos=\"a\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Combining stemming with lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "드물지만, 데이터를 압축하기 위해 stemming과 lemmatization을 같이 사용하는 경우가 있다. <br>\n",
    "즉, 다음과 같이 lemmatiser를 먼저하고 stemmer를 한 번더하면 bu가 출력이된다. <br>\n",
    "이는 최초 5개의 문자로 이뤄진 단어를 2개의 문자로 이뤄진 단어로 압축할 수 있다. 압축률은 60%이다. <br>\n",
    "만약, 이러한 단어가 수천개가 있고, 이러한 단어들이 등장하는 횟수가 매우 많다면 이러한 압축으로 매우 큰 이득을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buse\n",
      "bus\n",
      "bu\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('buses')) \n",
    "print(lemmatiser.lemmatize('buses'))\n",
    "# lemmatise -> stemmer\n",
    "print(stemmer.stem('bus'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Use Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "\"\"\" lemmatisation and stemming \"\"\"\n",
    "##################################\n",
    "## lemmatisation이 wordnet기반이라 하나로 통일해주는 것에 잘 한다. 그런데, 명사, 동사를 구분해야 되기 때문에 \n",
    "## 정확하게 하려면 POS를 먼저하고 명사, 동사에 맞게 lemmatisation을 해줘야 한다. 그렇지 않으면 명사의 단/복수 구분도 못한다.\n",
    "## 일단 stemming만 하고. 나중에 lemmatisation을 적용시켜보자. lemmatisation을 하면 문제가 되는 것이 pos를 통해 syntactic 정보를 사용한다는 점이다.\n",
    "## 성능을 높이기 위해 일정 개수 이하의 단어(1회씩 등장하는 단어)들은 모두 unknown token으로 규정하자.\n",
    "## 여기서 만약 비슷한 단어끼리 clustering해서 하나의 단어로 표현할 수 있으면, word embedding보다 좋은 성능을 가져올 수도 있다.\n",
    "## soft clustering이 아니라 hard clustering을 한다는 것인데, 적은 데이터셋에서는 더 유용할 수도 있다.\n",
    "## 그냥 사전을 구축하는 단계까지는 syntactic / semantic 정보를 허용하는 것은 어떠할까...?\n",
    "## lemmatiser는 default로 context를 n(명사)를 기반으로 한다. (wordnet과 연결되어 있어서 stemming보다는 훨씬 느리다.)\n",
    "from nltk.corpus import wordnet as wn\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer() # WordNet Lemmatizer\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS', 'IN'] # IN 추가 IN중에서 as가 그냥 lemmitization되면 a가 되므로, 'a'로 설정해준다.\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def lemmatise(tuple): \n",
    "## to distinguish whether token is noun or verb\n",
    "## because in lemmatization, there are different result according to them \n",
    "    verb_tag_set = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] # verb tag list from 'nltk.help.upenn_tagset()'\n",
    "    token = tuple[0]\n",
    "    pos_tag = tuple[1]\n",
    "    \n",
    "    if penn_to_wn(pos_tag) == None:\n",
    "        return str(lemmatiser.lemmatize(token))\n",
    "    else:\n",
    "        return str(lemmatiser.lemmatize(token, penn_to_wn(pos_tag)))\n",
    "    \n",
    "#     if any(pos_tag in t for t in verb_tag_set):\n",
    "#         return str(lemmatiser.lemmatize(token, pos=\"v\"))\n",
    "#     else:\n",
    "#         return str(lemmatiser.lemmatize(token)) # default = pos='n'\n",
    "\n",
    "\n",
    "if stemmming_lemmatisation == 0:\n",
    "    # 1. Stemming\n",
    "    for i, sentence in enumerate(X_train): # for training data\n",
    "        for j, token in enumerate(sentence):\n",
    "            X_train[i][j] = str(stemmer.stem(token))\n",
    "    for i, sentence in enumerate(X_test): # for testing data\n",
    "        for j, token in enumerate(sentence):\n",
    "            X_test[i][j] = str(stemmer.stem(token))\n",
    "else:       \n",
    "    # 2. Lemmatise\n",
    "    for i, sentence in enumerate(X_train): # for training data\n",
    "        pos_sentence = nltk.pos_tag(sentence)\n",
    "        for j, token in enumerate(sentence):\n",
    "            X_train[i][j] = lemmatise(pos_sentence[j]) # input: tuple\n",
    "\n",
    "    for i, sentence in enumerate(X_test): # for training data\n",
    "        pos_sentence = nltk.pos_tag(sentence)\n",
    "        for j, token in enumerate(sentence):\n",
    "            X_test[i][j] = lemmatise(pos_sentence[j]) # input: tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
