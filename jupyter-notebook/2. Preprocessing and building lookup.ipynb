{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 statistical feature를 design할 때, 어떤 linguistic knowledge도 적용하지 않는게 원칙이지만, <br>\n",
    "여기서는 전처리할 때, stemming 대신 lemmatization (pos사용)을 사용한다. <br>\n",
    "그리고 n-gram 사전을 구축할 때, unknown_token을 단순히 1개로 정의하는게 아니라. <br>\n",
    "pos를 먼저 실시하여, unknown_noun과 unknown_verb 등과 같이 세부적으로 나눌수 있도록 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 563 ,  test: 141\n"
     ]
    }
   ],
   "source": [
    "# parameter setting for filtering\n",
    "stemmming_lemmatisation = 0 # 0: stemming, 1: lemmatisation\n",
    "\n",
    "# threshold\n",
    "thr_1gram = 1\n",
    "thr_2gram = 2\n",
    "thr_3gram = 2\n",
    "thr_5gram = 1\n",
    "\n",
    "thr_component = 1\n",
    "thr_refinement_of_component = 1\n",
    "thr_action = 1\n",
    "thr_refinement_of_action = 1\n",
    "thr_condition = 1\n",
    "thr_priority = 1\n",
    "thr_motivation = 1\n",
    "thr_role = 1\n",
    "thr_object = 1\n",
    "thr_refinement_of_object = 1\n",
    "thr_sub_action = 1\n",
    "thr_sub_argument_of_action = 1\n",
    "thr_sub_priority = 1\n",
    "thr_sub_role = 1\n",
    "thr_sub_object = 1\n",
    "thr_sub_refinement_of_object = 1\n",
    "thr_none = 1\n",
    "\n",
    "\n",
    "##################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import pandas as pd \n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from data_handler import *\n",
    "\n",
    "PATH = 'assets/freq-dist/'\n",
    "\n",
    "# Load files\n",
    "X_train = load('X_train')\n",
    "Y_train = load('Y_train')\n",
    "X_test = load('X_test')\n",
    "Y_test = load('Y_test')\n",
    "print('train:', len(X_train), ',  test:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "* 전처리 과정에서 가장 중요한 점이 sparsity 문제를 해결하는 것이다. (NLP의 기본적인 전처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def count_token(train):\n",
    "#     cnt = 0\n",
    "#     for i, sentence in enumerate(train): # for training data\n",
    "#         for j, token in enumerate(sentence):\n",
    "#              cnt += 1\n",
    "#     return cnt\n",
    "# print('before removing some tokens = %d' % count_token(X_train))\n",
    "\n",
    "#############\n",
    "\"\"\" Lower \"\"\"\n",
    "#############\n",
    "## Lower은 preprocessing 맨처음 하는게 좋다. 한 token에 불특정한 패턴들의 대문자들이 등장하는 경우가 있기 때문이다.\n",
    "## 대문자가 첫 글자에 있어버리면 제대로 동작을 못한다. (만약 Members 이면 member라고 못한다.)\n",
    "## 문제는 I 같은 경우는 i인 상태에서 pos하면 제대로 동작 안한다. 그러나 이는 미미한 문제이다.\n",
    "for i, sentence in enumerate(X_train): # for training data\n",
    "    for j, token in enumerate(sentence):\n",
    "        X_train[i][j] = token.lower()\n",
    "for i, sentence in enumerate(X_test): # for testing data\n",
    "    for j, token in enumerate(sentence):\n",
    "        X_test[i][j] = token.lower()\n",
    "\n",
    "# ###############\n",
    "# \"\"\" Removal \"\"\"\n",
    "# ###############\n",
    "# ### for reducing complexity (=high-dimensional) of our data and for dealing with sparse problem\n",
    "# ### these tend to lead for our model to be underfitted (if we have infinite number of data, we don't need to care about these)\n",
    "# ### so, we need to do these:\n",
    "# \"\"\" Remove puntuation (e.g.,  .  , ( ) ? ' ; ) \"\"\"\n",
    "# puntation_list = ['.', ',', '(', ')', '*', ':', '``', \"''\", ';', '-', '--', '?', '.', ',', '$']\n",
    "# puntation_list = ['.', ',', '(', ')', '*', ':']\n",
    "# puntation_list = []\n",
    "# \"\"\" Remove unnecessary token or not (e.g., 'i.e', 'e.g') \"\"\"\n",
    "# unnecessary_token_list = ['i.e', 'e.g', '1.0', \"n't\", \"'s\", ')']\n",
    "# unnecessary_token_list = ['i.e', 'e.g', 'the']\n",
    "# unnecessary_token_list = []\n",
    "# for i, sentence in enumerate(X_train): # for training data\n",
    "#     for j, token in enumerate(sentence):\n",
    "#         if any(token in t for t in puntation_list):\n",
    "#             sentence.remove(sentence[j])  # X delete\n",
    "#             del Y_train[i][j]             # Y delete\n",
    "#         elif any(token in t for t in unnecessary_token_list):\n",
    "#             sentence.remove(sentence[j])\n",
    "#             del Y_train[i][j]\n",
    "\n",
    "# for i, sentence in enumerate(X_test): # for testing data\n",
    "#     for j, token in enumerate(sentence):\n",
    "#         if any(token in t for t in puntation_list):\n",
    "#             sentence.remove(sentence[j])\n",
    "#             del Y_test[i][j]\n",
    "#         elif any(token in t for t in unnecessary_token_list):\n",
    "#             sentence.remove(sentence[j])\n",
    "#             del Y_test[i][j]\n",
    "# print('after removing some tokens = %d' % count_token(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "\"\"\" lemmatisation and stemming \"\"\"\n",
    "##################################\n",
    "## lemmatisation이 wordnet기반이라 하나로 통일해주는 것에 잘 한다. 그런데, 명사, 동사를 구분해야 되기 때문에 \n",
    "## 정확하게 하려면 POS를 먼저하고 명사, 동사에 맞게 lemmatisation을 해줘야 한다. 그렇지 않으면 명사의 단/복수 구분도 못한다.\n",
    "## 일단 stemming만 하고. 나중에 lemmatisation을 적용시켜보자. lemmatisation을 하면 문제가 되는 것이 pos를 통해 syntactic 정보를 사용한다는 점이다.\n",
    "## 성능을 높이기 위해 일정 개수 이하의 단어(1회씩 등장하는 단어)들은 모두 unknown token으로 규정하자.\n",
    "## 여기서 만약 비슷한 단어끼리 clustering해서 하나의 단어로 표현할 수 있으면, word embedding보다 좋은 성능을 가져올 수도 있다.\n",
    "## soft clustering이 아니라 hard clustering을 한다는 것인데, 적은 데이터셋에서는 더 유용할 수도 있다.\n",
    "## 그냥 사전을 구축하는 단계까지는 syntactic / semantic 정보를 허용하는 것은 어떠할까...?\n",
    "## lemmatiser는 default로 context를 n(명사)를 기반으로 한다. (wordnet과 연결되어 있어서 stemming보다는 훨씬 느리다.)\n",
    "## stemming을 할 수 밖에 없는 가장 큰 이유: as를 살리고 싶어서...\n",
    "from nltk.corpus import wordnet as wn\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer() # WordNet Lemmatizer\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS', 'IN'] # IN 추가 IN중에서 as가 그냥 lemmitization되면 a가 되므로, 'a'로 설정해준다.\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def lemmatise(tuple): \n",
    "## to distinguish whether token is noun or verb\n",
    "## because in lemmatization, there are different result according to them \n",
    "    verb_tag_set = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] # verb tag list from 'nltk.help.upenn_tagset()'\n",
    "    token = tuple[0]\n",
    "    pos_tag = tuple[1]\n",
    "    \n",
    "    if penn_to_wn(pos_tag) == None:\n",
    "        return str(lemmatiser.lemmatize(token))\n",
    "    else:\n",
    "        return str(lemmatiser.lemmatize(token, penn_to_wn(pos_tag)))\n",
    "    \n",
    "#     if any(pos_tag in t for t in verb_tag_set):\n",
    "#         return str(lemmatiser.lemmatize(token, pos=\"v\"))\n",
    "#     else:\n",
    "#         return str(lemmatiser.lemmatize(token)) # default = pos='n'\n",
    "\n",
    "if stemmming_lemmatisation == 0:\n",
    "    # 1. Stemming\n",
    "    for i, sentence in enumerate(X_train): # for training data\n",
    "        for j, token in enumerate(sentence):\n",
    "            X_train[i][j] = str(stemmer.stem(token))\n",
    "    for i, sentence in enumerate(X_test): # for testing data\n",
    "        for j, token in enumerate(sentence):\n",
    "            X_test[i][j] = str(stemmer.stem(token))\n",
    "else:       \n",
    "    # 2. Lemmatise\n",
    "    for i, sentence in enumerate(X_train): # for training data\n",
    "        pos_sentence = nltk.pos_tag(sentence)\n",
    "        for j, token in enumerate(sentence):\n",
    "            X_train[i][j] = lemmatise(pos_sentence[j]) # input: tuple\n",
    "\n",
    "    for i, sentence in enumerate(X_test): # for training data\n",
    "        pos_sentence = nltk.pos_tag(sentence)\n",
    "        for j, token in enumerate(sentence):\n",
    "            X_test[i][j] = lemmatise(pos_sentence[j]) # input: tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build frequency dictionary (using only training data)\n",
    "### (before making Look-up Table)\n",
    "\n",
    "unknown token이라 training data에 수정하지 않고, <br>\n",
    "그냥 곧바로 vocabulary set을 만들어서 나중에 feature extraction할 때 <br>\n",
    "사전에 없는 것은 자동으로 unknown token/pharse로 처리되도록 하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Word-level One-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio of (word-level) 1-gram unknown_word: 47.66355140186916\n",
      "1-gram voca size:  728\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Define unknown token using only training data \"\"\"\n",
    "## 데이터 셋이 작아서 그런지 생각보다 freq 1인 단어들이 많다. \n",
    "## stemming말고 lemmatisation을 통해 word 어원의 형태를 잡고, wordnet과 같은 외부 자원을 사용해서\n",
    "## freq 1인 단어라도 wordnet에 해당되는 단어는 unknown token으로 처리하지 않는 방식도 생각해보자.\n",
    "## Stemming 후, word freq 1 비율: ('ratio of unknown_word:', 48.505434782608695)\n",
    "\n",
    "# Word frequency dictionary\n",
    "word_count_dic = dict()\n",
    "for i, sentence in enumerate(X_train):\n",
    "    for j, token in enumerate(sentence):\n",
    "        word_count_dic[token] = word_count_dic.get(token, 0) + 1\n",
    "\n",
    "#sorted(word_count_dic.items(), key=lambda x:x[1], reverse=True)\n",
    "#sorted(word_count_dic.items(), key=lambda x:x[1]) \n",
    "\n",
    "# Write Frequency Distribution\n",
    "text_file = open(PATH+\"freqeucny_distribution_1gram.txt\", \"w\")\n",
    "for w in sorted(word_count_dic, key=word_count_dic.get, reverse=True):\n",
    "    text_file.write(w)\n",
    "    text_file.write('\\t')\n",
    "    text_file.write(str(word_count_dic[w]))\n",
    "    text_file.write('\\n')\n",
    "text_file.close()\n",
    "\n",
    "# Build unknwon word list based on frequency or wordnet (e.g., threshold: freq 1)\n",
    "unknown_word_list = [] \n",
    "word_one_gram_voca_set = []\n",
    "for key, value in word_count_dic.items():\n",
    "    if value <= thr_1gram: # if only 1 frequency\n",
    "        unknown_word_list.append(key)\n",
    "    else:\n",
    "        word_one_gram_voca_set.append(key)\n",
    "print(\"ratio of (word-level) 1-gram unknown_word:\", len(unknown_word_list) / float(len(word_count_dic)) * 100)\n",
    "\n",
    "# \"\"\" Replace unknown token using only training data \"\"\"\n",
    "# for i, sentence in enumerate(X_train): \n",
    "#     for j, token in enumerate(sentence):\n",
    "#         if any(token in t for t in unknown_word_list):\n",
    "#             X_train[i][j] = 'unknown_token'\n",
    "print('1-gram voca size: ', len(word_one_gram_voca_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word-level Bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-gram voca size:  537\n"
     ]
    }
   ],
   "source": [
    "### create bi-gram set\n",
    "bigram_list = []\n",
    "for i, sentence in enumerate(X_train):\n",
    "    \n",
    "    # Extract bigrams from a single sentence\n",
    "    bgs = nltk.ngrams(sentence, 2)\n",
    "    \n",
    "    # Compute frequency distribution\n",
    "    output = nltk.FreqDist(bgs).items()\n",
    "\n",
    "    # Integrate All\n",
    "    bigram_list += output\n",
    "\n",
    "### merge the same tuple\n",
    "merged_bigram_freq_list = []\n",
    "cnt = 0\n",
    "for i, bigram_and_freq in enumerate(bigram_list):\n",
    "    check = False\n",
    "    for j, merged_bigram_and_freq in enumerate(merged_bigram_freq_list):\n",
    "        if bigram_and_freq[0] == merged_bigram_and_freq[0]:\n",
    "#             print 'here', bigram_and_freq[0], merged_bigram_and_freq[0]\n",
    "            merged_bigram_freq_list[j][1] = merged_bigram_and_freq[1] + bigram_and_freq[1] # merging freqeucny\n",
    "            check = True\n",
    "    if check == False:\n",
    "        temp_list = []\n",
    "        temp_list.append(bigram_and_freq[0])\n",
    "        temp_list.append(bigram_and_freq[1])\n",
    "        merged_bigram_freq_list.append(temp_list)\n",
    "    \n",
    "#     cnt += 1\n",
    "#     if cnt==50:\n",
    "#         break\n",
    "    \n",
    "# print 'total # of bi-gram:', len(merged_bigram_freq_list)\n",
    "# print merged_bigram_freq_list\n",
    "# print np.array(merged_bigram_freq_list).shape\n",
    "\n",
    "### Write Frequency Distribution\n",
    "text_file = open(PATH+\"freqeucny_distribution_2gram.txt\", \"w\")\n",
    "for bigram, freq in sorted(merged_bigram_freq_list, key=lambda x: x[1], reverse=True):\n",
    "    text_file.write(str(bigram))\n",
    "    text_file.write('\\t')\n",
    "    text_file.write(str(freq))\n",
    "    text_file.write('\\n')\n",
    "text_file.close()\n",
    "\n",
    "### Filter and make bigram voca set\n",
    "word_bi_gram_voca_set = []\n",
    "\n",
    "for bigram, freq in merged_bigram_freq_list:\n",
    "    if freq > thr_2gram:\n",
    "        word_bi_gram_voca_set.append(bigram)\n",
    "        \n",
    "print('2-gram voca size: ', len(word_bi_gram_voca_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Word-level Tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-gram voca size:  311\n"
     ]
    }
   ],
   "source": [
    "### create tri-gram set\n",
    "trigram_list = []\n",
    "for i, sentence in enumerate(X_train):\n",
    "    \n",
    "    # Extract trigrams from a single sentence\n",
    "    tgs = nltk.ngrams(sentence, 3)\n",
    "    \n",
    "    # Compute frequency distribution\n",
    "    output = nltk.FreqDist(tgs).items()\n",
    "\n",
    "    # Integrate All\n",
    "    trigram_list += output\n",
    "    \n",
    "### merge the same tuple\n",
    "merged_trigram_freq_list = []\n",
    "cnt = 0\n",
    "for i, trigram_and_freq in enumerate(trigram_list):\n",
    "\n",
    "    check = False\n",
    "    for j, merged_trigram_and_freq in enumerate(merged_trigram_freq_list):\n",
    "\n",
    "        if trigram_and_freq[0] == merged_trigram_and_freq[0]:\n",
    "#             print 'here', bigram_and_freq[0], merged_bigram_and_freq[0]\n",
    "            merged_trigram_freq_list[j][1] = merged_trigram_and_freq[1] + trigram_and_freq[1] # merging freqeucny\n",
    "            check = True\n",
    "            \n",
    "    if check == False:\n",
    "        temp_list = []\n",
    "        temp_list.append(trigram_and_freq[0])\n",
    "        temp_list.append(trigram_and_freq[1])\n",
    "        merged_trigram_freq_list.append(temp_list)\n",
    "        \n",
    "#     cnt += 1\n",
    "#     if cnt==50:\n",
    "#         break\n",
    "    \n",
    "# print 'total # of bi-gram:', len(merged_bigram_freq_list)\n",
    "# print merged_bigram_freq_list\n",
    "# print np.array(merged_bigram_freq_list).shape\n",
    "\n",
    "### Write Frequency Distribution\n",
    "text_file = open(PATH+\"freqeucny_distribution_3gram.txt\", \"w\")\n",
    "for trigram, freq in sorted(merged_trigram_freq_list, key=lambda x: x[1], reverse=True):\n",
    "    text_file.write(str(trigram))\n",
    "    text_file.write('\\t')\n",
    "    text_file.write(str(freq))\n",
    "    text_file.write('\\n')\n",
    "text_file.close()\n",
    "\n",
    "### Filter and make trigram voca set\n",
    "word_tri_gram_voca_set = []\n",
    "\n",
    "for trigram, freq in merged_trigram_freq_list:\n",
    "    if freq > thr_3gram:\n",
    "        word_tri_gram_voca_set.append(trigram)\n",
    "print('3-gram voca size: ', len(word_tri_gram_voca_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Word-level Five-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-gram voca size:  379\n"
     ]
    }
   ],
   "source": [
    "### create five-gram set\n",
    "fivegram_list = []\n",
    "for i, sentence in enumerate(X_train):\n",
    "    \n",
    "    # Extract trigrams from a single sentence\n",
    "    fgs = nltk.ngrams(sentence, 5)\n",
    "    \n",
    "    # Compute frequency distribution\n",
    "    output = nltk.FreqDist(fgs).items()\n",
    "\n",
    "    # Integrate All\n",
    "    fivegram_list += output\n",
    "    \n",
    "### merge the same tuple\n",
    "merged_fivegram_freq_list = []\n",
    "cnt = 0\n",
    "for i, fivegram_and_freq in enumerate(fivegram_list):\n",
    "\n",
    "    check = False\n",
    "    for j, merged_fivegram_and_freq in enumerate(merged_fivegram_freq_list):\n",
    "\n",
    "        if fivegram_and_freq[0] == merged_fivegram_and_freq[0]:\n",
    "#             print 'here', bigram_and_freq[0], merged_bigram_and_freq[0]\n",
    "            merged_fivegram_freq_list[j][1] = merged_fivegram_and_freq[1] + fivegram_and_freq[1] # merging freqeucny\n",
    "            check = True\n",
    "            \n",
    "    if check == False:\n",
    "        temp_list = []\n",
    "        temp_list.append(fivegram_and_freq[0])\n",
    "        temp_list.append(fivegram_and_freq[1])\n",
    "        merged_fivegram_freq_list.append(temp_list)\n",
    "        \n",
    "#     cnt += 1\n",
    "#     if cnt==50:\n",
    "#         break\n",
    "    \n",
    "# print 'total # of bi-gram:', len(merged_bigram_freq_list)\n",
    "# print merged_bigram_freq_list\n",
    "# print np.array(merged_bigram_freq_list).shape\n",
    "\n",
    "### Write Frequency Distribution\n",
    "text_file = open(PATH+\"freqeucny_distribution_5gram.txt\", \"w\")\n",
    "for fivegram, freq in sorted(merged_fivegram_freq_list, key=lambda x: x[1], reverse=True):\n",
    "    text_file.write(str(fivegram))\n",
    "    text_file.write('\\t')\n",
    "    text_file.write(str(freq))\n",
    "    text_file.write('\\n')\n",
    "text_file.close()\n",
    "\n",
    "### Filtering out\n",
    "word_five_gram_voca_set = []\n",
    "for fivegram, freq in merged_fivegram_freq_list:\n",
    "    if freq > thr_5gram:\n",
    "        word_five_gram_voca_set.append(fivegram)\n",
    "print('5-gram voca size: ', len(word_five_gram_voca_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word List per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ann_info = {0: 'component', 1: 'refinement_of_component', 2: 'action',\n",
    "#                 3: 'refinement_of_action',\n",
    "#                 4: 'condition', 5: 'priority', 6: 'motivation', 7: 'role',\n",
    "#                 8: 'object', 9: 'refinement_of_object',\n",
    "#                 10: 'sub_action', 11: 'sub_argument_of_action', 12: 'sub_priority',\n",
    "#                 13: 'sub_role', 14: 'sub_object',\n",
    "#                 15: 'sub_refinement_of_object', 16: 'none'}\n",
    "\n",
    "# word_count_classes = [[] for i in range(len(ann_info))]    \n",
    "\n",
    "# for i, sentence in enumerate(Y_train): # y label\n",
    "#     for j, token in enumerate(sentence):\n",
    "#         for k in range(0, len(ann_info)):\n",
    "#             if token == k:\n",
    "#                 word_count_classes[k].append(X_train[i][j]) # x data \n",
    "                \n",
    "# ### Write out             \n",
    "# def dict_wirte_to_txt(class_list, file_name):\n",
    "#     text_file = open(file_name, \"w\")\n",
    "#     class_dict = dict(Counter(class_list))\n",
    "#     for w in sorted(class_dict, key=class_dict.get, reverse=True):\n",
    "#         text_file.write(w)\n",
    "#         text_file.write('\\t')\n",
    "#         text_file.write(str(class_dict[w]))\n",
    "#         text_file.write('\\n')\n",
    "#     text_file.close()\n",
    "#     return class_dict\n",
    "\n",
    "# dict_classes = [None] * len(ann_info)\n",
    "# for k in range(0, len(ann_info)):\n",
    "#     dict_classes[k] = dict_wirte_to_txt(word_count_classes[k], PATH+str(ann_info[k])+'.txt')\n",
    "    \n",
    "# ### Filtering out and make voca set\n",
    "# def namestr(obj, namespace):\n",
    "#     return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "# def filtering_out_for_dict(dict):\n",
    "#     temp_voca_set = []\n",
    "#     for key, value in dict.items():\n",
    "#         if value > thr_1gram: # if only 1 frequency\n",
    "#             temp_voca_set.append(key)\n",
    "#     return temp_voca_set\n",
    "\n",
    "# vocab_classes = [None] * len(ann_info)\n",
    "# for k in range(0, len(ann_info)):\n",
    "#     vocab_classes[k] = filtering_out_for_dict(dict_classes[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 \t: \t component_dict 's voca size\n",
      "12 \t: \t refinement_of_component_dict 's voca size\n",
      "92 \t: \t action_dict 's voca size\n",
      "138 \t: \t refinement_of_action_dict 's voca size\n",
      "104 \t: \t condition_dict 's voca size\n",
      "40 \t: \t priority_dict 's voca size\n",
      "66 \t: \t motivation_dict 's voca size\n",
      "32 \t: \t role_dict 's voca size\n",
      "178 \t: \t object_dict 's voca size\n",
      "147 \t: \t refinement_of_object_dict 's voca size\n",
      "24 \t: \t sub_action_dict 's voca size\n",
      "33 \t: \t sub_argument_of_action_dict 's voca size\n",
      "3 \t: \t sub_priority_dict 's voca size\n",
      "2 \t: \t sub_role_dict 's voca size\n",
      "22 \t: \t sub_object_dict 's voca size\n",
      "28 \t: \t sub_refinement_of_object_dict 's voca size\n",
      "74 \t: \t none_dict 's voca size\n"
     ]
    }
   ],
   "source": [
    "component_w_li = []\n",
    "refinement_of_component_w_li = []\n",
    "action_w_li = []\n",
    "refinement_of_action_w_li = []\n",
    "condition_w_li = []\n",
    "priority_w_li = []\n",
    "motivation_w_li = []\n",
    "role_w_li = []\n",
    "object_w_li = []\n",
    "refinement_of_object_w_li = []\n",
    "sub_action_w_li = []\n",
    "sub_argument_of_action_w_li = []\n",
    "sub_priority_w_li = []\n",
    "sub_role_w_li = []\n",
    "sub_object_w_li = []\n",
    "sub_refinement_of_object_w_li = []\n",
    "none_w_li = []\n",
    "\n",
    "for i, sentence in enumerate(Y_train):\n",
    "    for j, token in enumerate(sentence):\n",
    "        if token==0:\n",
    "            component_w_li.append(X_train[i][j])\n",
    "        elif token==1:\n",
    "            refinement_of_component_w_li.append(X_train[i][j])\n",
    "        elif token==2:\n",
    "            action_w_li.append(X_train[i][j])\n",
    "        elif token==3:\n",
    "            refinement_of_action_w_li.append(X_train[i][j])\n",
    "        elif token==4:\n",
    "            condition_w_li.append(X_train[i][j])\n",
    "        elif token==5:\n",
    "            priority_w_li.append(X_train[i][j])\n",
    "        elif token==6:\n",
    "            motivation_w_li.append(X_train[i][j])\n",
    "        elif token==7:\n",
    "            role_w_li.append(X_train[i][j])\n",
    "        elif token==8:\n",
    "            object_w_li.append(X_train[i][j])\n",
    "        elif token==9:\n",
    "            refinement_of_object_w_li.append(X_train[i][j])\n",
    "        elif token==10:\n",
    "            sub_action_w_li.append(X_train[i][j])\n",
    "        elif token==11:\n",
    "            sub_argument_of_action_w_li.append(X_train[i][j])\n",
    "        elif token==12:\n",
    "            sub_priority_w_li.append(X_train[i][j])\n",
    "        elif token==13:\n",
    "            sub_role_w_li.append(X_train[i][j])\n",
    "        elif token==14:\n",
    "            sub_object_w_li.append(X_train[i][j])\n",
    "        elif token==15:\n",
    "            sub_refinement_of_object_w_li.append(X_train[i][j])\n",
    "        else:\n",
    "            none_w_li.append(X_train[i][j]) \n",
    "            \n",
    "\n",
    "### Write out             \n",
    "def dict_wirte_to_txt(class_list, file_name):\n",
    "    text_file = open(file_name, \"w\")\n",
    "    class_dict = dict(Counter(class_list))\n",
    "    for w in sorted(class_dict, key=class_dict.get, reverse=True):\n",
    "        text_file.write(w)\n",
    "        text_file.write('\\t')\n",
    "        text_file.write(str(class_dict[w]))\n",
    "        text_file.write('\\n')\n",
    "    text_file.close()\n",
    "    return class_dict\n",
    "\n",
    "component_dict = dict_wirte_to_txt(component_w_li, PATH+'frequency_distribution_class_component.txt')\n",
    "refinement_of_component_dict = dict_wirte_to_txt(refinement_of_component_w_li, PATH+'frequency_distribution_class_refinement_of_component.txt')\n",
    "action_dict = dict_wirte_to_txt(action_w_li, PATH+'frequency_distribution_class_action.txt')\n",
    "refinement_of_action_dict = dict_wirte_to_txt(refinement_of_action_w_li, PATH+'frequency_distribution_class_refinement_of_action.txt')\n",
    "condition_dict = dict_wirte_to_txt(condition_w_li, PATH+'frequency_distribution_class_condition.txt')\n",
    "priority_dict = dict_wirte_to_txt(priority_w_li, PATH+'frequency_distribution_class_priority.txt')\n",
    "motivation_dict = dict_wirte_to_txt(motivation_w_li, PATH+'frequency_distribution_class_motivation.txt')\n",
    "role_dict = dict_wirte_to_txt(role_w_li, PATH+'frequency_distribution_class_role.txt')\n",
    "object_dict = dict_wirte_to_txt(object_w_li, PATH+'frequency_distribution_class_object.txt')\n",
    "refinement_of_object_dict = dict_wirte_to_txt(refinement_of_object_w_li, PATH+'frequency_distribution_class_refinement_of_object.txt')\n",
    "sub_action_dict = dict_wirte_to_txt(sub_action_w_li, PATH+'frequency_distribution_class_sub_action.txt')\n",
    "sub_argument_of_action_dict = dict_wirte_to_txt(sub_argument_of_action_w_li, PATH+'frequency_distribution_class_sub_argument_of_action.txt')\n",
    "sub_priority_dict = dict_wirte_to_txt(sub_priority_w_li, PATH+'frequency_distribution_class_sub_priority.txt')\n",
    "sub_role_dict = dict_wirte_to_txt(sub_role_w_li, PATH+'frequency_distribution_class_sub_role.txt')\n",
    "sub_object_dict = dict_wirte_to_txt(sub_object_w_li, PATH+'frequency_distribution_class_sub_object.txt')\n",
    "sub_refinement_of_object_dict = dict_wirte_to_txt(sub_refinement_of_object_w_li, PATH+'frequency_distribution_class_sub_refinement_of_object.txt')\n",
    "none_dict = dict_wirte_to_txt(none_w_li, PATH+'frequency_distribution_class_none.txt')\n",
    "\n",
    "### Filtering out and make voca set\n",
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "def filtering_out_for_dict(dict):\n",
    "    temp_voca_set = []\n",
    "    for key, value in dict.items():\n",
    "        if value > thr_1gram: # if only 1 frequency\n",
    "            temp_voca_set.append(key)\n",
    "    print(len(temp_voca_set), '\\t: ' , namestr(dict, globals())[0],\"'s voca size\")\n",
    "    return temp_voca_set\n",
    "\n",
    "voca_set_class_component = filtering_out_for_dict(component_dict)\n",
    "voca_set_class_refinement_of_component = filtering_out_for_dict(refinement_of_component_dict)\n",
    "voca_set_class_action = filtering_out_for_dict(action_dict)\n",
    "voca_set_class_refinement_of_action = filtering_out_for_dict(refinement_of_action_dict)\n",
    "voca_set_class_condition = filtering_out_for_dict(condition_dict)\n",
    "voca_set_class_priority = filtering_out_for_dict(priority_dict)\n",
    "voca_set_class_motivation = filtering_out_for_dict(motivation_dict)\n",
    "voca_set_class_role = filtering_out_for_dict(role_dict)\n",
    "voca_set_class_object = filtering_out_for_dict(object_dict)\n",
    "voca_set_class_refinement_of_object = filtering_out_for_dict(refinement_of_object_dict)\n",
    "voca_set_class_sub_action = filtering_out_for_dict(sub_action_dict)\n",
    "voca_set_class_sub_argument_of_action = filtering_out_for_dict(sub_argument_of_action_dict)\n",
    "voca_set_class_sub_priority = filtering_out_for_dict(sub_priority_dict)\n",
    "voca_set_class_sub_role = filtering_out_for_dict(sub_role_dict)\n",
    "voca_set_class_sub_object = filtering_out_for_dict(sub_object_dict)\n",
    "voca_set_class_sub_refinement_of_object = filtering_out_for_dict(sub_refinement_of_object_dict)\n",
    "voca_set_class_none = filtering_out_for_dict(none_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Lookup Table\n",
    "\n",
    "* Lookup Table을 만들고 마지막 element에 unknown token/phrase 넣어주기.\n",
    "* dic index를 0으로 하지말고 1로 하자. 왜냐면 0은 나중에 sentence boudnary로 인해 exception될 경우 0으로 하자. 즉, [0,0,0,...,0,0,0] 벡터\n",
    "* 일단 모든 voca_set를 대상으로 unknown token을 명시해서 lookup table을 만들자. 그러고 나중에 unknown token을 사용하고 싶지 않을 때는 나중에 feature extraction할 때, 맨 뒤의 element만 삭제하도록 하자. (unknown token은 항상 lookup table list의 맨 뒤이다)\n",
    "* unknown을 모든 lookup table에 포함시키는 것은 생각해볼 여지가 있다. \n",
    "* class lookup table같은 경우는 unknown token은 제외시켜야 될 것 같다... \n",
    "* 즉, 상대적으로 voca set 크기가 적은 lookup table같은 경우는 unknown token을 포함하는게 좋을지 의문이 든다. \n",
    "* 적은 크기의 voca set의 lookup table에 unknown token을 넣지 않는 것은 잘못된 판단일수도 있다. 모든 class lookup table에 unknown token을 추가하니 2% 가량 성능이 올랐다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram voca size:  729\n",
      "2-gram voca size:  538\n",
      "3-gram voca size:  312\n",
      "5-gram voca size:  380\n"
     ]
    }
   ],
   "source": [
    "### Create word_one_gram Lookup Table\n",
    "word_one_gram_lookup_table = { }\n",
    "for i, token in enumerate(word_one_gram_voca_set):\n",
    "    word_one_gram_lookup_table[token] = i+1 # 1부터 시작하게 유도\n",
    "# Add unknown token\n",
    "word_one_gram_lookup_table['unknown_token'] = len(word_one_gram_lookup_table)+1\n",
    "print('1-gram voca size: ',len(word_one_gram_lookup_table))\n",
    "\n",
    "### Create word_bi_gram Lookup Table\n",
    "word_bi_gram_lookup_table = { }\n",
    "for i, token in enumerate(word_bi_gram_voca_set):\n",
    "    word_bi_gram_lookup_table[token] = i+1\n",
    "# Add unknown token\n",
    "word_bi_gram_lookup_table[('unknown_token', 'unknown_token')] = len(word_bi_gram_lookup_table)+1\n",
    "print('2-gram voca size: ', len(word_bi_gram_lookup_table))\n",
    "\n",
    "### Create word_tri_gram Lookup Table\n",
    "word_tri_gram_lookup_table = { }\n",
    "for i, token in enumerate(word_tri_gram_voca_set):\n",
    "    word_tri_gram_lookup_table[token] = i+1\n",
    "# Add unknown token\n",
    "word_tri_gram_lookup_table['unknown_token', 'unknown_token', 'unknown_token'] = len(word_tri_gram_lookup_table)+1\n",
    "print('3-gram voca size: ', len(word_tri_gram_lookup_table))\n",
    "\n",
    "### Create word_five_gram Lookup Table\n",
    "word_five_gram_lookup_table = { }\n",
    "for i, token in enumerate(word_five_gram_voca_set):\n",
    "    word_five_gram_lookup_table[token] = i+1\n",
    "# Add unknown token\n",
    "word_five_gram_lookup_table['unknown_token', 'unknown_token', 'unknown_token', 'unknown_token', 'unknown_token'] = len(word_five_gram_lookup_table)+1\n",
    "print('5-gram voca size: ', len(word_five_gram_lookup_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_table_1gram(voca_set_1gram):\n",
    "    temp_1gram_lookup_table = { } # dictionary\n",
    "    for i, token in enumerate(voca_set_1gram):\n",
    "        temp_1gram_lookup_table[token] = i+1 # 1부터 시작하게 유도, 사전에 있는 모든 단어들에 해당안되면 0벡터로 정의할거니까...\n",
    "    # Add unknown token\n",
    "    temp_1gram_lookup_table['unknown_token'] = len(temp_1gram_lookup_table)+1\n",
    "    return temp_1gram_lookup_table # lookup_table_dictionary\n",
    "\n",
    "lookup_table_class_component = create_lookup_table_1gram(voca_set_class_component)\n",
    "lookup_table_class_refinement_of_component = create_lookup_table_1gram(voca_set_class_refinement_of_component)\n",
    "lookup_table_class_action = create_lookup_table_1gram(voca_set_class_action)\n",
    "lookup_table_class_refinement_of_action = create_lookup_table_1gram(voca_set_class_refinement_of_action)\n",
    "lookup_table_class_condition = create_lookup_table_1gram(voca_set_class_condition)\n",
    "lookup_table_class_priority = create_lookup_table_1gram(voca_set_class_priority)\n",
    "lookup_table_class_motivation = create_lookup_table_1gram(voca_set_class_motivation)\n",
    "lookup_table_class_role = create_lookup_table_1gram(voca_set_class_role)\n",
    "lookup_table_class_object = create_lookup_table_1gram(voca_set_class_object)\n",
    "lookup_table_class_refinement_of_object = create_lookup_table_1gram(voca_set_class_refinement_of_object)\n",
    "lookup_table_class_sub_action = create_lookup_table_1gram(voca_set_class_sub_action)\n",
    "lookup_table_class_sub_argument_of_action = create_lookup_table_1gram(voca_set_class_sub_argument_of_action)\n",
    "lookup_table_class_sub_priority = create_lookup_table_1gram(voca_set_class_sub_priority)\n",
    "lookup_table_class_sub_role = create_lookup_table_1gram(voca_set_class_sub_role)\n",
    "lookup_table_class_sub_object = create_lookup_table_1gram(voca_set_class_sub_object)\n",
    "lookup_table_class_sub_refinement_of_object = create_lookup_table_1gram(voca_set_class_sub_refinement_of_object)\n",
    "lookup_table_class_none = create_lookup_table_1gram(voca_set_class_none)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dump(X_train, 'pre_X_train')\n",
    "dump(X_test, 'pre_X_test')\n",
    "dump(Y_train, 'pre_Y_train')\n",
    "dump(Y_test, 'pre_Y_test')\n",
    "\n",
    "dump(word_one_gram_lookup_table, 'word_one_gram_lookup_table')\n",
    "dump(word_bi_gram_lookup_table, 'word_bi_gram_lookup_table')\n",
    "dump(word_tri_gram_lookup_table, 'word_tri_gram_lookup_table')\n",
    "dump(word_five_gram_lookup_table, 'word_five_gram_lookup_table')\n",
    "\n",
    "dump(lookup_table_class_component, 'lookup_table_class_component')\n",
    "dump(lookup_table_class_refinement_of_component, 'lookup_table_class_refinement_of_component')\n",
    "dump(lookup_table_class_action, 'lookup_table_class_action')\n",
    "dump(lookup_table_class_refinement_of_action, 'lookup_table_class_refinement_of_action')\n",
    "dump(lookup_table_class_condition, 'lookup_table_class_condition')\n",
    "dump(lookup_table_class_priority, 'lookup_table_class_priority')\n",
    "dump(lookup_table_class_motivation, 'lookup_table_class_motivation')\n",
    "dump(lookup_table_class_role, 'lookup_table_class_role')\n",
    "dump(lookup_table_class_object, 'lookup_table_class_object')\n",
    "dump(lookup_table_class_refinement_of_object, 'lookup_table_class_refinement_of_object')\n",
    "dump(lookup_table_class_sub_action, 'lookup_table_class_sub_action')\n",
    "dump(lookup_table_class_sub_argument_of_action, 'lookup_table_class_sub_argument_of_action')\n",
    "dump(lookup_table_class_sub_priority, 'lookup_table_class_sub_priority')\n",
    "dump(lookup_table_class_sub_role, 'lookup_table_class_sub_role')\n",
    "dump(lookup_table_class_sub_object, 'lookup_table_class_sub_object')\n",
    "dump(lookup_table_class_sub_refinement_of_object, 'lookup_table_class_sub_refinement_of_object')\n",
    "dump(lookup_table_class_none, 'lookup_table_class_none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Data\n",
    "transform numerical values into vectorized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max(word2idx.items(), key=operator.itemgetter(1))[1] # vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i, point in enumerate(X):\n",
    "#     binary = \"{0:{fill}12b}\".format(point, fill='0')\n",
    "#     X[i] = list(binary)\n",
    "    \n",
    "# # for i, point in enumerate(Xtest):\n",
    "# #     binary = \"{0:{fill}12b}\".format(point, fill='0')\n",
    "# #     Xtest[i] = list(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.shape(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data array to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_df = pd.DataFrame(X)\n",
    "# X_df.to_csv(\"X.csv\", sep=',', header=False, index=False)\n",
    "\n",
    "# Y_df = pd.DataFrame(Y)\n",
    "# Y_df.to_csv(\"Y.csv\", sep=',', header=False, index=False)\n",
    "\n",
    "# # Xtest_df = pd.DataFrame(Xtest)\n",
    "# # Xtest_df.to_csv(\"Xtest.csv\", sep=',', header=False, index=False)\n",
    "\n",
    "# # Ytest_df = pd.DataFrame(Ytest)\n",
    "# # Ytest_df.to_csv(\"Ytest.csv\", sep=',', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
