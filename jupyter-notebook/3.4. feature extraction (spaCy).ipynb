{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(lookup_table_POS) =  15\n",
      "len(lookup_table_dependent) =  43\n"
     ]
    }
   ],
   "source": [
    "real_window_size_pos = 3\n",
    "real_window_size_np_chunk = 1\n",
    "real_window_size_n_dep = 5\n",
    "real_window_size_depth = 5\n",
    "real_window_size_n_siblings = 5 \n",
    "\n",
    "using_pos = 1\n",
    "using_chunking = 1\n",
    "using_parsing = 1\n",
    "using_dependency = 1\n",
    "#################################################################################################################\n",
    "window_size_pos = (real_window_size_pos * 2) + 1\n",
    "window_size_np_chunk = (real_window_size_np_chunk * 2) + 1\n",
    "window_size_n_dep = (real_window_size_n_dep * 2) + 1\n",
    "window_size_depth = (real_window_size_depth * 2) + 1\n",
    "window_size_n_siblings = (real_window_size_n_siblings * 2) + 1\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "from data_handler import *\n",
    "from feature_design_for_spaCy import *\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "#from spacy.en import English\n",
    "#parser = English()\n",
    "detokenizer = MosesDetokenizer()\n",
    "\n",
    "pre_X_train = load('X_train') \n",
    "pre_X_test = load('X_test') \n",
    "Y_train = load('Y_train')\n",
    "\n",
    "\n",
    "### FE2_X_train와 FE2_X_test 초기화\n",
    "FE4_X_train = []\n",
    "for sentence in (pre_X_train):\n",
    "    temp = []\n",
    "    for token in (sentence):\n",
    "        temp.append([])\n",
    "    FE4_X_train.append(temp)\n",
    "\n",
    "FE4_X_test = []\n",
    "for sentence in (pre_X_test):\n",
    "    temp = []\n",
    "    for token in (sentence):\n",
    "        temp.append([])\n",
    "    FE4_X_test.append(temp)\n",
    "    \n",
    "### For POS look-up table    \n",
    "list_spacy_pos = ['DET', 'ADJ', 'NOUN', 'VERB', 'PUNCT', 'CONJ', 'ADP', 'PART', 'ADV', 'PRON', 'NUM', 'PROPN', 'INTJ', 'X', 'SYM']\n",
    "lookup_table_spacy_pos = { }\n",
    "for i, token in enumerate(list_spacy_pos):\n",
    "    lookup_table_spacy_pos[token] = i # POS에서는 1부터(i+1) 시작안해도 된다. POS에는 unknown이 없기 때문이다.\n",
    "# Add unknown token\n",
    "print('len(lookup_table_POS) = ', len(lookup_table_spacy_pos))\n",
    "\n",
    "### For dependent look-up table\n",
    "list_dependent = [\n",
    "    'det','amod','compound','nsubj','aux','ROOT','dobj','punct','appos','conj','cc','prep','pobj','pcomp','nsubjpass',\\\n",
    "    'auxpass','xcomp','advmod','advcl','acl','neg','relcl','poss','mark','ccomp','nummod','agent','acomp','expl','attr',\\\n",
    "    'intj','oprd','preconj','nmod','npadvmod','prt','csubj','quantmod','case','predet','dep','dative','parataxis']\n",
    "lookup_table_dependent = { }\n",
    "for i, token in enumerate(list_dependent):\n",
    "    lookup_table_dependent[token] = i\n",
    "print('len(lookup_table_dependent) = ', len(lookup_table_dependent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before POS: 0\n",
      "After POS: 105\n"
     ]
    }
   ],
   "source": [
    "print('Before POS:', len(FE4_X_train[0][0]))\n",
    "#####################\n",
    "\"\"\" Training Data \"\"\"\n",
    "#####################\n",
    "for i, sentence in enumerate(pre_X_train):\n",
    "    \n",
    "    detokenized_sent = detokenizer.detokenize(sentence, return_str=True)\n",
    "    ### for exception [START]\n",
    "    list_detokenized_sent = list(detokenized_sent)\n",
    "    if list_detokenized_sent[-1] == '.' and list_detokenized_sent[-2].isupper() == True:\n",
    "        list_detokenized_sent[-1] = ' .'\n",
    "    new_detokenized_sent = \"\".join(list_detokenized_sent)\n",
    "    ### for exception [END]\n",
    "    spacy_sent = nlp(new_detokenized_sent)\n",
    "    \n",
    "\n",
    "    #print(len(sentence), len(spacy_sent))\n",
    "    #print(sentence)\n",
    "    #print(spacy_sent)\n",
    "    assert(len(sentence)==len(spacy_sent))\n",
    "    \n",
    "    \n",
    "    ##################################################################################################\n",
    "    for j, token in enumerate(spacy_sent):\n",
    "       \n",
    "        FE4_X_train[i][j] += FE_spaCy_POS(j, spacy_sent, lookup_table_spacy_pos, window_size_pos)\n",
    "        \n",
    "\n",
    "\n",
    "####################\n",
    "\"\"\" Testing Data \"\"\"\n",
    "####################      \n",
    "for i, sentence in enumerate(pre_X_test):\n",
    "    \n",
    "    detokenized_sent = detokenizer.detokenize(sentence, return_str=True)\n",
    "    ### for exception [START]\n",
    "    list_detokenized_sent = list(detokenized_sent)\n",
    "    if list_detokenized_sent[-1] == '.' and list_detokenized_sent[-2].isupper() == True:\n",
    "        list_detokenized_sent[-1] = ' .'\n",
    "    new_detokenized_sent = \"\".join(list_detokenized_sent)\n",
    "    ### for exception [END]\n",
    "    spacy_sent = nlp(new_detokenized_sent)\n",
    "    \n",
    "    \n",
    "    assert(len(sentence)==len(spacy_sent))\n",
    "    \n",
    "      \n",
    "    ##################################################################################################\n",
    "    for j, token in enumerate(spacy_sent):\n",
    "       \n",
    "        FE4_X_test[i][j] += FE_spaCy_POS(j, spacy_sent, lookup_table_spacy_pos, window_size_pos) \n",
    "        \n",
    "print('After POS:', len(FE4_X_train[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NP Chuck Position\n",
    "* == 한 문장에서 나온 NP Chunk List에서 첫 번재 NP Chunk인지 아닌지\n",
    "* == 한 문장에서 나온 NP Chunk List에서 마지막 NP Chunk인지 아닌지\n",
    "* ==== NP인지 아닌지\n",
    "* ==== ROOT에서 오른쪽에 있는 NP인지\n",
    "* ==== ROOT에서 왼쪽에 있는 NP인지\n",
    "* ==== ROOT에서 첫 번째 오른쪽에 있는 NP인지\n",
    "* ====== PP인지 아닌지 (PP 정의: NP앞에 전치사가 있다면 PP이다.)\n",
    "* ====== PP라면 어떤 종류의 PP인지??? (e.g. for, to, with, ...)\n",
    "* ====== ROOT에서 오른쪽에 있는 PP인지\n",
    "* ====== ROOT에서 왼쪽에 있는 PP인지\n",
    "* ====== ROOT에서 첫 번째 오른쪽에 있는 PP인지\n",
    "* ======== PP wich right NP 인지 아닌지 (PP 정의: NP앞에 전치사가 있다면 PP이다.)\n",
    "* ======== ROOT에서 오른쪽에 있는 PP Chunk wich right NP인지\n",
    "* ======== ROOT에서 왼쪽에 있는 PP Chunk wich right NP인지\n",
    "* ======== ROOT에서 첫 번째 오른쪽에 있는 PP wich right NP인지\n",
    "<br>\n",
    "\n",
    "* =======================================================================\n",
    "< 더 구현해야될 부분들... >\n",
    "* NP Chunk가 ROOT 바로 왼쪽에 있는지 (즉, ROOT 사이 또 다른 NP가 없는지)\n",
    "* NPPP 쌍을 PP기준으로만 구현했는데, NP기준으로도 해야되나...?\n",
    "\n",
    "* PP가 동사바로 오른쪽 옆에 있는지 아닌지\n",
    "* PP가 동사바로 오른쪽 옆, 옆에 있는지 아닌지\n",
    "* PP가 ROOT 바로 옆 (오른쪽/왼쪽) 에 있는지\n",
    "* PP가 또 다른 PP 오른쪽 옆에 있는지 아닌지 (또는 왼쪽 옆에 있는지 아닌지)\n",
    "\n",
    "\n",
    "<br>\n",
    "* second root를 정의해볼까??? (일단은 보류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i, sentence in enumerate(pre_X_train):\n",
    "    \n",
    "#     detokenized_sent = detokenizer.detokenize(sentence, return_str=True)\n",
    "#     ### for exception [START]\n",
    "#     list_detokenized_sent = list(detokenized_sent)\n",
    "#     if list_detokenized_sent[-1] == '.' and list_detokenized_sent[-2].isupper() == True:\n",
    "#         list_detokenized_sent[-1] = ' .'\n",
    "#     new_detokenized_sent = \"\".join(list_detokenized_sent)\n",
    "#     ### for exception [END]\n",
    "#     spacy_sent = nlp(new_detokenized_sent)\n",
    "#     assert(len(sentence)==len(spacy_sent))\n",
    "#     npchunk_numbered_sent = numbering_npchunk(spacy_sent)\n",
    "#     ppchunk_numbered_sent = numbering_ppchunk(npchunk_numbered_sent, spacy_sent)\n",
    "    \n",
    "#     whichprep_sent = numbering_which_ppchunk(ppchunk_numbered_sent, spacy_sent)\n",
    "    \n",
    "    \n",
    "#     for j, token in enumerate(spacy_sent):\n",
    "#         print(whichprep_sent[j])\n",
    "        \n",
    "#     print('==================================================================')\n",
    "    \n",
    "# #     print(spacy_sent)\n",
    "# #     print(npchunk_numbered_sent)\n",
    "# #     print(ppchunk_numbered_sent)\n",
    "# #     print(whichprep_sent)\n",
    "# #     print('\\n')\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# #     pp_isjustright_fromnp_numbered_sent = numbering_pp_isJustRight_fromNP_sent(npchunk_numbered_sent, ppchunk_numbered_sent)\n",
    "# #     ### For just right from Root \n",
    "# #     firstNP_fromroot_sent = Xchunk_isFirstRight_fromRoot(npchunk_numbered_sent, spacy_sent)\n",
    "# #     firstPP_fromroot_sent = Xchunk_isFirstRight_fromRoot(ppchunk_numbered_sent, spacy_sent)\n",
    "# #     firstPPjustrightNP_fromroot_sent = Xchunk_isFirstRight_fromRoot(pp_isjustright_fromnp_numbered_sent, spacy_sent)\n",
    "    \n",
    "# #     print(sentence)\n",
    "# #     print(npchunk_numbered_sent)\n",
    "# #     print(ppchunk_numbered_sent)\n",
    "# #     print(pp_isjustright_fromnp_numbered_sent)\n",
    "# #     print(firstNP_fromroot_sent)\n",
    "# #     print(firstPP_fromroot_sent)\n",
    "# #     print(firstPPjustrightNP_fromroot_sent)\n",
    "#     ####################\n",
    "#     #for j, token in enumerate(spacy_sent):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before POS: 105\n",
      "After POS: 134\n"
     ]
    }
   ],
   "source": [
    "print('Before POS:', len(FE4_X_train[0][0]))\n",
    "\n",
    "#####################\n",
    "\"\"\" Training Data \"\"\"\n",
    "##################### \n",
    "for i, sentence in enumerate(pre_X_train):\n",
    "    \n",
    "    detokenized_sent = detokenizer.detokenize(sentence, return_str=True)\n",
    "    ### for exception [START]\n",
    "    list_detokenized_sent = list(detokenized_sent)\n",
    "    if list_detokenized_sent[-1] == '.' and list_detokenized_sent[-2].isupper() == True:\n",
    "        list_detokenized_sent[-1] = ' .'\n",
    "    new_detokenized_sent = \"\".join(list_detokenized_sent)\n",
    "    ### for exception [END]\n",
    "    spacy_sent = nlp(new_detokenized_sent)\n",
    "    assert(len(sentence)==len(spacy_sent))\n",
    "    npchunk_numbered_sent = numbering_npchunk(spacy_sent)\n",
    "    ppchunk_numbered_sent = numbering_ppchunk(npchunk_numbered_sent, spacy_sent)\n",
    "    pp_isjustright_fromnp_numbered_sent = numbering_pp_isJustRight_fromNP_sent(npchunk_numbered_sent, ppchunk_numbered_sent)\n",
    "    ### For just right from Root (밑에 3개는 0또는1밖에없다.)\n",
    "    firstNP_fromroot_sent = Xchunk_isFirstRight_fromRoot(npchunk_numbered_sent, spacy_sent)\n",
    "    firstPP_fromroot_sent = Xchunk_isFirstRight_fromRoot(ppchunk_numbered_sent, spacy_sent)\n",
    "    firstPPjustrightNP_fromroot_sent = Xchunk_isFirstRight_fromRoot(pp_isjustright_fromnp_numbered_sent, spacy_sent)\n",
    "    ## for what is the prep\n",
    "    whichprep_sent = numbering_which_ppchunk(ppchunk_numbered_sent, spacy_sent)\n",
    "\n",
    "    ####################\n",
    "    for j, token in enumerate(spacy_sent):\n",
    "        \n",
    "        FE4_X_train[i][j] += is_first_np_chunk(j, npchunk_numbered_sent)\n",
    "        FE4_X_train[i][j] += is_last_np_chunk(j, npchunk_numbered_sent, spacy_sent)\n",
    "        FE4_X_train[i][j] += is_np_chuck(j, spacy_sent)\n",
    "        FE4_X_train[i][j] += np_isLeft_fromRoot(j, npchunk_numbered_sent, spacy_sent)\n",
    "        FE4_X_train[i][j] += np_isRight_fromRoot(j, npchunk_numbered_sent, spacy_sent)\n",
    "        FE4_X_train[i][j] += Xchunk_isLeft_fromSubRoot(j, npchunk_numbered_sent, spacy_sent)\n",
    "        FE4_X_train[i][j] += Xchunk_isRight_fromSubRoot(j, npchunk_numbered_sent, spacy_sent)\n",
    "        \n",
    "        # ppchunk_numbered_sent\n",
    "        FE4_X_train[i][j] += is_Xchunk(j, ppchunk_numbered_sent)\n",
    "        FE4_X_train[i][j] += Xchunk_isLeft_fromRoot(j, ppchunk_numbered_sent, spacy_sent)\n",
    "        FE4_X_train[i][j] += Xchunk_isRight_fromRoot(j, ppchunk_numbered_sent, spacy_sent)\n",
    "        FE4_X_train[i][j] += Xchunk_isLeft_fromSubRoot(j, ppchunk_numbered_sent, spacy_sent)\n",
    "        FE4_X_train[i][j] += Xchunk_isRight_fromSubRoot(j, ppchunk_numbered_sent, spacy_sent)        \n",
    "        \n",
    "        # pp_isjustright_fromnp_numbered_sent\n",
    "        FE4_X_train[i][j] += is_Xchunk(j, pp_isjustright_fromnp_numbered_sent)\n",
    "        FE4_X_train[i][j] += Xchunk_isLeft_fromRoot(j, pp_isjustright_fromnp_numbered_sent, spacy_sent)\n",
    "        FE4_X_train[i][j] += Xchunk_isRight_fromRoot(j, pp_isjustright_fromnp_numbered_sent, spacy_sent)\n",
    "        \n",
    "        # For just right from Root\n",
    "        FE4_X_train[i][j] += is_Xchunk(j, firstNP_fromroot_sent)\n",
    "        FE4_X_train[i][j] += is_Xchunk(j, firstPP_fromroot_sent)\n",
    "        FE4_X_train[i][j] += is_Xchunk(j, firstPPjustrightNP_fromroot_sent)\n",
    "        \n",
    "        # for what is the prep\n",
    "        FE4_X_train[i][j] += whichprep_sent[j]\n",
    "        \n",
    "####################\n",
    "\"\"\" Testing Data \"\"\"\n",
    "#################### \n",
    "for i, sentence in enumerate(pre_X_test):\n",
    "    \n",
    "    detokenized_sent = detokenizer.detokenize(sentence, return_str=True)\n",
    "    ### for exception [START]\n",
    "    list_detokenized_sent = list(detokenized_sent)\n",
    "    if list_detokenized_sent[-1] == '.' and list_detokenized_sent[-2].isupper() == True:\n",
    "        list_detokenized_sent[-1] = ' .'\n",
    "    new_detokenized_sent = \"\".join(list_detokenized_sent)\n",
    "    ### for exception [END]\n",
    "    spacy_sent = nlp(new_detokenized_sent)\n",
    "    assert(len(sentence)==len(spacy_sent))\n",
    "    npchunk_numbered_sent = numbering_npchunk(spacy_sent)\n",
    "    ppchunk_numbered_sent = numbering_ppchunk(npchunk_numbered_sent, spacy_sent)\n",
    "    pp_isjustright_fromnp_numbered_sent = numbering_pp_isJustRight_fromNP_sent(npchunk_numbered_sent, ppchunk_numbered_sent)    \n",
    "    assert(len(npchunk_numbered_sent)==len(ppchunk_numbered_sent)==len(pp_isjustright_fromnp_numbered_sent))\n",
    "    ### For just right from Root\n",
    "    firstNP_fromroot_sent = Xchunk_isFirstRight_fromRoot(npchunk_numbered_sent, spacy_sent)\n",
    "    firstPP_fromroot_sent = Xchunk_isFirstRight_fromRoot(ppchunk_numbered_sent, spacy_sent)\n",
    "    firstPPjustrightNP_fromroot_sent = Xchunk_isFirstRight_fromRoot(pp_isjustright_fromnp_numbered_sent, spacy_sent)\n",
    "    ## for what is the prep\n",
    "    whichprep_sent = numbering_which_ppchunk(ppchunk_numbered_sent, spacy_sent)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    for j, token in enumerate(spacy_sent):\n",
    "        \n",
    "        FE4_X_test[i][j] += is_first_np_chunk(j, npchunk_numbered_sent)\n",
    "        FE4_X_test[i][j] += is_last_np_chunk(j, npchunk_numbered_sent, spacy_sent)\n",
    "        FE4_X_test[i][j] += is_np_chuck(j, spacy_sent)\n",
    "        FE4_X_test[i][j] += np_isLeft_fromRoot(j, npchunk_numbered_sent, spacy_sent)\n",
    "        FE4_X_test[i][j] += np_isRight_fromRoot(j, npchunk_numbered_sent, spacy_sent)\n",
    "        FE4_X_test[i][j] += Xchunk_isLeft_fromSubRoot(j, npchunk_numbered_sent, spacy_sent)\n",
    "        FE4_X_test[i][j] += Xchunk_isRight_fromSubRoot(j, npchunk_numbered_sent, spacy_sent)\n",
    "        \n",
    "        # ppchunk_numbered_sent\n",
    "        FE4_X_test[i][j] += is_Xchunk(j, ppchunk_numbered_sent)\n",
    "        FE4_X_test[i][j] += Xchunk_isLeft_fromRoot(j, ppchunk_numbered_sent, spacy_sent)\n",
    "        FE4_X_test[i][j] += Xchunk_isRight_fromRoot(j, ppchunk_numbered_sent, spacy_sent)\n",
    "        FE4_X_test[i][j] += Xchunk_isLeft_fromSubRoot(j, ppchunk_numbered_sent, spacy_sent)\n",
    "        FE4_X_test[i][j] += Xchunk_isRight_fromSubRoot(j, ppchunk_numbered_sent, spacy_sent) \n",
    "        \n",
    "        # pp_isjustright_fromnp_numbered_sent\n",
    "        FE4_X_test[i][j] += is_Xchunk(j, pp_isjustright_fromnp_numbered_sent)\n",
    "        FE4_X_test[i][j] += Xchunk_isLeft_fromRoot(j, pp_isjustright_fromnp_numbered_sent, spacy_sent)\n",
    "        FE4_X_test[i][j] += Xchunk_isRight_fromRoot(j, pp_isjustright_fromnp_numbered_sent, spacy_sent)\n",
    "\n",
    "        # For just right from Root\n",
    "        FE4_X_test[i][j] += is_Xchunk(j, firstNP_fromroot_sent)\n",
    "        FE4_X_test[i][j] += is_Xchunk(j, firstPP_fromroot_sent)\n",
    "        FE4_X_test[i][j] += is_Xchunk(j, firstPPjustrightNP_fromroot_sent)        \n",
    "        \n",
    "        # for what is the prep\n",
    "        FE4_X_test[i][j] += whichprep_sent[j]        \n",
    "        \n",
    "\n",
    "print('After POS:', len(FE4_X_train[0][0]))\n",
    "\n",
    "# [A clinical lab section, the clinical site name, day, time, the lab]\n",
    "# A clinical lab section shall include the clinical site name, the class, instructor, day and time of the lab.\n",
    "# [1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 3, 0, 4, 0, 5, 5, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing (Dependency based Parsing Tree)\n",
    "* aux -> ROOT dependency인지 아닌지...(for sub priority)\n",
    "\n",
    "<br>\n",
    "* = ROOT까지 (token단위) 거리(distance) (normalization은 한 문장에서 총 token 개수)\n",
    "* = ROOT까지 (head단위) 거리(distance): 거리단위는 단어가 아니고, anscestor 개수, (normalization은 한 문장에서 \n",
    "    * 부사절에 속하는 단어들 또는 수식어구들이 root까지의 거리가 멀다.\n",
    "* === 자신의 dependency 크기\n",
    "* === 자신의 dependent 종류\n",
    "* === token이 ROOT 왼쪽에 있는지 오른쪽에 있는지\n",
    "* =========== token이 SUB ROOT (VERB) 왼쪽에 있는지 오른쪽에 있는지\n",
    "* =========== token이 SUB NOUN (NOUN) 왼쪽에 있는지 오른쪽에 있는지\n",
    "* ===== Head단어의 dependency 크기\n",
    "* ===== Head단어의 dependent 종류\n",
    "* ===== Head단어의 pos가 무엇인지\n",
    "* ======== ROOT까지가는 모든 dependent가 있는 path를 통해서 implictly하게 grouping할 수 있다.\n",
    "   * high-level의 ancestor의 dependent기준으로 그룹핑을 할 수 있다. (즉, 하위에 있는 dependent이거나 덜 중요한 dependent (compound, amod, appos, ..)들은 고려하지 않는다.\n",
    "   * 그룹핑 크기가 큰 순서부터 차례대로 그룹핑해준다. (advcl -> acl -> prep -> ...\n",
    "   * root까지가는 head list 중에서 가장 첫 번째의 verb head가 일치하는 단어들끼리의 묶음 -> implictly하게 sentence segmentation을 할 수 있다.\n",
    "   * 그 묶음의 동사가 ROOT동사인지 (즉, 주절인지)\n",
    "   * ROOT로가는 dependent path 길목 중에서 중요한 dependent가 있는지 없는지...\n",
    "   * voice구별안해도, dependent로 구별할 수 있다. nsubjpass와 nsubj로...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "* =========================== 고려해볼 점들...\n",
    "* head단어의 단어가 무엇인지 (1-gram lookup table과 연결)\n",
    "* sentence segmentation (clause단위로 나누기)\n",
    "* 여러개의 clause가 있다하면, 현재 token이 주절에 속하는지 안속하는지 (가장 높은 dependency subtree개수로 주절로 인식하자.) 그리고 부사절은 앞에서부터 numbering한다.\n",
    "* 동사의 voice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(pre_X_train):\n",
    "    detokenized_sent = detokenizer.detokenize(sentence, return_str=True)\n",
    "    ### for exception [START]\n",
    "    list_detokenized_sent = list(detokenized_sent)\n",
    "    if list_detokenized_sent[-1] == '.' and list_detokenized_sent[-2].isupper() == True:\n",
    "        list_detokenized_sent[-1] = ' .'\n",
    "    new_detokenized_sent = \"\".join(list_detokenized_sent)\n",
    "    ### for exception [END]\n",
    "    spacy_sent = nlp(new_detokenized_sent)\n",
    "    assert(len(sentence)==len(spacy_sent))\n",
    "    # For Normalization\n",
    "    len_sentence = len(sentence)\n",
    "    max_len_heads = lenMax_amongHeadList_toRoot(spacy_sent)\n",
    "    max_len_subtrees = lenMax_amongSubtreeList_toRoot(spacy_sent)\n",
    "\n",
    "    ####################\n",
    "    for j, token in enumerate(spacy_sent):\n",
    "        \n",
    "        # Distance\n",
    "        FE4_X_train[i][j] += normalization(len_sentence, 1, j+1)\n",
    "        FE4_X_train[i][j] += normalization(max_len_heads, 0, len(headList_to_root(token)))\n",
    "        \n",
    "        # Current token\n",
    "        FE4_X_train[i][j] += normalization(max_len_subtrees, 1, len(list(token.subtree)))\n",
    "        FE4_X_train[i][j] += what_is_myDependent(token, lookup_table_dependent)\n",
    "        FE4_X_train[i][j] += is_LeftRight_fromRoot(j, spacy_sent)\n",
    "        \n",
    "        # Head token\n",
    "        FE4_X_train[i][j] += normalization(max_len_subtrees, 1, len(list(token.head.subtree)))\n",
    "        FE4_X_train[i][j] += what_is_headDependent(token, lookup_table_dependent)\n",
    "        FE4_X_train[i][j] += what_is_headPOS(token, lookup_table_spacy_pos)\n",
    "        \n",
    "        # Implicitly Grouping\n",
    "        headlist_to_root = [(dependent_token.dep_) for dependent_token in headList_to_root(token)]\n",
    "        headpath_to_root = remove_duplicate(headlist_to_root)        \n",
    "        FE4_X_train[i][j] += high_level_implicit_grouping(headpath_to_root)\n",
    "        \n",
    "        # for sub prioirty\n",
    "        FE4_X_train[i][j] += is_subpriority(j, spacy_sent)\n",
    "        \n",
    "        # SUB ROOT & SUB NOUN\n",
    "        FE4_X_train[i][j] += is_middle_between_RtAndSubRt(j, spacy_sent)\n",
    "        FE4_X_train[i][j] += is_Right_fromSubRoot(j, spacy_sent)\n",
    "        \n",
    "        ### Ancestors와 children은 패턴이 안보여서 일단 pass\n",
    "        #print(token, '---' , headList_to_root(token))\n",
    "        #print(token, len(list(token.subtree)))\n",
    "        #print(token, '\\t', len(list(token.subtree)),len(list(token.ancestors)),len(list(token.children)), '\\t\\t', list(token.ancestors), '\\t\\t', list(token.children)  )\n",
    "        \n",
    "#     for j, token in enumerate(spacy_sent):    \n",
    "#         print(token.orth_,'\\t', token.dep_,'\\t', token.head.orth_, [t.orth_ for t in token.lefts], [t.orth_ for t in token.rights])\n",
    "   \n",
    "#     print('-----------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(pre_X_test):\n",
    "    detokenized_sent = detokenizer.detokenize(sentence, return_str=True)\n",
    "    ### for exception [START]\n",
    "    list_detokenized_sent = list(detokenized_sent)\n",
    "    if list_detokenized_sent[-1] == '.' and list_detokenized_sent[-2].isupper() == True:\n",
    "        list_detokenized_sent[-1] = ' .'\n",
    "    new_detokenized_sent = \"\".join(list_detokenized_sent)\n",
    "    ### for exception [END]\n",
    "    spacy_sent = nlp(new_detokenized_sent)\n",
    "    assert(len(sentence)==len(spacy_sent))\n",
    "    # For Normalization\n",
    "    len_sentence = len(sentence)\n",
    "    max_len_heads = lenMax_amongHeadList_toRoot(spacy_sent)\n",
    "    max_len_subtrees = lenMax_amongSubtreeList_toRoot(spacy_sent)\n",
    "\n",
    "    ####################\n",
    "    for j, token in enumerate(spacy_sent):\n",
    "        \n",
    "        # Distance\n",
    "        FE4_X_test[i][j] += normalization(len_sentence, 1, j+1)\n",
    "        FE4_X_test[i][j] += normalization(max_len_heads, 0, len(headList_to_root(token)))\n",
    "        \n",
    "        # Current token\n",
    "        FE4_X_test[i][j] += normalization(max_len_subtrees, 1, len(list(token.subtree)))\n",
    "        FE4_X_test[i][j] += what_is_myDependent(token, lookup_table_dependent)\n",
    "        FE4_X_test[i][j] += is_LeftRight_fromRoot(j, spacy_sent)\n",
    "        \n",
    "        # Head token\n",
    "        FE4_X_test[i][j] += normalization(max_len_subtrees, 1, len(list(token.head.subtree)))\n",
    "        FE4_X_test[i][j] += what_is_headDependent(token, lookup_table_dependent)\n",
    "        FE4_X_test[i][j] += what_is_headPOS(token, lookup_table_spacy_pos)\n",
    "        \n",
    "        # Implicitly Grouping\n",
    "        headlist_to_root = [(dependent_token.dep_) for dependent_token in headList_to_root(token)]\n",
    "        headpath_to_root = remove_duplicate(headlist_to_root)        \n",
    "        FE4_X_test[i][j] += high_level_implicit_grouping(headpath_to_root)\n",
    "\n",
    "        # for sub prioirty\n",
    "        FE4_X_test[i][j] += is_subpriority(j, spacy_sent)\n",
    "        \n",
    "        # SUB ROOT & SUB NOUN\n",
    "        FE4_X_test[i][j] += is_middle_between_RtAndSubRt(j, spacy_sent)\n",
    "        FE4_X_test[i][j] += is_Right_fromSubRoot(j, spacy_sent)\n",
    "        \n",
    "        ### Ancestors와 children은 패턴이 안보여서 일단 pass\n",
    "        #print(token, '---' , headList_to_root(token))\n",
    "        #print(token, len(list(token.subtree)))\n",
    "        #print(token, '\\t', len(list(token.subtree)),len(list(token.ancestors)),len(list(token.children)), '\\t\\t', list(token.ancestors), '\\t\\t', list(token.children)  )\n",
    "        \n",
    "#     for j, token in enumerate(spacy_sent):    \n",
    "#         print(token.orth_,'\\t', token.dep_,'\\t', token.head.orth_, [t.orth_ for t in token.lefts], [t.orth_ for t in token.rights])\n",
    "   \n",
    "#     print('-----------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Total feature length = 295\n"
     ]
    }
   ],
   "source": [
    "dump(FE4_X_train, 'FE4_X_train')\n",
    "dump(FE4_X_test, 'FE4_X_test')\n",
    "print('===> Total feature length =', len(FE4_X_train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # all you have to do to parse text is this:\n",
    "# #note: the first time you run spaCy in a file it takes a little while to load up its modules\n",
    "# doc = nlp('If you have a product like message, as a user I want to be able to give it to him by using them, so that I can remove expired and erroneous of ads.')\n",
    "\n",
    "# # Let's look at the tokens\n",
    "# # All you have to do is iterate through the parsedData\n",
    "# # Each token is an object with lots of different properties\n",
    "# # A property with an underscore at the end returns the string representation\n",
    "# # while a property without the underscore returns an index (int) into spaCy's vocabulary\n",
    "# # The probability estimate is based on counts from a 3 billion word\n",
    "# # corpus, smoothed using the Simple Good-Turing method.\n",
    "# for i, token in enumerate(doc):\n",
    "#     print(\"original:\", token.orth, token.orth_)\n",
    "#     print(\"lowercased:\", token.lower, token.lower_)\n",
    "#     print(\"lemma:\", token.lemma, token.lemma_)\n",
    "#     print(\"shape:\", token.shape, token.shape_)\n",
    "#     print(\"prefix:\", token.prefix, token.prefix_)\n",
    "#     print(\"suffix:\", token.suffix, token.suffix_)\n",
    "#     print(\"log probability:\", token.prob)\n",
    "#     print(\"Brown cluster id:\", token.cluster)\n",
    "#     print(\"TOKEN POS:\", token.pos_)\n",
    "#     print(\"Word Vector:\", token.vector.shape)\n",
    "#     print(\"----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As\n",
      "As-mark-> get-advcl-> get-advcl-> like-ROOT\n",
      "\n",
      "\n",
      "I\n",
      "I-nsubj-> get-advcl-> get-advcl-> like-ROOT\n",
      "\n",
      "\n",
      "get\n",
      "get-advcl-> like-ROOT\n",
      "\n",
      "\n",
      "cramps\n",
      "cramps-dobj-> get-advcl-> get-advcl-> like-ROOT\n",
      "\n",
      "\n",
      "in\n",
      "in-prep-> get-advcl-> get-advcl-> like-ROOT\n",
      "\n",
      "\n",
      "my\n",
      "my-poss-> hand-pobj-> hand-pobj-> in-prep-> in-prep-> get-advcl-> get-advcl-> like-ROOT\n",
      "\n",
      "\n",
      "right\n",
      "right-amod-> hand-pobj-> hand-pobj-> in-prep-> in-prep-> get-advcl-> get-advcl-> like-ROOT\n",
      "\n",
      "\n",
      "hand\n",
      "hand-pobj-> in-prep-> in-prep-> get-advcl-> get-advcl-> like-ROOT\n",
      "\n",
      "\n",
      "from\n",
      "from-prep-> get-advcl-> get-advcl-> like-ROOT\n",
      "\n",
      "\n",
      "holding\n",
      "holding-pcomp-> from-prep-> from-prep-> get-advcl-> get-advcl-> like-ROOT\n",
      "\n",
      "\n",
      "the\n",
      "the-det-> mouse-dobj-> mouse-dobj-> holding-pcomp-> holding-pcomp-> from-prep-> from-prep-> get-advcl-> get-advcl-> like-ROOT\n",
      "\n",
      "\n",
      "mouse\n",
      "mouse-dobj-> holding-pcomp-> holding-pcomp-> from-prep-> from-prep-> get-advcl-> get-advcl-> like-ROOT\n",
      "\n",
      "\n",
      ",\n",
      ",-punct-> like-ROOT\n",
      "\n",
      "\n",
      "I\n",
      "I-nsubj-> like-ROOT\n",
      "\n",
      "\n",
      "would\n",
      "would-aux-> like-ROOT\n",
      "\n",
      "\n",
      "like\n",
      "\n",
      "\n",
      "\n",
      "to\n",
      "to-aux-> have-xcomp-> have-xcomp-> like-ROOT\n",
      "\n",
      "\n",
      "have\n",
      "have-xcomp-> like-ROOT\n",
      "\n",
      "\n",
      "some\n",
      "some-det-> hotkeys-dobj-> hotkeys-dobj-> have-xcomp-> have-xcomp-> like-ROOT\n",
      "\n",
      "\n",
      "hotkeys\n",
      "hotkeys-dobj-> have-xcomp-> have-xcomp-> like-ROOT\n",
      "\n",
      "\n",
      "so\n",
      "so-mark-> have-advcl-> have-advcl-> have-xcomp-> have-xcomp-> like-ROOT\n",
      "\n",
      "\n",
      "that\n",
      "that-mark-> have-advcl-> have-advcl-> have-xcomp-> have-xcomp-> like-ROOT\n",
      "\n",
      "\n",
      "I\n",
      "I-nsubj-> have-advcl-> have-advcl-> have-xcomp-> have-xcomp-> like-ROOT\n",
      "\n",
      "\n",
      "can\n",
      "can-aux-> have-advcl-> have-advcl-> have-xcomp-> have-xcomp-> like-ROOT\n",
      "\n",
      "\n",
      "have\n",
      "have-advcl-> have-xcomp-> have-xcomp-> like-ROOT\n",
      "\n",
      "\n",
      "cramps\n",
      "cramps-dobj-> have-advcl-> have-advcl-> have-xcomp-> have-xcomp-> like-ROOT\n",
      "\n",
      "\n",
      "in\n",
      "in-prep-> have-advcl-> have-advcl-> have-xcomp-> have-xcomp-> like-ROOT\n",
      "\n",
      "\n",
      "my\n",
      "my-poss-> hand-pobj-> hand-pobj-> in-prep-> in-prep-> have-advcl-> have-advcl-> have-xcomp-> have-xcomp-> like-ROOT\n",
      "\n",
      "\n",
      "left\n",
      "left-amod-> hand-pobj-> hand-pobj-> in-prep-> in-prep-> have-advcl-> have-advcl-> have-xcomp-> have-xcomp-> like-ROOT\n",
      "\n",
      "\n",
      "hand\n",
      "hand-pobj-> in-prep-> in-prep-> have-advcl-> have-advcl-> have-xcomp-> have-xcomp-> like-ROOT\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dependency parsing이 제대로 동작하는 않는 경우도 많다.\n",
    "# 특히 동사에 따라 dependent가 바뀐다. 구조는 똑같지만,..... \n",
    "# if 를 that으로 바꾸면, 어느정도 advcl을 면할 수는 있지만, 특정 동사들에 따라서 advcl를 가지게 되는 경우가 종종 있다. 이경우는 감수해야 될 듯하다.\n",
    "# Process sentences 'Hello, world. Natural Language Processing in 10 lines of code.' using spaCy\n",
    "text1 = 'It would be nice to have a feature whereby you can reset the statistics,  but still retain the existing statistics.'\n",
    "text2 = 'It would be great that there was the ability to configure a Today button to be displayed just under the calendar.'\n",
    "text3 = 'It would be nice in the future that you could save two or three user generated filename patterns.'\n",
    "text4 = 'It would be very nice that I could control the TV volume directly in the program.'\n",
    "text5 = 'It would be great that there was the ability to configure a Today button to be displayed just under the calendar.'\n",
    "text6 = 'It would be a very nice that I could control the TV volume directly in the program.'\n",
    "text7 = 'it Would be nice that you could have the width of the indent and use tabs or spaces.'\n",
    "text8 = 'it Would be nice that you could configure the width of the indent and use tabs or spaces.'\n",
    "\n",
    "text9 = 'It would be nice to configure a ticker bar at the bottom of the UI window that scrolls clickable track names chosen by the JukeDaddy,  configured in the same way as AutoPlay.'\n",
    "text10 = 'It would be nice that there were some way to Synchronise/subscribe to my published phpiCalendar with SyncMLcompliant mobile devices.'\n",
    "text11 = 'It would be nice that you could be an hide option for tabbed document inside a docking container similar to visual studio'\n",
    "text12 = 'Some sort of mini checkmark would be nice, so that users can chose if they want to certain lines to be excluded/included from a diff.'\n",
    "text13 = 'It would be nice if the configuration file would allow the admin to give text names to the drop down list.'\n",
    "text14 = \"It would be nice that the focus was automatically set to the text fields in PS and Tex (so that we don't have to click on them to start typing)\"\n",
    "text15 = \"It would really be nice that the class XYSeriesCollection would provide a method public Vector getSeries that would return a Vector with XYSeries objects.\"\n",
    "text16 = \"As an alternative it could be nice that the application could be launched from the terminal with the .csv of .xml file path as a parameter.\"\n",
    "text17 = 'As a salesperson, I would like for the UI of the application to remain responsive while the report is loading, so that I can cancel the report if desired.'\n",
    "\n",
    "t1 = 'The report of needed classes shall include (but not be limited to) classes to be offered, number of sections needed, number of labs needed, and room types needed'\n",
    "t1 = ''# dependency parsing이 제대로 동작하는 않는 경우도 많다.\n",
    "# 특히 동사에 따라 dependent가 바뀐다. 구조는 똑같지만,..... \n",
    "# if 를 that으로 바꾸면, 어느정도 advcl을 면할 수는 있지만, 특정 동사들에 따라서 advcl를 가지게 되는 경우가 종종 있다. 이경우는 감수해야 될 듯하다.\n",
    "# Process sentences 'Hello, world. Natural Language Processing in 10 lines of code.' using spaCy\n",
    "text1 = 'It would be nice to have a feature whereby you can reset the statistics,  but still retain the existing statistics.'\n",
    "text20 = 'there was the ability to configure a Today button to be displayed just under the calendar.'\n",
    "\n",
    "\n",
    "text3 = 'It would be nice in the future that you could save two or three user generated filename patterns.'\n",
    "text4 = 'It would be very nice that I could control the TV volume directly in the program.'\n",
    "text5 = 'It would be great that there was the ability to configure a Today button to be displayed just under the calendar.'\n",
    "text6 = 'It would be a very nice that I could control the TV volume directly in the program.'\n",
    "text7 = 'it Would be nice that you could have the width of the indent and use tabs or spaces.'\n",
    "text8 = 'it Would be nice that you could configure the width of the indent and use tabs or spaces.'\n",
    "\n",
    "text9 = 'It would configure a ticker bar at the bottom of the UI window that scrolls clickable track names chosen by the JukeDaddy,  configured in the same way as AutoPlay.'\n",
    "text10 = 'It would be nice that there were some way to Synchronise/subscribe to my published phpiCalendar with SyncMLcompliant mobile devices.'\n",
    "text11 = 'It would be nice that you could be an hide option for tabbed document inside a docking container similar to visual studio'\n",
    "text12 = 'Some sort of mini checkmark would be nice, so that users can chose if they want to certain lines to be excluded/included from a diff.'\n",
    "text13 = 'It would be nice if the configuration file would allow the admin to give text names to the drop down list.'\n",
    "text14 = \"It would be nice that the focus was automatically set to the text fields in PS and Tex (so that we don't have to click on them to start typing)\"\n",
    "text15 = \"It would really be nice that the class XYSeriesCollection would provide a method public Vector getSeries that would return a Vector with XYSeries objects.\"\n",
    "text16 = \"As an alternative it could be nice that the application could be launched from the terminal with the .csv of .xml file path as a parameter.\"\n",
    "text17 = 'As a salesperson, I would like for the UI of the application to remain responsive while the report is loading, so that I can cancel the report if desired.'\n",
    "\n",
    "t1 = 'The report of needed classes shall include (but not be limited to) classes to be offered, number of sections needed, number of labs needed, and room types needed'\n",
    "t1 = 'Classes for a given cohort shall not conflict with regards to the time and day that they are offered.'\n",
    "t1 = \"The Disputes System must be beutiful by both internal and external users.\"\n",
    "\n",
    "t1 = \"The IQA subsystem shall take into account current inventory status and adjust inventory of substitutionary ingredients accordingly.\"\n",
    "t1 = 'Product shall allow entering, storing and modifying product menus used by POS terminals.'\n",
    "t1 = 'PME Subsystem shall allow keeping submenus within menus and products within submenus in a managed and persisted order.\"'\n",
    "t1 = \"The RMS System shall have Windows XP looking and feeling and use GUI futures making its use intuitive\"\n",
    "h1 = 'The system shall have a MDI form that allows for the viewing of the graph and the data table.'\n",
    "t1 = 'The system shall offer the ability to pause and resume the refresh of data.'\n",
    "t1 = 'The Disputes System must prevent users from accessing any dispute cases that do not belong to their cardholder base.'\n",
    "#t1='The Disputes System must allow the user to create three unique types of ticket retrieval requests.'\n",
    "#t1=\"the search functionality must further allow the user to limit the results of the search by a date range, the type of dispute (ticket retrieval request or chargeback notification), the case status (open, closed or all) and the dispute reason code.\"\n",
    "#t1=\"The Disputes System shall provide the users with the ability to view the history of all activities that have occurred on the dispute case.\"\n",
    "t1='Website shall allow customers to view reviews of selected movies by other customers'\n",
    "t1='It would get also Information about user specified packages installed on that system like glibc or apache or mysql etc.'\n",
    "t1=\"It would be great to have columns in the Parent and Student interfaces where users could see calendar and web page links added by faculty.\"\n",
    "t1=\"A team member can view the current burndown chart on the status page, and can click it for a larger view.\"\n",
    "t1='It would be nice that you could save two or three user generated by filename patterns so that I can easily switch between them'\n",
    "t1 = 'It would be nice to add a column to mailbox table so that this would be used to store a path to user sieve script'\n",
    "#t1='As a user I want to be able to manage ads, so that I can remove expired and erroneous ads.'\n",
    "t1='It would be great that you could add the aspect ratio of the possible resolutions one can choose from.'\n",
    "t2='As I get cramps in my right hand from holding the mouse, I would like to have some hotkeys so that I can have cramps in my left hand'\n",
    "t1='is it possible to add a Description column in the Attributes & Operations html table that I can put some comment in my Delphi surce code and Model just pull these comment into the html document?'\n",
    "t1='It would be nice to have a feature whereby you can reset the statistics, but still retain the existing statistics.'\n",
    "t1='Combined with tcp/ip stack it could be handy to set a treshold in which someone can specify how many bad clusters is acceptable.'\n",
    "t1='A Program of Study shall consist of a program name and listing of required classes (both clinical and nonclinical) that must be completed.'\n",
    "h1='The system shall allow a Program Administrator or Nursing staff member to remove a student from a clinical lab section.'\n",
    "tt1='allow option to hide tray icon.'\n",
    "doc = nlp(t2)\n",
    "\n",
    "\n",
    "# # For every token in document, print it's tokens to the root\n",
    "# for token in doc:\n",
    "#     print('{} --> {}'.format(token, headList_to_root(token)))\n",
    "\n",
    "# Print dependency labels of the tokens\n",
    "# for token in doc:\n",
    "#     print(token)\n",
    "#     print(token.dep_, token.head.dep_, token.head.head.dep_)\n",
    "# import operator\n",
    "# verb_withNsubtree = {}\n",
    "\n",
    "\n",
    "# for i, token in enumerate(doc):\n",
    "\n",
    "#     if token.pos_ == 'VERB':\n",
    "#         verb_withNsubtree[i] = len(list(token.subtree))\n",
    "        \n",
    "#     #print(token, '---' , headList_to_root(token))\n",
    "# #     print(token, token.pos_ ,'\\t', len(list(token.subtree)))\n",
    "#     #print(token, '\\t', len(list(token.subtree)),len(list(token.ancestors)),len(list(token.children)), '\\t\\t', list(token.ancestors), '\\t\\t', list(token.children)  )\n",
    "\n",
    "# print(verb_withNsubtree)\n",
    "\n",
    "# verb_withNsubtree = sorted(verb_withNsubtree.items(), key=operator.itemgetter(1), reverse=True) \n",
    "# print(verb_withNsubtree)\n",
    "\n",
    "for token in doc:\n",
    "    print(token)\n",
    "    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in headList_to_root(token)]))\n",
    "    print('\\n')\n",
    "doc = nlp(t1)\n",
    "\n",
    "\n",
    "# # For every token in document, print it's tokens to the root\n",
    "# for token in doc:\n",
    "#     print('{} --> {}'.format(token, headList_to_root(token)))\n",
    "\n",
    "# Print dependency labels of the tokens\n",
    "# for token in doc:\n",
    "#     print(token)\n",
    "#     print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in headList_to_root(token)]))\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They\n",
      "They-nsubj-> gave-ROOT\n",
      "\n",
      "\n",
      "gave\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "the-det-> money-dobj-> money-dobj-> gave-ROOT\n",
      "\n",
      "\n",
      "money\n",
      "money-dobj-> gave-ROOT\n",
      "\n",
      "\n",
      "to\n",
      "to-prep-> gave-ROOT\n",
      "\n",
      "\n",
      "whoever\n",
      "whoever-nsubj-> presented-pcomp-> presented-pcomp-> to-prep-> to-prep-> gave-ROOT\n",
      "\n",
      "\n",
      "presented\n",
      "presented-pcomp-> to-prep-> to-prep-> gave-ROOT\n",
      "\n",
      "\n",
      "the\n",
      "the-det-> ticket-dobj-> ticket-dobj-> presented-pcomp-> presented-pcomp-> to-prep-> to-prep-> gave-ROOT\n",
      "\n",
      "\n",
      "winning\n",
      "winning-amod-> ticket-dobj-> ticket-dobj-> presented-pcomp-> presented-pcomp-> to-prep-> to-prep-> gave-ROOT\n",
      "\n",
      "\n",
      "ticket\n",
      "ticket-dobj-> presented-pcomp-> presented-pcomp-> to-prep-> to-prep-> gave-ROOT\n",
      "\n",
      "\n",
      ".\n",
      ".-punct-> gave-ROOT\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h1='They gave the money to whoever presented the winning ticket.'\n",
    "doc = nlp(h1)\n",
    "\n",
    "for token in doc:\n",
    "    print(token)\n",
    "    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in headList_to_root(token)]))\n",
    "    print('\\n')\n",
    "doc = nlp(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n",
      "He-nsubj-> was-ROOT\n",
      "\n",
      "\n",
      "was\n",
      "\n",
      "\n",
      "\n",
      "a\n",
      "a-det-> boy-attr-> boy-attr-> was-ROOT\n",
      "\n",
      "\n",
      "good\n",
      "good-amod-> boy-attr-> boy-attr-> was-ROOT\n",
      "\n",
      "\n",
      "boy\n",
      "boy-attr-> was-ROOT\n",
      "\n",
      "\n",
      "who\n",
      "who-nsubj-> helps-relcl-> helps-relcl-> boy-attr-> boy-attr-> was-ROOT\n",
      "\n",
      "\n",
      "helps\n",
      "helps-relcl-> boy-attr-> boy-attr-> was-ROOT\n",
      "\n",
      "\n",
      "mother\n",
      "mother-dobj-> helps-relcl-> helps-relcl-> boy-attr-> boy-attr-> was-ROOT\n",
      "\n",
      "\n",
      ".\n",
      ".-punct-> was-ROOT\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h1='He was a good boy who helps mother.'\n",
    "doc = nlp(h1)\n",
    "\n",
    "for token in doc:\n",
    "    print(token)\n",
    "    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in headList_to_root(token)]))\n",
    "    print('\\n')\n",
    "doc = nlp(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n",
      "He-nsubj-> has-ROOT\n",
      "\n",
      "\n",
      "has\n",
      "\n",
      "\n",
      "\n",
      "a\n",
      "a-det-> bag-dobj-> bag-dobj-> has-ROOT\n",
      "\n",
      "\n",
      "bag\n",
      "bag-dobj-> has-ROOT\n",
      "\n",
      "\n",
      "in\n",
      "in-prep-> has-ROOT\n",
      "\n",
      "\n",
      "order\n",
      "order-pobj-> in-prep-> in-prep-> has-ROOT\n",
      "\n",
      "\n",
      "to\n",
      "to-aux-> swim-acl-> swim-acl-> order-pobj-> order-pobj-> in-prep-> in-prep-> has-ROOT\n",
      "\n",
      "\n",
      "swim\n",
      "swim-acl-> order-pobj-> order-pobj-> in-prep-> in-prep-> has-ROOT\n",
      "\n",
      "\n",
      ".\n",
      ".-punct-> has-ROOT\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h1='He has a bag in order to swim.'\n",
    "doc = nlp(h1)\n",
    "\n",
    "for token in doc:\n",
    "    print(token)\n",
    "    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in headList_to_root(token)]))\n",
    "    print('\\n')\n",
    "doc = nlp(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n",
      "He-nsubj-> has-ROOT\n",
      "\n",
      "\n",
      "has\n",
      "\n",
      "\n",
      "\n",
      "a\n",
      "a-det-> bag-dobj-> bag-dobj-> has-ROOT\n",
      "\n",
      "\n",
      "bag\n",
      "bag-dobj-> has-ROOT\n",
      "\n",
      "\n",
      "which\n",
      "which-nsubj-> is-relcl-> is-relcl-> bag-dobj-> bag-dobj-> has-ROOT\n",
      "\n",
      "\n",
      "is\n",
      "is-relcl-> bag-dobj-> bag-dobj-> has-ROOT\n",
      "\n",
      "\n",
      "heavy\n",
      "heavy-acomp-> is-relcl-> is-relcl-> bag-dobj-> bag-dobj-> has-ROOT\n",
      "\n",
      "\n",
      ".\n",
      ".-punct-> has-ROOT\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h1='He has a bag which is heavy.'\n",
    "doc = nlp(h1)\n",
    "\n",
    "for token in doc:\n",
    "    print(token)\n",
    "    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in headList_to_root(token)]))\n",
    "    print('\\n')\n",
    "doc = nlp(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n",
      "He-nsubj-> says-ROOT\n",
      "\n",
      "\n",
      "says\n",
      "\n",
      "\n",
      "\n",
      "that\n",
      "that-mark-> like-ccomp-> like-ccomp-> says-ROOT\n",
      "\n",
      "\n",
      "you\n",
      "you-nsubj-> like-ccomp-> like-ccomp-> says-ROOT\n",
      "\n",
      "\n",
      "like\n",
      "like-ccomp-> says-ROOT\n",
      "\n",
      "\n",
      "to\n",
      "to-aux-> swim-xcomp-> swim-xcomp-> like-ccomp-> like-ccomp-> says-ROOT\n",
      "\n",
      "\n",
      "swim\n",
      "swim-xcomp-> like-ccomp-> like-ccomp-> says-ROOT\n",
      "\n",
      "\n",
      ".\n",
      ".-punct-> says-ROOT\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h1='He says that you like to swim.'\n",
    "doc = nlp(h1)\n",
    "\n",
    "for token in doc:\n",
    "    print(token)\n",
    "    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in headList_to_root(token)]))\n",
    "    print('\\n')\n",
    "doc = nlp(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # test = 'The report of needed classes shall include (but not be limited to) classes to be offered, number of sections needed, number of labs needed, and room types needed'\n",
    "# # spacy_sent = nlp(test)\n",
    "# for j, token in enumerate(doc):\n",
    "#     headlist_to_root = [(dependent_token.dep_) for dependent_token in headList_to_root(token)]\n",
    "#     headpath_to_root = remove_duplicate(headlist_to_root)  \n",
    "#     print(high_level_implicit_grouping(headpath_to_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
