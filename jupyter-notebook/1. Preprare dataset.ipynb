{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of sentences: 704\n"
     ]
    }
   ],
   "source": [
    "load_file_name = 'dataset/labeled-requirements.txt'\n",
    "seed = 7\n",
    "\n",
    "#####################################################\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "ann_info = {0: '/component/', 1: '/refinement_of_component/', 2: '/action/', \n",
    "            3: '/refinement_of_action/',\n",
    "            4: '/condition/', 5: '/priority/', 6: '/motivation/', 7: '/role/',\n",
    "            8: '/object/', 9: '/refinement_of_object/',\n",
    "            10: '/sub_action/', 11: '/sub_argument_of_action/', 12: '/sub_priority/', \n",
    "            13: '/sub_role/', 14: '/sub_object/',\n",
    "            15: '/sub_refinement_of_object/', 16: '/none/'}\n",
    "\n",
    "\n",
    "####################\n",
    "\"\"\" Load Dataset \"\"\"\n",
    "####################\n",
    "# flip key, value pairs in the dictionary\n",
    "ann_info = dict((v,k) for k,v in ann_info.items())\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "currentX = []\n",
    "currentY = []\n",
    "split_sequences = True\n",
    "\n",
    "for line in open(load_file_name):\n",
    "    line = line.rstrip()\n",
    "\n",
    "    if line:\n",
    "        row = line.split()\n",
    "        word, tag = row\n",
    "        currentX.append(word)\n",
    "        currentY.append(ann_info[tag])\n",
    "    \n",
    "    elif split_sequences: # the end of sentence\n",
    "        X.append(currentX)\n",
    "        Y.append(currentY)\n",
    "        currentX = []\n",
    "        currentY = []\n",
    "\n",
    "print(\"The total number of sentences:\", len(X))\n",
    "\n",
    "##############################\n",
    "\"\"\" Shuffle Data with seed \"\"\"\n",
    "##############################\n",
    "# shuffle...\n",
    "merged_data = list(zip(X, Y))\n",
    "random.seed(seed)\n",
    "random.shuffle(merged_data)\n",
    "\n",
    "X, Y = zip(*merged_data)\n",
    "X = list(X)\n",
    "Y = list(Y)\n",
    "assert(len(X) == len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences per class\n",
      "308 \t /component/\n",
      "32 \t /refinement_of_component/\n",
      "687 \t /action/\n",
      "269 \t /refinement_of_action/\n",
      "129 \t /condition/\n",
      "664 \t /priority/\n",
      "81 \t /motivation/\n",
      "300 \t /role/\n",
      "653 \t /object/\n",
      "202 \t /refinement_of_object/\n",
      "117 \t /sub_action/\n",
      "45 \t /sub_argument_of_action/\n",
      "49 \t /sub_priority/\n",
      "51 \t /sub_role/\n",
      "101 \t /sub_object/\n",
      "30 \t /sub_refinement_of_object/\n",
      "703 \t /none/\n"
     ]
    }
   ],
   "source": [
    "# key-value switching\n",
    "ann_info = {val:key for (key, val) in ann_info.items()}\n",
    "\n",
    "def tkn_assign(y, boolean_list):\n",
    "    boolean_list[y] = True\n",
    "def increment_cnt(sent_num_list, boolean_list):\n",
    "    for i in range(0, len(sent_num_list)):\n",
    "        if boolean_list[i] == True:\n",
    "            sent_num_list[i] += 1\n",
    "            boolean_list[i] = False\n",
    "\n",
    "boolean_list = [False] * 17\n",
    "sent_num_list = [0] * 17\n",
    "\n",
    "for i, sent in enumerate(Y):\n",
    "    for j, token in enumerate(sent):\n",
    "        tkn_assign(Y[i][j], boolean_list)\n",
    "    increment_cnt(sent_num_list, boolean_list)    \n",
    "    \n",
    "print('# of sentences per class')\n",
    "for i in range(0, len(sent_num_list)):\n",
    "    print(sent_num_list[i], '\\t', ann_info[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "정말 기초적인 단계에서 데이터를 손질하자. 형태적인 변형은 여기서 하지 않는다. (엄격한 기준으로) 삭제하거나 추가하기만 한다. 물론 의도적으로 마음에 안드는 (부분적으로) 데이터를 삭제해서는 안되고 삭제하려면 form형태와 같이 삭제되어야 되는 기준이 있어야 한다. (unseen data에 대한 대비를 해야되기 때문이다.)\n",
    "  1. 정말 말도 안되거나 진짜 도움 안되는 것(noise)들만 삭제: etc. 의 .삭제\n",
    "  2. 모든 문장 끝에 콤마 있도록 하기\n",
    "  \n",
    "보통 puntuation은 NLP에서 삭제하기 마련인데, 여기 task에서 자주 등장하는 puntuation은 유용하게 사용될 수 있다. 물론 매우 드물게 등장하는 것들은 infrequend token으로 분류되어 삭제하는 것이 generalization하는게 도움을 준다. <br>\n",
    "분류되는 단위가 token이기 때문에 그들의 위치정보도 많이 도움이 될 것이다. <br>\n",
    "다른 것들은 아직 잘 판단이 안서나, terminator term인 콤마는 유용하게 사용될 수 있다. 왜냐면 그들의 label은 모두 none이면서 동시에 항상 문장 맨 끝에 위치하기 때문이다. 따라서, noise가 아닌 좋은 signal로 작용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data cleaning for parsing doing well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELETE counts =  3\n",
      "(first) DELETE puntuation counts =  108\n",
      "(second) DELETE puntuation counts =  2\n",
      "ADD counts =  149\n"
     ]
    }
   ],
   "source": [
    "##### Parser가 잘 동작하기 위해서 (parsing에 방해되는) 불필요한 charactor들은 삭제한다. \n",
    "##### Parser가 이상하게 동작하면 모델에 치명적 손상을 입힌다. 따라서 최대한 이쁘게 input으로 넣어주는게 필요하다.\n",
    "\n",
    "#\"\"\" Punctuation \"\"\"\n",
    "# 마지막 콤마빼고는 왠만한 punctuation은 다 지운다.\n",
    "\n",
    "### 하이픈 (-) 관련\n",
    "# 동사,명사 중에 end-user와 같이 -하이픈이 있는 경우, 왼쪽 char들과 함께 삭제한다.\n",
    "# 단, non- 패턴은 하이픈만 삭제한다. e.g. non-clinical -> nonclinical \n",
    "# 단, pre- 패턴은 하이픈만 삭제한다. e.g. pre-paid -> prepaid\n",
    "# 단, re- 패턴은 하이픈만 삭제한다. e.g. re-enter -> reenter\n",
    "# 단, un- \n",
    "# 단, -up 패턴은 하이픈만 삭제한다. e.g. pop-up -> popup\n",
    "# 단, -in 패턴은 하이픈만 삭제한다. e.g. mail-in -> mailin\n",
    "### 아니 그냥 여기선 다 삭제\n",
    "\n",
    "# 단, -based, -coded 패턴들은 하이픈과 based, coded를 삭제한다.\n",
    "# 단, e-mail, check-box인 경우도 하이픈만 삭제\n",
    "# 단, 고유명사인경우 하이픈만 삭제 e.g. FACT-ADDRESS -> FACTADDRESS\n",
    "# 2개의 하이픈이 있으면 뒤에 있는 하이픈을 기준으로 한다. e.g. side-by-side - > side\n",
    "\n",
    "\n",
    "### 예외 명사들 (e.g. ????-??-??)\n",
    "# 명사, 동사에 따라 duck으로 바꾼다. (그냥 duck이 명사,동사 다 해당하는 단어라서...)\n",
    "\n",
    "### 오퍼스트로피 (') 관련\n",
    "# 문법적인 '는 제외한다.\n",
    "# ' ' 쌍이 있는 경우에 button, option, funtion, feature, item, section 등과 같이 옆에 관련 명사가 있으면 다음과 같이 처리한다. soft-require text 특징인데, button, option 등을 보조 설명하는 말이다. parsing에 방해된다.\n",
    "# 오른쪽에 명사가 있는 경우 ' '-> beutiful 치환하고 왼쪽에 명사가 있으면 그냥 다 삭제\n",
    "# 단, ' '쌍 안에 명사가 하나면 그냥 '만 삭제한다.\n",
    "# 오른쪽 왼쪽 명사가 모두 없으면 또는 왼쪽에 형용사가 있고 오른쪽에 명사가 없을경우: feature로 치환해준다.\n",
    "# 동사구에서 맨 오른쪽이 명사이면 명사를 살려둔다. \n",
    "\n",
    "### 정상적이지 않은 패턴들 관련\n",
    "# 중간 중간에 ... .와 같이 이어져있는 패턴들 -> 지운다 또는 ... 여러개 있는거 삭제 또는 한개로\n",
    "# 맨 앞에 or이 있으면 삭제\n",
    "\n",
    "### 부연 설명하는 i.e. 와 e.g. 관련\n",
    "# 부연 설명하는 i.e와 e.g.는 annotation하지 않아도 큰 문제가 되지 않는다.\n",
    "# 따라서, 괄호로 둘러싸여있는 i.e. 와 e.g.가 있을 경우 모두 삭제한다. (,로 구분되어 있는 경우도 삭제를 하였다.)\n",
    "# 단, 괄호가 없으면 i.e.또는 e.g.만 삭제한다.\n",
    "\n",
    "### and/or 관련\n",
    "# 정확한 parsing을 하기 위해 모두 and로 통일한다.\n",
    "\n",
    "#\"\"\" for a clear sentence \"\"\"\n",
    "## Parser가 잘 동작하기 위해서 문장이 1개가 있어야 한다.\n",
    "## 따라서 input data는 하나의 문장이 이루워져있다고 가정한다.\n",
    "## 단, 하나의 문장은 여러개의 clause들을 포함한다. 그리고 여러개의 clause들을 and/or로 확장한 것도 하나의 문장이라고 간주한다.\n",
    "\n",
    "### :와 함께 보충 설명하는 문장 삭제\n",
    "# \"This is a feature request: It would be nice to have a Rename checkbox on the encrypt menu along with the feature and beutiful checkboxes\"\n",
    "# -> \"It would be nice to have a Rename checkbox on the encrypt menu along with the feature and beutiful checkboxes\"\n",
    "# \"May I propose that: Results are shown one per line.\n",
    "# -> Results are shown one per line.\n",
    "\n",
    "\n",
    "### : -> where로 바꿔준다. (우리의 데이터에서 :은 문장들을 연결해주는 연결고리이다.)\n",
    "# \"The display shall have two regions:   left 2/3 of the display is graphical, right 1/3 of the display is a data table.\"\n",
    "# 여기서 :를 where로 교체한다. (하나의 문장으로 바꾸기 위해)\n",
    "\n",
    "### : -> which are (접속사)로 바꿔준다. \n",
    "# \"The table side of the display shall be split into 2 regions:  sequential and temporal.\"\n",
    "# \"The table side of the display shall be split into 2 regions which are sequential and temporal.\"\n",
    "\n",
    "### So는 독립적인 2개의 문장을 뜻한다. 하나로 만들어주기 위해 so that구문으로 바꾼다.\n",
    "# \"As an administrator I want to have centralised configuration so I can remotely change settings across all units\"\n",
    "# \"As an administrator I want to have centralised configuration so that I can remotely change settings across all units\"\n",
    "\n",
    "### ; -> which are\n",
    "# The confirmation must contain the following information; the dispute case number, the type of chargeback requested (pending or immediate) and the date that the merchant response is due.\n",
    "# The confirmation must contain the following information which are the dispute case number, the type of chargeback requested (pending or immediate) and the date that the merchant response is due.\n",
    "\n",
    "### ; -> so that\n",
    "# \"It would be nice to add a column to mailbox table; this would be used to store a path to user sieve script\"\n",
    "# \"It would be nice to add a column to mailbox table so that this would be used to store a path to user sieve script\"\n",
    "\n",
    "### 다음과 같이 괄호 삭제\n",
    "#  by (1) the dispute case number, (2) the merchant account number, (3) the cardmember account number and (4) the issuer number\n",
    "\n",
    "##### 불필요한 표현들 삭제 (특히 앞부분에서)\n",
    "## Hello, Also, Hi, In general, In addtion, Or, i think 등등..\n",
    "## e.g., In addition to the above criteria -> 삭제\n",
    "## getSeries (); -> (); 삭제\n",
    "\n",
    "##### it would be X if 패턴\n",
    "##### if -> that으로 바꿔준다. \n",
    "# 보통 if절은 부사절이다. 하지만, 이 패턴이 있는 문장에서 if절은 주절로 사용된다. \n",
    "# 따라서, if를 that로 바꿔 parser가 주절로 인식할 수 있도록 해준다.\n",
    "\n",
    "##### 명령문이 있을 경우\n",
    "# 맨앞에 it을 그냥 추가\n",
    "# e.g. allow option to hide tray icon -> it allow option to hide tray icon\n",
    "\n",
    "### change word\n",
    "for i, sentence in enumerate(X):\n",
    "    for j, token in enumerate(sentence):\n",
    "        if token == 'maybe':\n",
    "            X[i][j] = 'may'\n",
    "            \n",
    "### 자동사 전치사 삭제\n",
    "# search for 와 같은 자동사의 전치사를 삭제해줘서 object가 제대로 인식하도록 해준다.\n",
    "# # Convert tuple to list\n",
    "# X = list(X)\n",
    "# Y = list(Y)\n",
    "\n",
    "# etc. 를 tokenization하면 'etc', '.'가 된다.\n",
    "# etc. -> terminator .와 구별되야 한다.\n",
    "# 따라서 etc 다음의 .는 그냥 삭제하도록 한다.\n",
    "cnt = 0\n",
    "for i, sentence in enumerate(X):\n",
    "    for j, token in enumerate(sentence):\n",
    "        if sentence[j] == '.':\n",
    "            if sentence[j-1] == 'etc':\n",
    "                sentence.remove(sentence[j])  # X delete\n",
    "                del Y[i][j]\n",
    "                cnt += 1\n",
    "print('DELETE counts = ', cnt)\n",
    "\n",
    "puntation_list = ['(', ')', ';']\n",
    "# # 이상하게도 한 번만 for loop 돌면서 삭제하면, 완전히 다 삭제가 안된다.\n",
    "# # 또 다른 이유가 있겠지만, 그냥 여기서 for loop를 2번 돌려서 삭제한다.\n",
    "\n",
    "# First delete\n",
    "cnt = 0\n",
    "for i, sentence in enumerate(X):\n",
    "    for j, token in enumerate(sentence):\n",
    "        if any(token in t for t in puntation_list):\n",
    "            sentence.remove(sentence[j])  # X delete\n",
    "            del Y[i][j]\n",
    "            cnt += 1\n",
    "print('(first) DELETE puntuation counts = ', cnt)\n",
    "\n",
    "# Second delete\n",
    "cnt = 0\n",
    "for i, sentence in enumerate(X):\n",
    "    for j, token in enumerate(sentence):\n",
    "        if any(token in t for t in puntation_list):\n",
    "            sentence.remove(sentence[j])  # X delete\n",
    "            del Y[i][j]\n",
    "            cnt += 1\n",
    "print('(second) DELETE puntuation counts = ', cnt)\n",
    "\n",
    "# terminator comma add!\n",
    "cnt = 0 \n",
    "for i, sentence in enumerate(X):\n",
    "    if not sentence[-1] == '.':\n",
    "        if not sentence[-1] == '?':\n",
    "            X[i] = X[i] + ['.'] # 조심: X[i] += ['.'] 사용x \n",
    "            Y[i] = Y[i] + [16] # none label \n",
    "            cnt += 1\n",
    "print('ADD counts = ', cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 563 ,  test: 141\n"
     ]
    }
   ],
   "source": [
    "validation_size = 0.20 \n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "print('train:', len(X_train), ',  test:',len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Write out files\n",
    "def dump(data, name):\n",
    "    filehandler = open(name,\"wb\")\n",
    "    pickle.dump(data, filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "dump(X_train, 'X_train')\n",
    "dump(Y_train, 'Y_train')\n",
    "dump(X_test, 'X_test')\n",
    "dump(Y_test, 'Y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
