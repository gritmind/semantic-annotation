{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make dataset to be general form\n",
    "\n",
    "by transforming json into text <br>\n",
    "by dividign data into training and test according to sentence <br>\n",
    "training data를 만들 때, 공백도 하나의 token으로 인정할지 안할지도 생각해볼 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# after merging multiple json files\n",
    "path_read_json_file = 'data/labeled-requirements.json'\n",
    "path_write_text_file = \"labeled-requirements.txt\"\n",
    "\n",
    "###################################################################################################\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import *\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from pprint import pprint\n",
    "import string\n",
    "import re\n",
    "import ast\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "result_data = []\n",
    "\n",
    "# DEFINE\n",
    "SENTENCE = \"0\"\n",
    "COMPONENT = \"1___component\"\n",
    "R_COMPONENT = \"2___refinement_of_component\"\n",
    "ACTION = \"3___action\"\n",
    "R_ACTION = \"4___argument_of_action\"\n",
    "CONDITION = \"5___condition\"\n",
    "PRIORITY = \"6___priority\"\n",
    "MOTIVATION = \"7___motivation\"\n",
    "ROLE = \"8___role\"\n",
    "OBJECT = \"9___object\"\n",
    "R_OBJECT = \"a___refinement_of_object\"\n",
    "S_ACTION = \"b___sub_action\"\n",
    "S_R_ACTION = \"c___sub_argument_of_action\"\n",
    "S_PRIORITY = \"d___sub_priority\"\n",
    "S_ROLE = \"e___sub_role\"\n",
    "S_OBJECT = \"f___sub_object\"\n",
    "S_R_OBJECT = \"g___sub_refinement_of_object\"\n",
    "\n",
    "### LOAD DATA (Read Json)\n",
    "with open(path_read_json_file) as json_data:\n",
    "    data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function (for transforming from json to text)\n",
    "\n",
    "여기서 어느 하나 example 중에서 annotation, \" \" -> [ ] 로 바꿀 필요 있음. (에러)\n",
    "\n",
    "* puntuation 중에서 . (점)을 삭제안하고 사용할 것이다. 즉, 문장 마지막 token에서 . (점)이 없으면 모두 추가하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ann_tokenize(point, LABEL): # For distinguish whether string or list\n",
    "    temp2D = []\n",
    "    \n",
    "    #print(point[SENTENCE])\n",
    "    #print('ann_tokinaw!!!!!!!!!!!!', list(str(point[LABEL]))[0], type(str(point[LABEL])))\n",
    "\n",
    "    # 배열과 string을 구분지어줄라고...\n",
    "    # 인위적으로 첫글자가 '['이면 배열이라 생각하고 ast.literal_eval함수를 사용해 배열로 복귀시킨다.\n",
    "    if list(str(point[LABEL]))[0] != '[':\n",
    "        #print('---string', word_tokenize(str(point[LABEL])))\n",
    "        return word_tokenize(str(point[LABEL]))\n",
    "    else:\n",
    "        arr = ast.literal_eval(str(point[LABEL]))\n",
    "        #print('ast.literal:::::', arr)\n",
    "        for chunk in arr:\n",
    "            temp1D = word_tokenize(str(chunk))\n",
    "            temp2D.append(temp1D)\n",
    "        #print('---array', temp2D)\n",
    "        return temp2D         \n",
    "\n",
    "def ann_sentence(tok_sentence, phrase, p, ann_n_t_sen, check_t_sen):\n",
    "    # tok_sentence: ['I','am','a','boy']\n",
    "    # phrase: ['a','boy']\n",
    "    # ann_n_t_sen: [16, 16, 16, 16]\n",
    "    # check_t_sen: [0, 0, 0, 0]\n",
    "    \n",
    "    len_phrase = len(phrase)\n",
    "    \n",
    "#     print('phrase:', phrase)\n",
    "    \n",
    "    for i, token in enumerate(tok_sentence):\n",
    "#         print('token', token)\n",
    "        if tok_sentence[i] == phrase[0] and check_t_sen[i] == 0:\n",
    " \n",
    "            if tok_sentence[i+len_phrase-1] == phrase[-1]:\n",
    "\n",
    "                if check_t_sen[i+len_phrase-1] == 0:\n",
    "                \n",
    "                    for n in range(i, i+len_phrase):\n",
    "\n",
    "                        ann_n_t_sen[n] = p\n",
    "                        check_t_sen[n] = 1\n",
    "                    \n",
    "                    break\n",
    "    \n",
    "    return ann_n_t_sen, check_t_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_preprocessing(point):\n",
    "    \n",
    "    # Remove 'be able to'\n",
    "    point['0'] = re.sub(' be able to', '', point['0'])\n",
    "    if type(point['6___priority']) != type([]):\n",
    "        point['6___priority'] = re.sub(' be able to', '', point['6___priority'])\n",
    "        point['6___priority'] = re.sub(' be able', '', point['6___priority'])\n",
    "      \n",
    "    # Remove 'it would be great/nice that'\n",
    "    point['0'] = re.sub('it Would be great that ', '', point['0'])    \n",
    "    point['0'] = re.sub('It would be great that ', '', point['0'])\n",
    "    point['0'] = re.sub('It would be nice in the future that ', '', point['0'])\n",
    "    point['0'] = re.sub('It would be nice that ', '', point['0'])\n",
    "    \n",
    "    # Remove 'be nice to'\n",
    "    point['0'] = re.sub(' be nice to', '', point['0'])\n",
    "    if type(point['6___priority']) != type([]):\n",
    "        point['6___priority'] = re.sub(' be nice to', '', point['6___priority'])\n",
    "        point['6___priority'] = re.sub(' be nice', '', point['6___priority'])    \n",
    "    \n",
    "    # Remove 'be great to'\n",
    "    point['0'] = re.sub(' be great to', '', point['0'])\n",
    "    if type(point['6___priority']) != type([]):\n",
    "        point['6___priority'] = re.sub(' be great to', '', point['6___priority'])\n",
    "        point['6___priority'] = re.sub(' be great', '', point['6___priority'])     \n",
    "    \n",
    "    # Remove 'like to'\n",
    "    point['0'] = re.sub(' like to', '', point['0'])\n",
    "    if type(point['6___priority']) != type([]):\n",
    "        point['6___priority'] = re.sub(' like to', '', point['6___priority'])\n",
    "        point['6___priority'] = re.sub(' like', '', point['6___priority'])     \n",
    "    \n",
    "    return point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def substitute_preprocessing(point):\n",
    "    \n",
    "    # Remove 'be able to'\n",
    "    point['0'] = re.sub('search for', 'search', point['0'])\n",
    "    point['9___object'] = re.sub('for ', '', point['9___object'])\n",
    "\n",
    "    return point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "point_cnt = 0\n",
    "test_cnt = 0\n",
    "ann_info = {0: '/component/', 1: '/refinement_of_component/', 2: '/action/', 3: '/refinement_of_action/',\n",
    "            4: '/condition/', 5: '/priority/', 6: '/motivation/', 7: '/role/', 8: '/object/', 9: '/refinement_of_object/',\n",
    "            10: '/sub_action/', 11: '/sub_argument_of_action/', 12: '/sub_priority/', 13: '/sub_role/', 14: '/sub_object/',\n",
    "            15: '/sub_refinement_of_object/', 16: '/none/'}\n",
    "\n",
    "with open(path_write_text_file, \"w\") as text_file:\n",
    "\n",
    "    for point in data: # point: 1 example\n",
    "\n",
    "#         print(point['0'])\n",
    "#         print(point['6___priority'])\n",
    "        #point = preprocessing(point)\n",
    "        #point = substitute_preprocessing(point)\n",
    "#         print(point['0'])\n",
    "#         print(point['6___priority'])\n",
    "#         print('\\n')\n",
    "        \n",
    "        # 1. Tokenize All Annotations      \n",
    "        tok_sentence = word_tokenize(str(point[SENTENCE]))\n",
    "        tok_component = ann_tokenize(point, COMPONENT)\n",
    "        tok_r_component = ann_tokenize(point, R_COMPONENT)\n",
    "        tok_action = ann_tokenize(point, ACTION)\n",
    "        tok_r_action = ann_tokenize(point, R_ACTION)\n",
    "        tok_condition = ann_tokenize(point, CONDITION)\n",
    "        tok_priority = ann_tokenize(point, PRIORITY)\n",
    "        tok_motivation = ann_tokenize(point, MOTIVATION)\n",
    "        tok_role = ann_tokenize(point, ROLE)\n",
    "        tok_object = ann_tokenize(point, OBJECT)\n",
    "        tok_r_object = ann_tokenize(point, R_OBJECT)\n",
    "        tok_s_action = ann_tokenize(point, S_ACTION)\n",
    "        tok_s_r_action = ann_tokenize(point, S_R_ACTION)\n",
    "        tok_s_priority = ann_tokenize(point, S_PRIORITY)\n",
    "        tok_s_role = ann_tokenize(point, S_ROLE)\n",
    "        tok_s_object = ann_tokenize(point, S_OBJECT)\n",
    "        tok_s_r_object = ann_tokenize(point, S_R_OBJECT)\n",
    "\n",
    "        \n",
    "  \n",
    "        #print(tok_priority) \n",
    "        #print('\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        tok_collection = [tok_component] + [tok_r_component] + [tok_action] + [tok_r_action] + [tok_condition] \\\n",
    "                        + [tok_priority] + [tok_motivation] + [tok_role] + [tok_object] + [tok_r_object] + [tok_s_action] \\\n",
    "                        + [tok_s_r_action] + [tok_s_priority] + [tok_s_role] + [tok_s_object] + [tok_s_r_object]\n",
    "        \n",
    "#         print(tok_collection)\n",
    "        #[['system'], ['have', 'a', 'MDI', 'form'], ['viewing'], [], [], ['shall'], [], [], [['graph'], ['data', 'table']], [], [], [], [], [], [], []]\n",
    "        \n",
    "        # initialize 16(none) for integer check array for token sentence \n",
    "        ann_num_t_sen = [16] * len(tok_sentence)\n",
    "        check_t_sen = [0] * len(tok_sentence)\n",
    "        \n",
    "        # initialize 0 for binary check array for token annotations\n",
    "        check_t_coll = []\n",
    "        len_t_coll = []\n",
    "        for i, ann_arr in enumerate(tok_collection):\n",
    "\n",
    "            if len(ann_arr) == 0: # [] ann 정보가 없을 때\n",
    "                check_t_coll.append([-1])\n",
    "                len_t_coll.append(0)\n",
    "            else: # ann 정보가 있을 때...\n",
    "                if type(ann_arr[0]) == type('str'): # \"shall be able\"\n",
    "                    if len(ann_arr) == 1:\n",
    "                        check_t_coll.append([0])\n",
    "                        len_t_coll.append(1)\n",
    "                    else:\n",
    "                        check_t_coll.append([0] * len(ann_arr))\n",
    "                        len_t_coll.append(len(ann_arr))\n",
    "                else: # 배열일 때 e.g. [\"program name\", \"listing of required classes\"]\n",
    "                    temp_arr = []\n",
    "                    \n",
    "                    for j, ann in enumerate(ann_arr):\n",
    "                        if len(ann) == 1:\n",
    "                            temp_arr.append([0])\n",
    "                            len_t_coll.append(1)\n",
    "                        else:\n",
    "                            temp_arr.append([0] * len(ann_arr[j]))\n",
    "                            len_t_coll.append(len(ann_arr[j]))\n",
    "#                             temp_arr.append([0] * len(ann_arr[j]))\n",
    "#                             len_t_coll.append(len(ann_arr[j]))\n",
    "                    check_t_coll.append(temp_arr)\n",
    "                    \n",
    "\n",
    "\n",
    "        # 재료들...\n",
    "        sorted_len_arr = sorted(len_t_coll, reverse=True)\n",
    "\n",
    "        #print(check_t_coll)\n",
    "        #[[0], [0, 0, 0, 0], [0], [-1], [-1], [0], [-1], [-1], [[0], [0, 0]], [-1], [-1], [-1], [-1], [-1], [-1], [-1]]\n",
    "        #print(len_t_coll)\n",
    "        #[1, 4, 1, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0]\n",
    "        #print(sorted_len_arr)\n",
    "        #[4, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        \n",
    "#         print(ann_num_t_sen)\n",
    "#         print(check_t_coll)\n",
    "#         print(sorted_len_arr)\n",
    "#         print(\"=====================================================================\")        \n",
    "        \n",
    "        \n",
    "        ### Annotation Copy 시작...\n",
    "        # 첫 번째 목표 check_t_coll에 있는 모든 element들이 -1값을 가지도록 할 것.\n",
    "        # 두 번째 목표 sorted(len_t_coll, reverse=True)를 참고해서 큰 len를 가진 phrase들부터 copy할 것.\n",
    "        # 세 번째 목표 ann_num_t_sen에 모든 ann 번호 할당 할 것.\n",
    "\n",
    "        for length in sorted_len_arr:\n",
    "            \n",
    "#             print(length)\n",
    "#             print(check_t_coll)\n",
    "            if length == 0:\n",
    "                break\n",
    "            else:\n",
    "                \n",
    "                for i, ann_arr in enumerate(tok_collection):\n",
    "                    \n",
    "                    if len(ann_arr) != 0:\n",
    "                    \n",
    "                        if type(ann_arr[0]) == type('str'):\n",
    "#                             print('check_t_coll[0][0]', check_t_coll[i][0])\n",
    "                            if check_t_coll[i][0] == 0: # 0: 아직 copy가 안된 ann\n",
    "                                if len(ann_arr) == length:\n",
    "#                                     print('here!!!')\n",
    "                                    phrase = ann_arr\n",
    "                                    ann_num_t_sen, check_t_sen = ann_sentence(tok_sentence, phrase, i, ann_num_t_sen, check_t_sen)\n",
    "                                    check_t_coll[i] = [-1] * len(ann_arr)\n",
    "                                    break\n",
    "\n",
    "\n",
    "                        else:\n",
    "                            for j, ann in enumerate(ann_arr):\n",
    "                                \n",
    "                                if check_t_coll[i][j][0] == 0: # 0: 아직 copy가 안된 ann\n",
    "                                    if len(ann) == length:\n",
    "#                                         print('here!!!')\n",
    "                                        phrase = ann\n",
    "                                        #list\n",
    "                                        ann_num_t_sen, check_t_sen = ann_sentence(tok_sentence, phrase, i, ann_num_t_sen, check_t_sen)\n",
    "                                        check_t_coll[i][j] = [-1] * len(ann)\n",
    "                                        break\n",
    "                \n",
    "#         print(ann_num_t_sen)\n",
    "#         print(check_t_coll)\n",
    "#         print(sorted_len_arr)        \n",
    "        \n",
    "        #print(tok_sentence)\n",
    "        \n",
    "    \n",
    "        #print(ann_num_t_sen)\n",
    "        ###############################\n",
    "        \"\"\" Text file에 write하기... \"\"\" \n",
    "        ###############################\n",
    "        for i, token in enumerate(tok_sentence):\n",
    "            \n",
    "            #ann = which_ann(str(token), tok_sentence, i, tok_collection)\n",
    "            \n",
    "#             print((token) + '     ' + ann_info[ann_num_t_sen[i]])\n",
    "            text_file.write((token) + '     ' + ann_info[ann_num_t_sen[i]])\n",
    "            text_file.write(\"\\n\")\n",
    "            \n",
    "        # one sentence over...\n",
    "#         print((\"\\n\"))\n",
    "        text_file.write(\"\\n\")\n",
    "\n",
    "#         point_cnt += 1  \n",
    "#         if point_cnt == 1:\n",
    "#             break\n",
    "\n",
    "#print(test_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def which_ann(token, tok_sentence, i, tok_collection):\n",
    "    \n",
    "#     ann_info = {0: '/component/', 1: '/refinement_of_component/', 2: '/action/', 3: '/refinement_of_action/',\n",
    "#                 4: '/condition/', 5: '/priority/', 6: '/motivation/', 7: '/role/', 8: '/object/', 9: '/refinement_of_object/',\n",
    "#                 10: '/sub_action/', 11: '/sub_argument_of_action/', 12: '/sub_priority/', 13: '/sub_role/', 14: '/sub_object/',\n",
    "#                 15: '/sub_refinement_of_object/', 16: '/none/'}\n",
    "    \n",
    "#     check_assign = False\n",
    "#     result_ann_num = 16\n",
    "#     check_3gram_right = False\n",
    "#     check_3gram_left = False \n",
    "#     check_2gram_right = False\n",
    "#     check_2gram_left = False     \n",
    "    \n",
    "#     evenif_3gram = False\n",
    "#     evenif_2gram = False\n",
    "#     #print(tok_collection)\n",
    "    \n",
    "#     for m, tok_array in enumerate(tok_collection):\n",
    "#         for n, tok in enumerate(tok_array):\n",
    "            \n",
    "#             # Json 포맷에 annotation은 배열과 string으로 구분된다.\n",
    "#             # 따라서, array와 string을 구분지어서 처리해준다.\n",
    "#             # 3-gram까지 보기 때문에 정확도가 매우 높을 것이다.\n",
    "            \n",
    "#             ##### In case of 'array': \n",
    "#             #####\n",
    "#             ########################\n",
    "#             if type(tok) == type([]):\n",
    "#                 #log('[[[array]]] --------------------------- len(tok_array)=', len(tok_array), 'n=', n)\n",
    "            \n",
    "#                 for l, _ in enumerate(tok): # array이므로, element들을 모두 순회해야 한다.\n",
    "#                     #print('[[[array_element]]] --------------------------- len(tok)=', len(tok), 'l=', l)\n",
    "#                     ### 1단계: 3-gram check\n",
    "#                     #######################\n",
    "#                     if len(tok) >= 3: # 최소 조건\n",
    "#                         log('array = start len(tok_array) >= 3')\n",
    "#                         ##### 1-1단계: RIGHT DIRECTION\n",
    "#                         ### In terms of annotated token\n",
    "#                         # last-1\n",
    "#                         if l == len(tok)-2:\n",
    "#                             check_3gram_right = False\n",
    "#                         # last\n",
    "#                         elif l == len(tok)-1:\n",
    "#                             check_3gram_right = False\n",
    "#                         ### In terms of sentence token\n",
    "#                         # last-1\n",
    "#                         elif i == len(tok_sentence)-2:\n",
    "#                             check_3gram_right = False\n",
    "#                         # last\n",
    "#                         elif i == len(tok_sentence)-1:\n",
    "#                             check_3gram_right = False\n",
    "#                         else:\n",
    "#                             check_3gram_right = True\n",
    "                            \n",
    "#                         ##### 1-2단계: LEFT DIRECTION\n",
    "#                         ### In terms of annotated token\n",
    "#                         # first-1\n",
    "#                         if l == 1:\n",
    "#                             check_3gram_left = False\n",
    "#                         # first\n",
    "#                         elif l == 0:\n",
    "#                             check_3gram_left = False\n",
    "#                         ### In terms of sentence token\n",
    "#                         # first-1\n",
    "#                         elif i == 1:\n",
    "#                             check_3gram_left = False\n",
    "#                         # first\n",
    "#                         elif i == 0:\n",
    "#                             check_3gram_left = False\n",
    "#                         else:\n",
    "#                             check_3gram_left = True\n",
    "                        \n",
    "#                         ##### 마무리...\n",
    "#                         if check_3gram_right == True and check_3gram_left == True:\n",
    "#                             # then compare\n",
    "#                             if tok_sentence[i] == tok[l]:\n",
    "#                                 if tok_sentence[i+1] == tok[l+1] and tok_sentence[i+2] == tok[l+2]:\n",
    "#                                     if tok_sentence[i-1] == tok[l-1] and tok_sentence[i-2] == tok[l-2]:\n",
    "#                                         return ann_info[m]\n",
    "#                                     else:\n",
    "#                                         return ann_info[16] # annotation 길이가 3이상인데, 여기서 안되면, 나가리다... \n",
    "#                         elif check_3gram_right == True and check_3gram_left == False:\n",
    "#                             if tok_sentence[i] == tok[l]:\n",
    "#                                 if tok_sentence[i+1] == tok[l+1] and tok_sentence[i+2] == tok[l+2]:\n",
    "#                                     return ann_info[m]\n",
    "#                                 else:\n",
    "#                                     return ann_info[16]\n",
    "#                         elif check_3gram_right == False and check_3gram_left == True:\n",
    "#                             if tok_sentence[i] == tok[l]:\n",
    "#                                 if tok_sentence[i-1] == tok[l-1] and tok_sentence[i-2] == tok[l-2]:\n",
    "#                                     return ann_info[m]\n",
    "#                                 else:\n",
    "#                                     return ann_info[16]\n",
    "\n",
    "#                         log('array = over len(tok_array) >= 3')\n",
    "                        \n",
    "\n",
    "#                     ### 1단계: 2-gram check\n",
    "#                     #######################\n",
    "#                     if len(tok) >= 2: # 최소 조건\n",
    "#                         log('array = start len(tok_array) >= 2')    \n",
    "#                         ##### 2-1단계: RIGHT DIRECTION\n",
    "#                         ### In terms of annotated token\n",
    "#                         # last\n",
    "#                         if l == len(tok)-1:\n",
    "#                             check_2gram_right = False                        \n",
    "#                         ### In terms of sentence token\n",
    "#                         # last\n",
    "#                         elif i == len(tok_sentence)-1:\n",
    "#                             check_2gram_right = False\n",
    "#                         else:\n",
    "#                             check_2gram_right = True                        \n",
    "                        \n",
    "#                         ##### 2-2단계: LEFT DIRECTION\n",
    "#                         ### In terms of annotated token\n",
    "#                         # first\n",
    "#                         if l == 0:\n",
    "#                             check_2gram_left = False                        \n",
    "#                         ### In terms of sentence token\n",
    "#                         # first                      \n",
    "#                         elif i == 0:\n",
    "#                             check_2gram_left = False\n",
    "#                         else:\n",
    "#                             check_2gram_left = True                        \n",
    "                        \n",
    "#                         ##### 마무리...\n",
    "#                         if check_2gram_right == True and check_2gram_left == True:\n",
    "#                             # then compare\n",
    "#                             if tok_sentence[i] == tok[l]:\n",
    "#                                 if tok_sentence[i+1] == tok[l+1]:\n",
    "#                                     if tok_sentence[i-1] == tok[l-1]:\n",
    "#                                         return ann_info[m]\n",
    "#                                     else:\n",
    "#                                         return ann_info[16]\n",
    "#                         elif check_2gram_right == True and check_2gram_left == False:\n",
    "#                             if tok_sentence[i] == tok[l]:\n",
    "#                                 if tok_sentence[i+1] == tok[l+1]:\n",
    "#                                     return ann_info[m]\n",
    "#                                 else:\n",
    "#                                     return ann_info[16]\n",
    "#                         elif check_2gram_right == False and check_2gram_left == True:\n",
    "#                             if tok_sentence[i] == tok[l]:\n",
    "#                                 if tok_sentence[i-1] == tok[l-1]:\n",
    "#                                     return ann_info[m]\n",
    "#                                 else:\n",
    "#                                     return ann_info[16]\n",
    "                                                \n",
    "#                         log('array = over len(tok_array) >= 2')\n",
    "                                \n",
    "#                     ### 3단계: 1-gram check\n",
    "#                     ######################\n",
    "#                     if len(tok) >= 1:\n",
    "#                         log('array = start len(tok_array) >= 1')\n",
    "#                         if tok_sentence[i] == tok[l]:\n",
    "#                             return ann_info[m]\n",
    "                        \n",
    "#                         log('array = over len(tok_array) >= 1')\n",
    "                           \n",
    "                        \n",
    "                   \n",
    "#             # 위의 array 방법과 똑같이...            \n",
    "#             ##### In case of 'string'\n",
    "#             #####\n",
    "#             ###########################################################################################################\n",
    "#             else:\n",
    "#                 #print('tok-is-string-', tok_array, len((tok_array)), type(tok), tok)\n",
    "#                 check_3gram_right = False\n",
    "#                 check_3gram_left = False \n",
    "#                 check_2gram_right = False\n",
    "#                 check_2gram_left = False\n",
    "#                 evenif_3gram = False\n",
    "#                 evenif_2gram = False\n",
    "#                 l = n\n",
    "#                 #print('[[[string]]] --------------------------- len(tok_array)=', len(tok_array), 'n=', n)\n",
    "                \n",
    "#                 ### 1단계: 3-gram check\n",
    "#                 #######################\n",
    "#                 if len(tok_array) >= 3: # 최소 조건\n",
    "#                     log('string = start len(tok_array) >= 3')\n",
    "#                     ##### 1-1단계: RIGHT DIRECTION\n",
    "#                     ### In terms of annotated token\n",
    "#                     # last-1\n",
    "#                     if l == len(tok_array)-2:\n",
    "#                         check_3gram_right = False\n",
    "#                     # last\n",
    "#                     elif l == len(tok_array)-1:\n",
    "#                         check_3gram_right = False\n",
    "#                     ### In terms of sentence token\n",
    "#                     # last-1\n",
    "#                     elif i == len(tok_sentence)-2:\n",
    "#                         check_3gram_right = False\n",
    "#                     # last\n",
    "#                     elif i == len(tok_sentence)-1:\n",
    "#                         check_3gram_right = False\n",
    "#                     else:\n",
    "#                         check_3gram_right = True\n",
    "                            \n",
    "#                     ##### 1-2단계: LEFT DIRECTION\n",
    "#                     ### In terms of annotated token\n",
    "#                     # first-1\n",
    "#                     if l == 1:\n",
    "#                         check_3gram_left = False\n",
    "#                     # first\n",
    "#                     elif l == 0:\n",
    "#                         check_3gram_left = False\n",
    "#                     ### In terms of sentence token\n",
    "#                     # first-1\n",
    "#                     elif i == 1:\n",
    "#                         check_3gram_left = False\n",
    "#                     # first\n",
    "#                     elif i == 0:\n",
    "#                         check_3gram_left = False\n",
    "#                     else:\n",
    "#                         check_3gram_left = True\n",
    "                        \n",
    "#                     ##### 마무리...\n",
    "#                     if check_3gram_right == True:\n",
    "#                         if check_3gram_left == True:\n",
    "#                             # then compare\n",
    "#                             if tok_sentence[i] == tok_array[l]:\n",
    "#                                 if tok_sentence[i+1] == tok_array[l+1] and tok_sentence[i+2] == tok_array[l+2]:\n",
    "#                                     if tok_sentence[i-1] == tok_array[l-1] and tok_sentence[i-2] == tok_array[l-2]:\n",
    "#                                         return ann_info[m]\n",
    "#                                     else:\n",
    "#                                         return ann_info[16]\n",
    "#                     elif check_3gram_right == True and check_3gram_left == False:\n",
    "#                         if tok_sentence[i] == tok_array[l]:\n",
    "#                             if tok_sentence[i+1] == tok_array[l+1] and tok_sentence[i+2] == tok_array[l+2]:\n",
    "#                                 return ann_info[m]\n",
    "#                             else:\n",
    "#                                 return ann_info[16]\n",
    "#                     elif check_3gram_right == False and check_3gram_left == True:\n",
    "#                         if tok_sentence[i] == tok_array[l]:\n",
    "#                             if tok_sentence[i-1] == tok_array[l-1] and tok_sentence[i-2] == tok_array[l-2]:\n",
    "#                                 return ann_info[m]\n",
    "#                             else:\n",
    "#                                 return ann_info[16]\n",
    "                       \n",
    "#                     log('string = over len(tok_array) >= 3')\n",
    "                \n",
    "#                 ### 1단계: 2-gram check\n",
    "#                 #######################\n",
    "#                 if len(tok_array) >= 2: # 최소 조건\n",
    "#                     log('string = start len(tok_array) >= 2')  \n",
    "#                     ##### 2-1단계: RIGHT DIRECTION\n",
    "#                     ### In terms of annotated token\n",
    "#                     # last\n",
    "#                     if l == len(tok_array)-1:\n",
    "#                         check_2gram_right = False                        \n",
    "                    \n",
    "#                     ### In terms of sentence token\n",
    "#                     # last\n",
    "#                     elif i == len(tok_sentence)-1:\n",
    "#                         check_2gram_right = False\n",
    "#                     else:\n",
    "#                         check_2gram_right = True                        \n",
    "                        \n",
    "#                     ##### 2-2단계: LEFT DIRECTION\n",
    "#                     ### In terms of annotated token\n",
    "#                     # first\n",
    "#                     if l == 0:\n",
    "#                         check_2gram_left = False                        \n",
    "#                     ### In terms of sentence token\n",
    "#                     # first                      \n",
    "#                     elif i == 0:\n",
    "#                         check_2gram_left = False\n",
    "#                     else:\n",
    "#                         check_2gram_left = True                        \n",
    "                        \n",
    "                    \n",
    "#                     ##### 마무리...\n",
    "#                     if check_2gram_right == True and check_2gram_left == True:\n",
    "#                         # then compare\n",
    "#                         if tok_sentence[i] == tok_array[l]:\n",
    "#                             if tok_sentence[i+1] == tok_array[l+1]:\n",
    "#                                 if tok_sentence[i-1] == tok_array[l-1]:\n",
    "#                                     return ann_info[m]\n",
    "#                                 else:\n",
    "#                                     return ann_info[16]\n",
    "#                     elif check_2gram_right == True and check_2gram_left == False:\n",
    "#                         if tok_sentence[i] == tok_array[l]:\n",
    "#                             if tok_sentence[i+1] == tok_array[l+1]:\n",
    "#                                 return ann_info[m]\n",
    "#                             else:\n",
    "#                                 return ann_info[16]\n",
    "#                     elif check_2gram_right == False and check_2gram_left == True:\n",
    "#                         if tok_sentence[i] == tok_array[l]:\n",
    "#                             if tok_sentence[i-1] == tok_array[l-1]:\n",
    "#                                 return ann_info[m]\n",
    "#                             else:\n",
    "#                                 return ann_info[16]\n",
    "                                \n",
    "#                     log('string = over len(tok_array) >= 2')                \n",
    "#                 ### 3단계: 1-gram check\n",
    "#                 ######################\n",
    "#                 if len(tok_array) >= 1:\n",
    "#                     log('string = start len(tok_array) >= 1')\n",
    "#                     if tok_sentence[i] == tok_array[l]:\n",
    "#                         return ann_info[m]\n",
    "                    \n",
    "#                 log('string = over len(tok_array) >= 1') \n",
    "    \n",
    "#     # 위의 다돌았는데도 없으면...\n",
    "#     # ann 안되는 것으로 간주하고...\n",
    "#     return ann_info[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import logging, sys\n",
    "# logging.basicConfig(stream=sys.stderr, level=logging.DEBUG)\n",
    "# logging.debug('A debug message!')\n",
    "# logging.info('We processed %d records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# logger.critical('This is a critical message.')\n",
    "# logger.error('This is an error message.')\n",
    "# logger.warning('This is a warning message.')\n",
    "# logger.info('This is an informative message.')\n",
    "# logger.debug('This is a low-level debug message.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def which_ann(token, tok_sentence, i, tok_collection):\n",
    "    \n",
    "#     ann_info = {0: '/component/', 1: '/refinement_of_component/', 2: '/action/', 3: '/refinement_of_action/',\n",
    "#                 4: '/condition/', 5: '/priority/', 6: '/motivation/', 7: '/role/', 8: '/object/', 9: '/refinement_of_object/',\n",
    "#                 10: '/sub_action/', 11: '/sub_argument_of_action/', 12: '/sub_priority/', 13: '/sub_role/', 14: '/sub_object/',\n",
    "#                 15: '/sub_refinement_of_object/', 16: '/none/'}\n",
    "    \n",
    "#     num_assign = 0\n",
    "#     result_ann_num = 16\n",
    "    \n",
    "#     for m, tok_array in enumerate(tok_collection):\n",
    "#         for n, tok in enumerate(tok_array):\n",
    "            \n",
    "#             if type(tok) == type([]): \n",
    "#                 for l, ttt in enumerate(tok):\n",
    "#                     if token == ttt:\n",
    "#                         num_assign += 1\n",
    "#                         result_ann_num = m\n",
    "#             else:\n",
    "#                 if token == tok:\n",
    "#                     num_assign += 1\n",
    "#                     result_ann_num = m\n",
    "    \n",
    "#     if num_assign == 1: # 1개만 assign되면 그대로 해당 annotation을 할당\n",
    "#         return ann_info[result_ann_num]\n",
    "#     elif num_assign == 0: # 1개도 assign되지 않으면 annotation안되는 것으로 판단해 none으로 처리\n",
    "#         return '/none/'\n",
    "#     else: # 2개이상 assign되면 2-gram을 사용해 재검사를 실시\n",
    "#         num_assign = 0\n",
    "#         for m, tok_array in enumerate(tok_collection):\n",
    "#             for n, tok in enumerate(tok_array):\n",
    "                \n",
    "#                 # In case of 'array'\n",
    "#                 if type(tok) == type([]): \n",
    "#                     for l, ttt in enumerate(tok): # because of array\n",
    "#                         if i != len(tok_sentence)-1 and l != len(tok)-1: # not last token               \n",
    "#                             if token == ttt and tok_sentence[i+1] == tok[l+1]:\n",
    "#                                 #print(ttt)\n",
    "#                                 num_assign += 1\n",
    "#                                 result_ann_num = m\n",
    "#                         elif i != 0 and l != 0: # not first token\n",
    "#                             if token == ttt and tok_sentence[i-1] == tok[l-1]:\n",
    "#                                 num_assign += 1\n",
    "#                                 result_ann_num = m\n",
    "\n",
    "#                 # In case of 'string'\n",
    "#                 elif type(tok) == type('string'):\n",
    "#                     if i != len(tok_sentence)-1 and n != len(tok_array)-1: # not last token\n",
    "#                         if token == tok and tok_sentence[i+1] == tok_array[n+1]:\n",
    "#                             num_assign += 1\n",
    "#                             result_ann_num = m\n",
    "#                     elif i != 0 and n != 0: # not first token\n",
    "#                         if token == tok and tok_sentence[i-1] == tok_array[n-1]:\n",
    "#                             num_assign += 1\n",
    "#                             result_ann_num = m                    \n",
    "                    \n",
    "#         return ann_info[result_ann_num]\n",
    "    \n",
    "#         if num_assign >= 2:\n",
    "#             return '/test/' # check how many /test/ are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def untokenize(words):\n",
    "#     \"\"\"\n",
    "#     Untokenizing a text undoes the tokenizing operation, restoring\n",
    "#     punctuation and spaces to the places that people expect them to be.\n",
    "#     Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "#     except for line breaks.\n",
    "#     \"\"\"\n",
    "#     text = ' '.join(words)\n",
    "#     step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "#     step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "#     step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "#     step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "#     step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "#          \"can not\", \"cannot\").replace(\" //www\", \"//www\")\n",
    "#     step6 = step5.replace(\" ` \", \" '\")\n",
    "#     return step6.strip()\n",
    "\n",
    "# tokenized = word_tokenize(EXAMPLE_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
